{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6105a668-7319-45df-add9-f47fd3f90b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "136/137 [============================>.] - ETA: 0s - loss: 5.5490 - accuracy: 0.8139\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25365, saving model to best_model_480.h5\n",
      "137/137 [==============================] - 5s 30ms/step - loss: 5.5127 - accuracy: 0.8141 - val_loss: 6.7398 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "  4/137 [..............................] - ETA: 3s - loss: 0.4279 - accuracy: 0.9219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/137 [============================>.] - ETA: 0s - loss: 0.4933 - accuracy: 0.8745\n",
      "Epoch 2: val_accuracy did not improve from 0.25365\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.4931 - accuracy: 0.8741 - val_loss: 3.7628 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.8729\n",
      "Epoch 3: val_accuracy did not improve from 0.25365\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.4704 - accuracy: 0.8729 - val_loss: 2.0225 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.4564 - accuracy: 0.8780\n",
      "Epoch 4: val_accuracy improved from 0.25365 to 0.44343, saving model to best_model_480.h5\n",
      "137/137 [==============================] - 4s 30ms/step - loss: 0.4554 - accuracy: 0.8784 - val_loss: 0.8256 - val_accuracy: 0.4434 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.4504 - accuracy: 0.8883\n",
      "Epoch 5: val_accuracy improved from 0.44343 to 0.87135, saving model to best_model_480.h5\n",
      "137/137 [==============================] - 4s 29ms/step - loss: 0.4490 - accuracy: 0.8884 - val_loss: 0.3185 - val_accuracy: 0.8714 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.4506 - accuracy: 0.8877\n",
      "Epoch 6: val_accuracy improved from 0.87135 to 0.89781, saving model to best_model_480.h5\n",
      "137/137 [==============================] - 4s 30ms/step - loss: 0.4506 - accuracy: 0.8873 - val_loss: 0.2981 - val_accuracy: 0.8978 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.8873\n",
      "Epoch 7: val_accuracy did not improve from 0.89781\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.4490 - accuracy: 0.8875 - val_loss: 0.3890 - val_accuracy: 0.8631 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.4285 - accuracy: 0.8928\n",
      "Epoch 8: val_accuracy did not improve from 0.89781\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.4268 - accuracy: 0.8928 - val_loss: 0.3720 - val_accuracy: 0.8604 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.4222 - accuracy: 0.8904\n",
      "Epoch 9: val_accuracy did not improve from 0.89781\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.4230 - accuracy: 0.8905 - val_loss: 0.2924 - val_accuracy: 0.8768 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8987\n",
      "Epoch 10: val_accuracy did not improve from 0.89781\n",
      "137/137 [==============================] - 4s 27ms/step - loss: 0.3828 - accuracy: 0.8987 - val_loss: 0.2743 - val_accuracy: 0.8932 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3684 - accuracy: 0.9072\n",
      "Epoch 11: val_accuracy did not improve from 0.89781\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.3674 - accuracy: 0.9067 - val_loss: 0.4004 - val_accuracy: 0.8504 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3531 - accuracy: 0.9030\n",
      "Epoch 12: val_accuracy improved from 0.89781 to 0.91058, saving model to best_model_480.h5\n",
      "137/137 [==============================] - 4s 30ms/step - loss: 0.3521 - accuracy: 0.9030 - val_loss: 0.2533 - val_accuracy: 0.9106 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.9074\n",
      "Epoch 13: val_accuracy did not improve from 0.91058\n",
      "137/137 [==============================] - 4s 27ms/step - loss: 0.3623 - accuracy: 0.9071 - val_loss: 0.2925 - val_accuracy: 0.9088 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3586 - accuracy: 0.9067\n",
      "Epoch 14: val_accuracy did not improve from 0.91058\n",
      "137/137 [==============================] - 4s 27ms/step - loss: 0.3586 - accuracy: 0.9067 - val_loss: 0.2908 - val_accuracy: 0.9051 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.9097\n",
      "Epoch 15: val_accuracy did not improve from 0.91058\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.3465 - accuracy: 0.9097 - val_loss: 0.2768 - val_accuracy: 0.9106 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.9097\n",
      "Epoch 16: val_accuracy did not improve from 0.91058\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.3423 - accuracy: 0.9099 - val_loss: 0.2673 - val_accuracy: 0.9051 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3368 - accuracy: 0.9150\n",
      "Epoch 17: val_accuracy did not improve from 0.91058\n",
      "137/137 [==============================] - 4s 29ms/step - loss: 0.3382 - accuracy: 0.9142 - val_loss: 0.2527 - val_accuracy: 0.9060 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.9143\n",
      "Epoch 18: val_accuracy did not improve from 0.91058\n",
      "137/137 [==============================] - 4s 29ms/step - loss: 0.3285 - accuracy: 0.9140 - val_loss: 0.2990 - val_accuracy: 0.9060 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3327 - accuracy: 0.9102\n",
      "Epoch 19: val_accuracy did not improve from 0.91058\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.3338 - accuracy: 0.9097 - val_loss: 0.2887 - val_accuracy: 0.9015 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9207\n",
      "Epoch 20: val_accuracy improved from 0.91058 to 0.91423, saving model to best_model_480.h5\n",
      "137/137 [==============================] - 4s 29ms/step - loss: 0.2971 - accuracy: 0.9208 - val_loss: 0.2749 - val_accuracy: 0.9142 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9261\n",
      "Epoch 21: val_accuracy did not improve from 0.91423\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2877 - accuracy: 0.9261 - val_loss: 0.2725 - val_accuracy: 0.9124 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.9310\n",
      "Epoch 22: val_accuracy did not improve from 0.91423\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2779 - accuracy: 0.9304 - val_loss: 0.2967 - val_accuracy: 0.9097 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.9258\n",
      "Epoch 23: val_accuracy improved from 0.91423 to 0.91788, saving model to best_model_480.h5\n",
      "137/137 [==============================] - 4s 30ms/step - loss: 0.2747 - accuracy: 0.9258 - val_loss: 0.2683 - val_accuracy: 0.9179 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2600 - accuracy: 0.9320\n",
      "Epoch 24: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2593 - accuracy: 0.9320 - val_loss: 0.2656 - val_accuracy: 0.9161 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9338\n",
      "Epoch 25: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2608 - accuracy: 0.9338 - val_loss: 0.3142 - val_accuracy: 0.9097 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.9318\n",
      "Epoch 26: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2594 - accuracy: 0.9320 - val_loss: 0.2879 - val_accuracy: 0.9170 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9361\n",
      "Epoch 27: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2487 - accuracy: 0.9361 - val_loss: 0.2986 - val_accuracy: 0.9151 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9357\n",
      "Epoch 28: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2492 - accuracy: 0.9361 - val_loss: 0.2821 - val_accuracy: 0.9106 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9359\n",
      "Epoch 29: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2518 - accuracy: 0.9359 - val_loss: 0.3034 - val_accuracy: 0.9161 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9442\n",
      "Epoch 30: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2245 - accuracy: 0.9443 - val_loss: 0.2826 - val_accuracy: 0.9161 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2152 - accuracy: 0.9449\n",
      "Epoch 31: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 29ms/step - loss: 0.2147 - accuracy: 0.9450 - val_loss: 0.3376 - val_accuracy: 0.9124 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2063 - accuracy: 0.9485\n",
      "Epoch 32: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 29ms/step - loss: 0.2055 - accuracy: 0.9489 - val_loss: 0.3014 - val_accuracy: 0.9151 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9464\n",
      "Epoch 33: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2118 - accuracy: 0.9464 - val_loss: 0.3385 - val_accuracy: 0.9069 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2019 - accuracy: 0.9483\n",
      "Epoch 34: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2021 - accuracy: 0.9484 - val_loss: 0.3098 - val_accuracy: 0.9170 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.9507\n",
      "Epoch 35: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.2017 - accuracy: 0.9507 - val_loss: 0.3406 - val_accuracy: 0.9078 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9449\n",
      "Epoch 36: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.1906 - accuracy: 0.9446 - val_loss: 0.2990 - val_accuracy: 0.9133 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9487\n",
      "Epoch 37: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.1933 - accuracy: 0.9487 - val_loss: 0.3467 - val_accuracy: 0.9161 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1858 - accuracy: 0.9524\n",
      "Epoch 38: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.1857 - accuracy: 0.9523 - val_loss: 0.3467 - val_accuracy: 0.9151 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9490\n",
      "Epoch 39: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.1985 - accuracy: 0.9493 - val_loss: 0.3746 - val_accuracy: 0.9142 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9502\n",
      "Epoch 40: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.1775 - accuracy: 0.9503 - val_loss: 0.3687 - val_accuracy: 0.9124 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9564\n",
      "Epoch 41: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.1645 - accuracy: 0.9564 - val_loss: 0.3409 - val_accuracy: 0.9161 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9569\n",
      "Epoch 42: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.1572 - accuracy: 0.9571 - val_loss: 0.3612 - val_accuracy: 0.9151 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9562Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.1684 - accuracy: 0.9562 - val_loss: 0.3778 - val_accuracy: 0.9170 - lr: 6.2500e-04\n",
      "Epoch 43: early stopping\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_excel('Final_non_redundant_sequences.xlsx', na_filter=False)\n",
    "X_data_name = 'whole_sample_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv'\n",
    "X_data = pd.read_csv(X_data_name, header=0, index_col=0, delimiter=',')\n",
    "X = np.array(X_data)\n",
    "y = np.array(dataset['label'])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input = Input(input_shape)\n",
    "    x = Conv1D(64, 5, strides=1, padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv1D(128, 5, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr * np.power(drop, np.floor((1 + epoch) / epochs_drop))\n",
    "    return lr\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    input_shape = (480, 1)\n",
    "    model = build_model(input_shape)\n",
    "    \n",
    "    # Optimizer\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, restore_best_weights=True)\n",
    "    mc = ModelCheckpoint('best_model_480.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    callbacks_list = [lrate, early_stop, mc]\n",
    "    \n",
    "    class_weight = {0: 1, 1: 2}  # Adjust the weights as needed\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, callbacks=callbacks_list, batch_size=32, class_weight=class_weight)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Load the best model\n",
    "saved_model = load_model('best_model_480.h5')\n",
    "\n",
    "# Function to optimize threshold based on MCC\n",
    "def optimize_threshold(y_true, y_pred_probas):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    best_mcc = -1\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_probas > threshold).astype(int)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        \n",
    "        if mcc > best_mcc:\n",
    "            best_mcc = mcc\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6324ed60-7935-4472-a851-fb1f53a6d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 10.9823 - accuracy: 0.7961\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24601, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 31ms/step - loss: 10.9823 - accuracy: 0.7961 - val_loss: 5.4077 - val_accuracy: 0.2460 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5363 - accuracy: 0.8681\n",
      "Epoch 2: val_accuracy did not improve from 0.24601\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.5358 - accuracy: 0.8679 - val_loss: 5.6977 - val_accuracy: 0.2460 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5089 - accuracy: 0.8599\n",
      "Epoch 3: val_accuracy did not improve from 0.24601\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.5085 - accuracy: 0.8603 - val_loss: 3.1789 - val_accuracy: 0.2460 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4783 - accuracy: 0.8681\n",
      "Epoch 4: val_accuracy improved from 0.24601 to 0.35991, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4770 - accuracy: 0.8684 - val_loss: 1.3617 - val_accuracy: 0.3599 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.8745\n",
      "Epoch 5: val_accuracy improved from 0.35991 to 0.82916, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4615 - accuracy: 0.8745 - val_loss: 0.3958 - val_accuracy: 0.8292 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.8758\n",
      "Epoch 6: val_accuracy improved from 0.82916 to 0.88838, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4646 - accuracy: 0.8765 - val_loss: 0.3598 - val_accuracy: 0.8884 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4418 - accuracy: 0.8824\n",
      "Epoch 7: val_accuracy improved from 0.88838 to 0.90888, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4429 - accuracy: 0.8821 - val_loss: 0.2695 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4503 - accuracy: 0.8803\n",
      "Epoch 8: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.4498 - accuracy: 0.8806 - val_loss: 0.2716 - val_accuracy: 0.9066 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4597 - accuracy: 0.8829\n",
      "Epoch 9: val_accuracy improved from 0.90888 to 0.92027, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4594 - accuracy: 0.8829 - val_loss: 0.2877 - val_accuracy: 0.9203 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4012 - accuracy: 0.8919\n",
      "Epoch 10: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.4016 - accuracy: 0.8917 - val_loss: 0.2689 - val_accuracy: 0.9180 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8963\n",
      "Epoch 11: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.3988 - accuracy: 0.8966 - val_loss: 0.3267 - val_accuracy: 0.8861 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3999 - accuracy: 0.8868\n",
      "Epoch 12: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3979 - accuracy: 0.8874 - val_loss: 0.2491 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8963\n",
      "Epoch 13: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.3910 - accuracy: 0.8963 - val_loss: 0.2542 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3878 - accuracy: 0.8963\n",
      "Epoch 14: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3878 - accuracy: 0.8963 - val_loss: 0.2529 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4038 - accuracy: 0.8943\n",
      "Epoch 15: val_accuracy improved from 0.92027 to 0.92711, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4040 - accuracy: 0.8943 - val_loss: 0.2685 - val_accuracy: 0.9271 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3757 - accuracy: 0.8994\n",
      "Epoch 16: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3755 - accuracy: 0.8993 - val_loss: 0.2390 - val_accuracy: 0.9248 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3742 - accuracy: 0.8965\n",
      "Epoch 17: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.3744 - accuracy: 0.8960 - val_loss: 0.2654 - val_accuracy: 0.9271 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8983\n",
      "Epoch 18: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3771 - accuracy: 0.8983 - val_loss: 0.2580 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3797 - accuracy: 0.9090\n",
      "Epoch 19: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3794 - accuracy: 0.9092 - val_loss: 0.2608 - val_accuracy: 0.9180 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3529 - accuracy: 0.9073\n",
      "Epoch 20: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3543 - accuracy: 0.9067 - val_loss: 0.3137 - val_accuracy: 0.8952 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.9130\n",
      "Epoch 21: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3167 - accuracy: 0.9130 - val_loss: 0.2599 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.9121\n",
      "Epoch 22: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3354 - accuracy: 0.9120 - val_loss: 0.2662 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.9188\n",
      "Epoch 23: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.3226 - accuracy: 0.9186 - val_loss: 0.2553 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.9142\n",
      "Epoch 24: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3225 - accuracy: 0.9146 - val_loss: 0.2931 - val_accuracy: 0.9203 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3339 - accuracy: 0.9170\n",
      "Epoch 25: val_accuracy improved from 0.92711 to 0.93394, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3325 - accuracy: 0.9173 - val_loss: 0.2578 - val_accuracy: 0.9339 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.9162\n",
      "Epoch 26: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.3201 - accuracy: 0.9161 - val_loss: 0.2573 - val_accuracy: 0.9180 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.9132\n",
      "Epoch 27: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3226 - accuracy: 0.9133 - val_loss: 0.2514 - val_accuracy: 0.9317 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.9141\n",
      "Epoch 28: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3095 - accuracy: 0.9140 - val_loss: 0.2684 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9186\n",
      "Epoch 29: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3054 - accuracy: 0.9186 - val_loss: 0.2489 - val_accuracy: 0.9339 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.9252\n",
      "Epoch 30: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2834 - accuracy: 0.9257 - val_loss: 0.3269 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9202\n",
      "Epoch 31: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2906 - accuracy: 0.9201 - val_loss: 0.2895 - val_accuracy: 0.9203 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.9311\n",
      "Epoch 32: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2665 - accuracy: 0.9310 - val_loss: 0.2858 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.9314\n",
      "Epoch 33: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.2637 - accuracy: 0.9313 - val_loss: 0.2874 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2702 - accuracy: 0.9215\n",
      "Epoch 34: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.2696 - accuracy: 0.9217 - val_loss: 0.2793 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9237\n",
      "Epoch 35: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.2659 - accuracy: 0.9237 - val_loss: 0.2821 - val_accuracy: 0.9089 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2552 - accuracy: 0.9327\n",
      "Epoch 36: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.2552 - accuracy: 0.9326 - val_loss: 0.3192 - val_accuracy: 0.9112 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2534 - accuracy: 0.9329\n",
      "Epoch 37: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2517 - accuracy: 0.9333 - val_loss: 0.3178 - val_accuracy: 0.9066 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9282\n",
      "Epoch 38: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2657 - accuracy: 0.9282 - val_loss: 0.3074 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2552 - accuracy: 0.9283\n",
      "Epoch 39: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2573 - accuracy: 0.9282 - val_loss: 0.2930 - val_accuracy: 0.9271 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9344\n",
      "Epoch 40: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2259 - accuracy: 0.9336 - val_loss: 0.3357 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9380\n",
      "Epoch 41: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2201 - accuracy: 0.9381 - val_loss: 0.3321 - val_accuracy: 0.9134 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2328 - accuracy: 0.9326\n",
      "Epoch 42: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2327 - accuracy: 0.9328 - val_loss: 0.3442 - val_accuracy: 0.9066 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9406\n",
      "Epoch 43: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2246 - accuracy: 0.9404 - val_loss: 0.3664 - val_accuracy: 0.9112 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9411\n",
      "Epoch 44: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2373 - accuracy: 0.9412 - val_loss: 0.3132 - val_accuracy: 0.9180 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2337 - accuracy: 0.9316Restoring model weights from the end of the best epoch: 25.\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2332 - accuracy: 0.9318 - val_loss: 0.3361 - val_accuracy: 0.9134 - lr: 6.2500e-04\n",
      "Epoch 45: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 10.3947 - accuracy: 0.8087\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26424, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 4s 30ms/step - loss: 10.3739 - accuracy: 0.8088 - val_loss: 5.3744 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.8846\n",
      "Epoch 2: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.4647 - accuracy: 0.8846 - val_loss: 6.4412 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.8854\n",
      "Epoch 3: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4340 - accuracy: 0.8854 - val_loss: 2.6747 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8882\n",
      "Epoch 4: val_accuracy improved from 0.26424 to 0.28474, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4156 - accuracy: 0.8884 - val_loss: 1.8076 - val_accuracy: 0.2847 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8900\n",
      "Epoch 5: val_accuracy improved from 0.28474 to 0.86560, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4154 - accuracy: 0.8900 - val_loss: 0.3396 - val_accuracy: 0.8656 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8978\n",
      "Epoch 6: val_accuracy improved from 0.86560 to 0.89066, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4154 - accuracy: 0.8978 - val_loss: 0.2785 - val_accuracy: 0.8907 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4086 - accuracy: 0.8966\n",
      "Epoch 7: val_accuracy did not improve from 0.89066\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.4082 - accuracy: 0.8966 - val_loss: 0.3051 - val_accuracy: 0.8679 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.9065\n",
      "Epoch 8: val_accuracy did not improve from 0.89066\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3872 - accuracy: 0.9067 - val_loss: 0.2840 - val_accuracy: 0.8884 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3907 - accuracy: 0.8971\n",
      "Epoch 9: val_accuracy improved from 0.89066 to 0.90433, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3907 - accuracy: 0.8971 - val_loss: 0.2958 - val_accuracy: 0.9043 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.9083\n",
      "Epoch 10: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.3652 - accuracy: 0.9085 - val_loss: 0.2558 - val_accuracy: 0.8907 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.9082\n",
      "Epoch 11: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3446 - accuracy: 0.9082 - val_loss: 0.2689 - val_accuracy: 0.8861 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.9170\n",
      "Epoch 12: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3353 - accuracy: 0.9176 - val_loss: 0.2695 - val_accuracy: 0.8907 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.9159\n",
      "Epoch 13: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3200 - accuracy: 0.9161 - val_loss: 0.2589 - val_accuracy: 0.8861 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.9168\n",
      "Epoch 14: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3226 - accuracy: 0.9166 - val_loss: 0.2547 - val_accuracy: 0.8815 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.9202\n",
      "Epoch 15: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3098 - accuracy: 0.9201 - val_loss: 0.2632 - val_accuracy: 0.8998 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.9171\n",
      "Epoch 16: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3202 - accuracy: 0.9171 - val_loss: 0.2663 - val_accuracy: 0.8998 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.9121\n",
      "Epoch 17: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3403 - accuracy: 0.9123 - val_loss: 0.2577 - val_accuracy: 0.8929 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.9152\n",
      "Epoch 18: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3107 - accuracy: 0.9153 - val_loss: 0.2381 - val_accuracy: 0.8952 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3178 - accuracy: 0.9142\n",
      "Epoch 19: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3166 - accuracy: 0.9146 - val_loss: 0.2820 - val_accuracy: 0.8838 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2720 - accuracy: 0.9301\n",
      "Epoch 20: val_accuracy did not improve from 0.90433\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2716 - accuracy: 0.9303 - val_loss: 0.2324 - val_accuracy: 0.8975 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.9347\n",
      "Epoch 21: val_accuracy improved from 0.90433 to 0.90661, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.2512 - accuracy: 0.9343 - val_loss: 0.2448 - val_accuracy: 0.9066 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.9324\n",
      "Epoch 22: val_accuracy did not improve from 0.90661\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.2638 - accuracy: 0.9328 - val_loss: 0.2507 - val_accuracy: 0.9066 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.9346\n",
      "Epoch 23: val_accuracy improved from 0.90661 to 0.91116, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.2556 - accuracy: 0.9346 - val_loss: 0.2437 - val_accuracy: 0.9112 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9339\n",
      "Epoch 24: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2548 - accuracy: 0.9333 - val_loss: 0.2449 - val_accuracy: 0.9066 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.9375\n",
      "Epoch 25: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2446 - accuracy: 0.9376 - val_loss: 0.2729 - val_accuracy: 0.9066 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9406\n",
      "Epoch 26: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2400 - accuracy: 0.9394 - val_loss: 0.2581 - val_accuracy: 0.9112 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2427 - accuracy: 0.9372\n",
      "Epoch 27: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2424 - accuracy: 0.9374 - val_loss: 0.2631 - val_accuracy: 0.9021 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2223 - accuracy: 0.9411\n",
      "Epoch 28: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2228 - accuracy: 0.9409 - val_loss: 0.2595 - val_accuracy: 0.8998 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9393\n",
      "Epoch 29: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2356 - accuracy: 0.9397 - val_loss: 0.2970 - val_accuracy: 0.8952 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1947 - accuracy: 0.9518\n",
      "Epoch 30: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1937 - accuracy: 0.9521 - val_loss: 0.2639 - val_accuracy: 0.9112 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9506\n",
      "Epoch 31: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1968 - accuracy: 0.9506 - val_loss: 0.2740 - val_accuracy: 0.9112 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9524\n",
      "Epoch 32: val_accuracy improved from 0.91116 to 0.92027, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.1825 - accuracy: 0.9523 - val_loss: 0.2734 - val_accuracy: 0.9203 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1903 - accuracy: 0.9477\n",
      "Epoch 33: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1903 - accuracy: 0.9478 - val_loss: 0.2597 - val_accuracy: 0.9203 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9484\n",
      "Epoch 34: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1894 - accuracy: 0.9483 - val_loss: 0.2456 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9531\n",
      "Epoch 35: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1789 - accuracy: 0.9531 - val_loss: 0.2924 - val_accuracy: 0.9112 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.9538\n",
      "Epoch 36: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1815 - accuracy: 0.9539 - val_loss: 0.2910 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9544\n",
      "Epoch 37: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1694 - accuracy: 0.9541 - val_loss: 0.2676 - val_accuracy: 0.9112 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9551\n",
      "Epoch 38: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1730 - accuracy: 0.9551 - val_loss: 0.2582 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9597\n",
      "Epoch 39: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1601 - accuracy: 0.9597 - val_loss: 0.2890 - val_accuracy: 0.9203 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9593\n",
      "Epoch 40: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1540 - accuracy: 0.9594 - val_loss: 0.2827 - val_accuracy: 0.9157 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9636\n",
      "Epoch 41: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1453 - accuracy: 0.9635 - val_loss: 0.2881 - val_accuracy: 0.9157 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1422 - accuracy: 0.9649\n",
      "Epoch 42: val_accuracy improved from 0.92027 to 0.92255, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.1439 - accuracy: 0.9648 - val_loss: 0.2939 - val_accuracy: 0.9226 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9658\n",
      "Epoch 43: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.1395 - accuracy: 0.9658 - val_loss: 0.3079 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.9665\n",
      "Epoch 44: val_accuracy improved from 0.92255 to 0.92483, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.1412 - accuracy: 0.9663 - val_loss: 0.3130 - val_accuracy: 0.9248 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1456 - accuracy: 0.9599\n",
      "Epoch 45: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1454 - accuracy: 0.9599 - val_loss: 0.3108 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9672\n",
      "Epoch 46: val_accuracy improved from 0.92483 to 0.92711, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.1336 - accuracy: 0.9675 - val_loss: 0.3386 - val_accuracy: 0.9271 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9667\n",
      "Epoch 47: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1286 - accuracy: 0.9668 - val_loss: 0.3327 - val_accuracy: 0.9157 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9685\n",
      "Epoch 48: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1268 - accuracy: 0.9686 - val_loss: 0.3353 - val_accuracy: 0.9134 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9682\n",
      "Epoch 49: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1330 - accuracy: 0.9683 - val_loss: 0.2950 - val_accuracy: 0.9134 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9733\n",
      "Epoch 50: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1222 - accuracy: 0.9734 - val_loss: 0.2971 - val_accuracy: 0.9180 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9723\n",
      "Epoch 51: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1101 - accuracy: 0.9724 - val_loss: 0.3416 - val_accuracy: 0.9134 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1168 - accuracy: 0.9716\n",
      "Epoch 52: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1168 - accuracy: 0.9716 - val_loss: 0.3341 - val_accuracy: 0.9203 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9739\n",
      "Epoch 53: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1116 - accuracy: 0.9739 - val_loss: 0.3342 - val_accuracy: 0.9226 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9733\n",
      "Epoch 54: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1104 - accuracy: 0.9734 - val_loss: 0.3428 - val_accuracy: 0.9248 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9728\n",
      "Epoch 55: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1125 - accuracy: 0.9729 - val_loss: 0.3377 - val_accuracy: 0.9248 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9718\n",
      "Epoch 56: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1093 - accuracy: 0.9716 - val_loss: 0.3415 - val_accuracy: 0.9248 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9719\n",
      "Epoch 57: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1134 - accuracy: 0.9719 - val_loss: 0.3589 - val_accuracy: 0.9226 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9719\n",
      "Epoch 58: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1077 - accuracy: 0.9719 - val_loss: 0.3606 - val_accuracy: 0.9180 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9739\n",
      "Epoch 59: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1081 - accuracy: 0.9739 - val_loss: 0.3748 - val_accuracy: 0.9203 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9741\n",
      "Epoch 60: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1013 - accuracy: 0.9744 - val_loss: 0.3683 - val_accuracy: 0.9226 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1008 - accuracy: 0.9744\n",
      "Epoch 61: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1009 - accuracy: 0.9741 - val_loss: 0.3779 - val_accuracy: 0.9248 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9764\n",
      "Epoch 62: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.0958 - accuracy: 0.9764 - val_loss: 0.3821 - val_accuracy: 0.9248 - lr: 1.5625e-04\n",
      "Epoch 63/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9756\n",
      "Epoch 63: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1035 - accuracy: 0.9757 - val_loss: 0.3809 - val_accuracy: 0.9248 - lr: 1.5625e-04\n",
      "Epoch 64/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9766\n",
      "Epoch 64: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.0916 - accuracy: 0.9767 - val_loss: 0.3674 - val_accuracy: 0.9226 - lr: 1.5625e-04\n",
      "Epoch 65/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.0950 - accuracy: 0.9752\n",
      "Epoch 65: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.0953 - accuracy: 0.9749 - val_loss: 0.3776 - val_accuracy: 0.9248 - lr: 1.5625e-04\n",
      "Epoch 66/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9767Restoring model weights from the end of the best epoch: 46.\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.0956 - accuracy: 0.9767 - val_loss: 0.3733 - val_accuracy: 0.9203 - lr: 1.5625e-04\n",
      "Epoch 66: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 5.9039 - accuracy: 0.8148\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26424, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 31ms/step - loss: 5.8956 - accuracy: 0.8147 - val_loss: 2.3956 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.8651\n",
      "Epoch 2: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.5300 - accuracy: 0.8651 - val_loss: 2.5277 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4879 - accuracy: 0.8799\n",
      "Epoch 3: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.4880 - accuracy: 0.8793 - val_loss: 4.4759 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4534 - accuracy: 0.8896\n",
      "Epoch 4: val_accuracy improved from 0.26424 to 0.30068, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4552 - accuracy: 0.8887 - val_loss: 1.5506 - val_accuracy: 0.3007 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4788 - accuracy: 0.8765\n",
      "Epoch 5: val_accuracy improved from 0.30068 to 0.87472, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4788 - accuracy: 0.8765 - val_loss: 0.3940 - val_accuracy: 0.8747 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.8860\n",
      "Epoch 6: val_accuracy improved from 0.87472 to 0.90433, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4359 - accuracy: 0.8856 - val_loss: 0.3013 - val_accuracy: 0.9043 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4403 - accuracy: 0.8889\n",
      "Epoch 7: val_accuracy improved from 0.90433 to 0.90888, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4403 - accuracy: 0.8889 - val_loss: 0.2842 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4244 - accuracy: 0.8915\n",
      "Epoch 8: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4244 - accuracy: 0.8912 - val_loss: 0.3922 - val_accuracy: 0.9066 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8950\n",
      "Epoch 9: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.4144 - accuracy: 0.8950 - val_loss: 0.3128 - val_accuracy: 0.9066 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3819 - accuracy: 0.9009\n",
      "Epoch 10: val_accuracy improved from 0.90888 to 0.91344, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.3823 - accuracy: 0.9006 - val_loss: 0.2905 - val_accuracy: 0.9134 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.9086\n",
      "Epoch 11: val_accuracy did not improve from 0.91344\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3640 - accuracy: 0.9087 - val_loss: 0.2878 - val_accuracy: 0.9043 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.9115\n",
      "Epoch 12: val_accuracy improved from 0.91344 to 0.91572, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3607 - accuracy: 0.9115 - val_loss: 0.2910 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.9114\n",
      "Epoch 13: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3538 - accuracy: 0.9113 - val_loss: 0.2865 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3414 - accuracy: 0.9174\n",
      "Epoch 14: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3425 - accuracy: 0.9173 - val_loss: 0.2972 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3335 - accuracy: 0.9119\n",
      "Epoch 15: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3356 - accuracy: 0.9110 - val_loss: 0.3045 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.9085\n",
      "Epoch 16: val_accuracy improved from 0.91572 to 0.92255, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3500 - accuracy: 0.9082 - val_loss: 0.2677 - val_accuracy: 0.9226 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.9176\n",
      "Epoch 17: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3226 - accuracy: 0.9176 - val_loss: 0.2802 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.9184\n",
      "Epoch 18: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3200 - accuracy: 0.9186 - val_loss: 0.2911 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3169 - accuracy: 0.9184\n",
      "Epoch 19: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3169 - accuracy: 0.9184 - val_loss: 0.3266 - val_accuracy: 0.9112 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.9273\n",
      "Epoch 20: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2932 - accuracy: 0.9277 - val_loss: 0.2577 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.9329\n",
      "Epoch 21: val_accuracy improved from 0.92255 to 0.93166, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.2698 - accuracy: 0.9328 - val_loss: 0.2781 - val_accuracy: 0.9317 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9369\n",
      "Epoch 22: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2642 - accuracy: 0.9369 - val_loss: 0.3118 - val_accuracy: 0.9294 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.9304\n",
      "Epoch 23: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2708 - accuracy: 0.9305 - val_loss: 0.3068 - val_accuracy: 0.9317 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9357\n",
      "Epoch 24: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2640 - accuracy: 0.9359 - val_loss: 0.2529 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2472 - accuracy: 0.9406\n",
      "Epoch 25: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2470 - accuracy: 0.9407 - val_loss: 0.3182 - val_accuracy: 0.9271 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.9362\n",
      "Epoch 26: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2669 - accuracy: 0.9369 - val_loss: 0.3100 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9372\n",
      "Epoch 27: val_accuracy improved from 0.93166 to 0.93394, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.2427 - accuracy: 0.9374 - val_loss: 0.3044 - val_accuracy: 0.9339 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9371\n",
      "Epoch 28: val_accuracy improved from 0.93394 to 0.93622, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2447 - accuracy: 0.9371 - val_loss: 0.2943 - val_accuracy: 0.9362 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9399\n",
      "Epoch 29: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2413 - accuracy: 0.9399 - val_loss: 0.2818 - val_accuracy: 0.9271 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.9452\n",
      "Epoch 30: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2258 - accuracy: 0.9457 - val_loss: 0.3105 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2091 - accuracy: 0.9516\n",
      "Epoch 31: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2085 - accuracy: 0.9521 - val_loss: 0.3463 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9513\n",
      "Epoch 32: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2016 - accuracy: 0.9513 - val_loss: 0.3234 - val_accuracy: 0.9317 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9508\n",
      "Epoch 33: val_accuracy improved from 0.93622 to 0.94077, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.2057 - accuracy: 0.9508 - val_loss: 0.3139 - val_accuracy: 0.9408 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9552\n",
      "Epoch 34: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1889 - accuracy: 0.9551 - val_loss: 0.3415 - val_accuracy: 0.9339 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1861 - accuracy: 0.9577\n",
      "Epoch 35: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1865 - accuracy: 0.9577 - val_loss: 0.3216 - val_accuracy: 0.9408 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9567\n",
      "Epoch 36: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1848 - accuracy: 0.9561 - val_loss: 0.3585 - val_accuracy: 0.9271 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.9501\n",
      "Epoch 37: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1900 - accuracy: 0.9501 - val_loss: 0.3884 - val_accuracy: 0.9203 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9564\n",
      "Epoch 38: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1890 - accuracy: 0.9564 - val_loss: 0.3528 - val_accuracy: 0.9317 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9588\n",
      "Epoch 39: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1728 - accuracy: 0.9589 - val_loss: 0.3551 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1663 - accuracy: 0.9637\n",
      "Epoch 40: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1663 - accuracy: 0.9637 - val_loss: 0.3466 - val_accuracy: 0.9317 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.9604\n",
      "Epoch 41: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1714 - accuracy: 0.9602 - val_loss: 0.3604 - val_accuracy: 0.9271 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9603\n",
      "Epoch 42: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1655 - accuracy: 0.9602 - val_loss: 0.3463 - val_accuracy: 0.9385 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9621\n",
      "Epoch 43: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1620 - accuracy: 0.9622 - val_loss: 0.3811 - val_accuracy: 0.9294 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9588\n",
      "Epoch 44: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1665 - accuracy: 0.9587 - val_loss: 0.3564 - val_accuracy: 0.9317 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1589 - accuracy: 0.9644\n",
      "Epoch 45: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1591 - accuracy: 0.9642 - val_loss: 0.3763 - val_accuracy: 0.9339 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1638 - accuracy: 0.9624\n",
      "Epoch 46: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1635 - accuracy: 0.9625 - val_loss: 0.3904 - val_accuracy: 0.9294 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1524 - accuracy: 0.9647\n",
      "Epoch 47: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1523 - accuracy: 0.9648 - val_loss: 0.3812 - val_accuracy: 0.9317 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1484 - accuracy: 0.9639\n",
      "Epoch 48: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1487 - accuracy: 0.9637 - val_loss: 0.4084 - val_accuracy: 0.9294 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9604\n",
      "Epoch 49: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1599 - accuracy: 0.9604 - val_loss: 0.3976 - val_accuracy: 0.9294 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9650\n",
      "Epoch 50: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1379 - accuracy: 0.9650 - val_loss: 0.4030 - val_accuracy: 0.9317 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9662\n",
      "Epoch 51: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1509 - accuracy: 0.9663 - val_loss: 0.3907 - val_accuracy: 0.9317 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9678\n",
      "Epoch 52: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1447 - accuracy: 0.9678 - val_loss: 0.4173 - val_accuracy: 0.9339 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 33.\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.94077\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1419 - accuracy: 0.9686 - val_loss: 0.4183 - val_accuracy: 0.9317 - lr: 3.1250e-04\n",
      "Epoch 53: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1297 - accuracy: 0.7817\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24429, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 32ms/step - loss: 7.1297 - accuracy: 0.7817 - val_loss: 8.0750 - val_accuracy: 0.2443 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.8626\n",
      "Epoch 2: val_accuracy did not improve from 0.24429\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5225 - accuracy: 0.8626 - val_loss: 14.6740 - val_accuracy: 0.2443 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5371 - accuracy: 0.8719\n",
      "Epoch 3: val_accuracy did not improve from 0.24429\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.5385 - accuracy: 0.8715 - val_loss: 8.4375 - val_accuracy: 0.2443 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5250 - accuracy: 0.8694\n",
      "Epoch 4: val_accuracy improved from 0.24429 to 0.29224, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.5250 - accuracy: 0.8695 - val_loss: 1.4976 - val_accuracy: 0.2922 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5027 - accuracy: 0.8755\n",
      "Epoch 5: val_accuracy did not improve from 0.29224\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.5050 - accuracy: 0.8753 - val_loss: 2.3497 - val_accuracy: 0.2626 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5187 - accuracy: 0.8788\n",
      "Epoch 6: val_accuracy improved from 0.29224 to 0.73059, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5187 - accuracy: 0.8783 - val_loss: 0.6864 - val_accuracy: 0.7306 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5237 - accuracy: 0.8755\n",
      "Epoch 7: val_accuracy improved from 0.73059 to 0.89041, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5237 - accuracy: 0.8755 - val_loss: 0.2809 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.8864\n",
      "Epoch 8: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4987 - accuracy: 0.8864 - val_loss: 0.3408 - val_accuracy: 0.8653 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5124 - accuracy: 0.8773\n",
      "Epoch 9: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5137 - accuracy: 0.8768 - val_loss: 0.3136 - val_accuracy: 0.8630 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4694 - accuracy: 0.8958\n",
      "Epoch 10: val_accuracy improved from 0.89041 to 0.89269, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4687 - accuracy: 0.8961 - val_loss: 0.2945 - val_accuracy: 0.8927 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.8974\n",
      "Epoch 11: val_accuracy did not improve from 0.89269\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4488 - accuracy: 0.8971 - val_loss: 0.3341 - val_accuracy: 0.8744 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4614 - accuracy: 0.9024\n",
      "Epoch 12: val_accuracy did not improve from 0.89269\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4620 - accuracy: 0.9022 - val_loss: 0.3141 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4705 - accuracy: 0.8913\n",
      "Epoch 13: val_accuracy did not improve from 0.89269\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.4696 - accuracy: 0.8915 - val_loss: 0.3302 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4380 - accuracy: 0.9014\n",
      "Epoch 14: val_accuracy did not improve from 0.89269\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.4387 - accuracy: 0.9011 - val_loss: 0.2629 - val_accuracy: 0.8927 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4372 - accuracy: 0.9006\n",
      "Epoch 15: val_accuracy improved from 0.89269 to 0.89726, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4355 - accuracy: 0.9011 - val_loss: 0.2905 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4481 - accuracy: 0.8899\n",
      "Epoch 16: val_accuracy did not improve from 0.89726\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4464 - accuracy: 0.8907 - val_loss: 0.4422 - val_accuracy: 0.8356 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4423 - accuracy: 0.8942\n",
      "Epoch 17: val_accuracy improved from 0.89726 to 0.90639, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4409 - accuracy: 0.8943 - val_loss: 0.2663 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4202 - accuracy: 0.9075\n",
      "Epoch 18: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4233 - accuracy: 0.9070 - val_loss: 0.2866 - val_accuracy: 0.8836 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.9034\n",
      "Epoch 19: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.4243 - accuracy: 0.9034 - val_loss: 0.3222 - val_accuracy: 0.8744 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.9124\n",
      "Epoch 20: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3904 - accuracy: 0.9123 - val_loss: 0.3388 - val_accuracy: 0.8904 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3926 - accuracy: 0.9101\n",
      "Epoch 21: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3923 - accuracy: 0.9098 - val_loss: 0.3440 - val_accuracy: 0.8699 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3829 - accuracy: 0.9136\n",
      "Epoch 22: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3829 - accuracy: 0.9136 - val_loss: 0.2589 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.9162\n",
      "Epoch 23: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3809 - accuracy: 0.9161 - val_loss: 0.2814 - val_accuracy: 0.8950 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3584 - accuracy: 0.9207\n",
      "Epoch 24: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3579 - accuracy: 0.9209 - val_loss: 0.3139 - val_accuracy: 0.8836 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.9131\n",
      "Epoch 25: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3808 - accuracy: 0.9131 - val_loss: 0.2712 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3730 - accuracy: 0.9158\n",
      "Epoch 26: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3730 - accuracy: 0.9158 - val_loss: 0.2478 - val_accuracy: 0.8950 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.9144\n",
      "Epoch 27: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3816 - accuracy: 0.9146 - val_loss: 0.2560 - val_accuracy: 0.9018 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.9221\n",
      "Epoch 28: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3591 - accuracy: 0.9219 - val_loss: 0.2551 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.9193\n",
      "Epoch 29: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3639 - accuracy: 0.9186 - val_loss: 0.3730 - val_accuracy: 0.8607 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.9232\n",
      "Epoch 30: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3360 - accuracy: 0.9237 - val_loss: 0.3164 - val_accuracy: 0.8904 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.9306\n",
      "Epoch 31: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3247 - accuracy: 0.9308 - val_loss: 0.2920 - val_accuracy: 0.8927 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3395 - accuracy: 0.9276\n",
      "Epoch 32: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3391 - accuracy: 0.9278 - val_loss: 0.3021 - val_accuracy: 0.8995 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3300 - accuracy: 0.9270\n",
      "Epoch 33: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3300 - accuracy: 0.9270 - val_loss: 0.2649 - val_accuracy: 0.8973 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.9317\n",
      "Epoch 34: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3210 - accuracy: 0.9318 - val_loss: 0.2993 - val_accuracy: 0.8973 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.9273\n",
      "Epoch 35: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3286 - accuracy: 0.9278 - val_loss: 0.3041 - val_accuracy: 0.8927 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.9284\n",
      "Epoch 36: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3234 - accuracy: 0.9280 - val_loss: 0.2601 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.9283Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3249 - accuracy: 0.9283 - val_loss: 0.3636 - val_accuracy: 0.8767 - lr: 0.0012\n",
      "Epoch 37: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 8.3925 - accuracy: 0.8163\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23973, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 31ms/step - loss: 8.3738 - accuracy: 0.8167 - val_loss: 0.9405 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4873 - accuracy: 0.8786\n",
      "Epoch 2: val_accuracy did not improve from 0.23973\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4885 - accuracy: 0.8786 - val_loss: 1.6226 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.8842\n",
      "Epoch 3: val_accuracy did not improve from 0.23973\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4623 - accuracy: 0.8842 - val_loss: 1.5313 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4547 - accuracy: 0.8801\n",
      "Epoch 4: val_accuracy improved from 0.23973 to 0.68950, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.4547 - accuracy: 0.8801 - val_loss: 0.5945 - val_accuracy: 0.6895 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4423 - accuracy: 0.8834\n",
      "Epoch 5: val_accuracy improved from 0.68950 to 0.90411, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4421 - accuracy: 0.8837 - val_loss: 0.3689 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4543 - accuracy: 0.8892\n",
      "Epoch 6: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4536 - accuracy: 0.8895 - val_loss: 0.3280 - val_accuracy: 0.8881 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.8826\n",
      "Epoch 7: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4470 - accuracy: 0.8826 - val_loss: 0.3639 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4672 - accuracy: 0.8788\n",
      "Epoch 8: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4672 - accuracy: 0.8788 - val_loss: 0.3101 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4417 - accuracy: 0.8887\n",
      "Epoch 9: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4415 - accuracy: 0.8887 - val_loss: 0.3164 - val_accuracy: 0.8836 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.8978\n",
      "Epoch 10: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4171 - accuracy: 0.8978 - val_loss: 0.2837 - val_accuracy: 0.8927 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.9075\n",
      "Epoch 11: val_accuracy improved from 0.90411 to 0.90639, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.3851 - accuracy: 0.9075 - val_loss: 0.2849 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.9065\n",
      "Epoch 12: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3848 - accuracy: 0.9067 - val_loss: 0.3348 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4026 - accuracy: 0.8989\n",
      "Epoch 13: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4021 - accuracy: 0.8991 - val_loss: 0.2880 - val_accuracy: 0.8813 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.9068\n",
      "Epoch 14: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3707 - accuracy: 0.9067 - val_loss: 0.3014 - val_accuracy: 0.8950 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3753 - accuracy: 0.9080\n",
      "Epoch 15: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3753 - accuracy: 0.9080 - val_loss: 0.2868 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3758 - accuracy: 0.9101\n",
      "Epoch 16: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3771 - accuracy: 0.9100 - val_loss: 0.2819 - val_accuracy: 0.8858 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3680 - accuracy: 0.9119\n",
      "Epoch 17: val_accuracy improved from 0.90639 to 0.91096, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3675 - accuracy: 0.9120 - val_loss: 0.2644 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3545 - accuracy: 0.9147\n",
      "Epoch 18: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.3556 - accuracy: 0.9133 - val_loss: 0.2968 - val_accuracy: 0.8858 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.9131\n",
      "Epoch 19: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3673 - accuracy: 0.9128 - val_loss: 0.3388 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.9149\n",
      "Epoch 20: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3333 - accuracy: 0.9148 - val_loss: 0.2647 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3166 - accuracy: 0.9275\n",
      "Epoch 21: val_accuracy improved from 0.91096 to 0.92009, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3166 - accuracy: 0.9275 - val_loss: 0.2577 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.9286\n",
      "Epoch 22: val_accuracy improved from 0.92009 to 0.92466, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.2979 - accuracy: 0.9288 - val_loss: 0.2484 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.9280\n",
      "Epoch 23: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2896 - accuracy: 0.9280 - val_loss: 0.2704 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.9294\n",
      "Epoch 24: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2819 - accuracy: 0.9293 - val_loss: 0.2895 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9266\n",
      "Epoch 25: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3026 - accuracy: 0.9267 - val_loss: 0.2918 - val_accuracy: 0.8881 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2730 - accuracy: 0.9326\n",
      "Epoch 26: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2732 - accuracy: 0.9328 - val_loss: 0.3251 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.9311\n",
      "Epoch 27: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2738 - accuracy: 0.9308 - val_loss: 0.3065 - val_accuracy: 0.8904 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.9290\n",
      "Epoch 28: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2753 - accuracy: 0.9290 - val_loss: 0.2962 - val_accuracy: 0.9041 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9351\n",
      "Epoch 29: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2642 - accuracy: 0.9351 - val_loss: 0.3364 - val_accuracy: 0.8950 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9388\n",
      "Epoch 30: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2467 - accuracy: 0.9387 - val_loss: 0.2763 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9385\n",
      "Epoch 31: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2375 - accuracy: 0.9389 - val_loss: 0.2999 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.9414\n",
      "Epoch 32: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2354 - accuracy: 0.9414 - val_loss: 0.2644 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2372 - accuracy: 0.9413\n",
      "Epoch 33: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2381 - accuracy: 0.9412 - val_loss: 0.2894 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2210 - accuracy: 0.9457\n",
      "Epoch 34: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2211 - accuracy: 0.9452 - val_loss: 0.2974 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2186 - accuracy: 0.9449\n",
      "Epoch 35: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2182 - accuracy: 0.9450 - val_loss: 0.3010 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2135 - accuracy: 0.9472\n",
      "Epoch 36: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2132 - accuracy: 0.9473 - val_loss: 0.3282 - val_accuracy: 0.8950 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2040 - accuracy: 0.9492\n",
      "Epoch 37: val_accuracy improved from 0.92466 to 0.92694, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.2036 - accuracy: 0.9493 - val_loss: 0.2743 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9428\n",
      "Epoch 38: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2245 - accuracy: 0.9430 - val_loss: 0.2699 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9497\n",
      "Epoch 39: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2134 - accuracy: 0.9498 - val_loss: 0.3085 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9533\n",
      "Epoch 40: val_accuracy improved from 0.92694 to 0.92922, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 34ms/step - loss: 0.1925 - accuracy: 0.9534 - val_loss: 0.3061 - val_accuracy: 0.9292 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9554\n",
      "Epoch 41: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.1820 - accuracy: 0.9551 - val_loss: 0.3149 - val_accuracy: 0.9224 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1788 - accuracy: 0.9555\n",
      "Epoch 42: val_accuracy improved from 0.92922 to 0.94064, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1785 - accuracy: 0.9556 - val_loss: 0.2963 - val_accuracy: 0.9406 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.9606\n",
      "Epoch 43: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1668 - accuracy: 0.9607 - val_loss: 0.3021 - val_accuracy: 0.9155 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1817 - accuracy: 0.9579\n",
      "Epoch 44: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1817 - accuracy: 0.9579 - val_loss: 0.3157 - val_accuracy: 0.9292 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9561\n",
      "Epoch 45: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1813 - accuracy: 0.9561 - val_loss: 0.3231 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.9572\n",
      "Epoch 46: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1764 - accuracy: 0.9572 - val_loss: 0.2962 - val_accuracy: 0.9361 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.9570\n",
      "Epoch 47: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1710 - accuracy: 0.9569 - val_loss: 0.3162 - val_accuracy: 0.9292 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9563\n",
      "Epoch 48: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1691 - accuracy: 0.9559 - val_loss: 0.3238 - val_accuracy: 0.9269 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.9637\n",
      "Epoch 49: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1554 - accuracy: 0.9638 - val_loss: 0.3281 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9649\n",
      "Epoch 50: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1561 - accuracy: 0.9648 - val_loss: 0.3169 - val_accuracy: 0.9338 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9635\n",
      "Epoch 51: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1569 - accuracy: 0.9635 - val_loss: 0.3430 - val_accuracy: 0.9269 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9667\n",
      "Epoch 52: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1497 - accuracy: 0.9665 - val_loss: 0.3461 - val_accuracy: 0.9315 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9649\n",
      "Epoch 53: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1527 - accuracy: 0.9648 - val_loss: 0.3480 - val_accuracy: 0.9292 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.9675\n",
      "Epoch 54: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1450 - accuracy: 0.9676 - val_loss: 0.3342 - val_accuracy: 0.9269 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1511 - accuracy: 0.9642\n",
      "Epoch 55: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1512 - accuracy: 0.9640 - val_loss: 0.3621 - val_accuracy: 0.9315 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1514 - accuracy: 0.9642\n",
      "Epoch 56: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1511 - accuracy: 0.9643 - val_loss: 0.3625 - val_accuracy: 0.9201 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9668\n",
      "Epoch 57: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1401 - accuracy: 0.9668 - val_loss: 0.3749 - val_accuracy: 0.9132 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9660\n",
      "Epoch 58: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1452 - accuracy: 0.9660 - val_loss: 0.3457 - val_accuracy: 0.9315 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9693\n",
      "Epoch 59: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1381 - accuracy: 0.9693 - val_loss: 0.3577 - val_accuracy: 0.9292 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.9682\n",
      "Epoch 60: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1429 - accuracy: 0.9683 - val_loss: 0.3564 - val_accuracy: 0.9269 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9670\n",
      "Epoch 61: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1449 - accuracy: 0.9670 - val_loss: 0.3545 - val_accuracy: 0.9201 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9690Restoring model weights from the end of the best epoch: 42.\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1358 - accuracy: 0.9691 - val_loss: 0.3621 - val_accuracy: 0.9224 - lr: 1.5625e-04\n",
      "Epoch 62: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0363 - accuracy: 0.8058\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26256, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 31ms/step - loss: 8.0363 - accuracy: 0.8058 - val_loss: 7.8193 - val_accuracy: 0.2626 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4906 - accuracy: 0.8676\n",
      "Epoch 2: val_accuracy did not improve from 0.26256\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4902 - accuracy: 0.8679 - val_loss: 7.4154 - val_accuracy: 0.2626 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4781 - accuracy: 0.8758\n",
      "Epoch 3: val_accuracy did not improve from 0.26256\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4767 - accuracy: 0.8760 - val_loss: 6.3438 - val_accuracy: 0.2626 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.8786\n",
      "Epoch 4: val_accuracy improved from 0.26256 to 0.38584, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.4715 - accuracy: 0.8786 - val_loss: 1.1476 - val_accuracy: 0.3858 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4595 - accuracy: 0.8859\n",
      "Epoch 5: val_accuracy improved from 0.38584 to 0.89269, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.4595 - accuracy: 0.8859 - val_loss: 0.3381 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4615 - accuracy: 0.8811\n",
      "Epoch 6: val_accuracy did not improve from 0.89269\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4614 - accuracy: 0.8811 - val_loss: 0.3568 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.8849\n",
      "Epoch 7: val_accuracy did not improve from 0.89269\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4478 - accuracy: 0.8847 - val_loss: 0.3218 - val_accuracy: 0.8881 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4555 - accuracy: 0.8793\n",
      "Epoch 8: val_accuracy did not improve from 0.89269\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4555 - accuracy: 0.8793 - val_loss: 0.5427 - val_accuracy: 0.7580 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8821\n",
      "Epoch 9: val_accuracy improved from 0.89269 to 0.90411, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.4440 - accuracy: 0.8821 - val_loss: 0.3054 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.9047\n",
      "Epoch 10: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3886 - accuracy: 0.9042 - val_loss: 0.3219 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3888 - accuracy: 0.9004\n",
      "Epoch 11: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3876 - accuracy: 0.9011 - val_loss: 0.2933 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3907 - accuracy: 0.9016\n",
      "Epoch 12: val_accuracy improved from 0.90411 to 0.91096, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3907 - accuracy: 0.9016 - val_loss: 0.2911 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3846 - accuracy: 0.9019\n",
      "Epoch 13: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3851 - accuracy: 0.9019 - val_loss: 0.3000 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.9012\n",
      "Epoch 14: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3785 - accuracy: 0.9011 - val_loss: 0.3464 - val_accuracy: 0.8950 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.9090\n",
      "Epoch 15: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3684 - accuracy: 0.9090 - val_loss: 0.2980 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.9035\n",
      "Epoch 16: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3611 - accuracy: 0.9037 - val_loss: 0.3020 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3552 - accuracy: 0.9070\n",
      "Epoch 17: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3555 - accuracy: 0.9067 - val_loss: 0.3525 - val_accuracy: 0.8813 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.9095\n",
      "Epoch 18: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3576 - accuracy: 0.9095 - val_loss: 0.3783 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.9103\n",
      "Epoch 19: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3740 - accuracy: 0.9105 - val_loss: 0.2919 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.9192\n",
      "Epoch 20: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3334 - accuracy: 0.9191 - val_loss: 0.2860 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.9273\n",
      "Epoch 21: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3052 - accuracy: 0.9267 - val_loss: 0.2783 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.9220\n",
      "Epoch 22: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3051 - accuracy: 0.9219 - val_loss: 0.3092 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.9222\n",
      "Epoch 23: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2994 - accuracy: 0.9222 - val_loss: 0.2842 - val_accuracy: 0.9018 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9293\n",
      "Epoch 24: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2840 - accuracy: 0.9290 - val_loss: 0.2786 - val_accuracy: 0.9018 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9249\n",
      "Epoch 25: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2886 - accuracy: 0.9250 - val_loss: 0.2713 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.9267\n",
      "Epoch 26: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2920 - accuracy: 0.9267 - val_loss: 0.3008 - val_accuracy: 0.9018 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.9244\n",
      "Epoch 27: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2981 - accuracy: 0.9245 - val_loss: 0.2922 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9247\n",
      "Epoch 28: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2905 - accuracy: 0.9247 - val_loss: 0.3163 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9267\n",
      "Epoch 29: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2757 - accuracy: 0.9267 - val_loss: 0.3291 - val_accuracy: 0.9041 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.9365\n",
      "Epoch 30: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2643 - accuracy: 0.9351 - val_loss: 0.3338 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.9395\n",
      "Epoch 31: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2490 - accuracy: 0.9399 - val_loss: 0.3129 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9412Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2425 - accuracy: 0.9412 - val_loss: 0.3311 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 32: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7436 - accuracy: 0.7845\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26027, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 34ms/step - loss: 7.7436 - accuracy: 0.7845 - val_loss: 2.3259 - val_accuracy: 0.2603 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5675 - accuracy: 0.8323\n",
      "Epoch 2: val_accuracy did not improve from 0.26027\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.5677 - accuracy: 0.8324 - val_loss: 4.1116 - val_accuracy: 0.2603 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.8342\n",
      "Epoch 3: val_accuracy did not improve from 0.26027\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5637 - accuracy: 0.8342 - val_loss: 2.0841 - val_accuracy: 0.2603 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.8430\n",
      "Epoch 4: val_accuracy improved from 0.26027 to 0.55479, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.5714 - accuracy: 0.8431 - val_loss: 0.7673 - val_accuracy: 0.5548 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5810 - accuracy: 0.8559\n",
      "Epoch 5: val_accuracy improved from 0.55479 to 0.72831, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.5799 - accuracy: 0.8563 - val_loss: 0.5789 - val_accuracy: 0.7283 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5545 - accuracy: 0.8659\n",
      "Epoch 6: val_accuracy improved from 0.72831 to 0.88813, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5545 - accuracy: 0.8657 - val_loss: 0.3241 - val_accuracy: 0.8881 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5450 - accuracy: 0.8679\n",
      "Epoch 7: val_accuracy improved from 0.88813 to 0.89726, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5460 - accuracy: 0.8674 - val_loss: 0.3770 - val_accuracy: 0.8973 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5522 - accuracy: 0.8704\n",
      "Epoch 8: val_accuracy did not improve from 0.89726\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5523 - accuracy: 0.8702 - val_loss: 0.3324 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5209 - accuracy: 0.8758\n",
      "Epoch 9: val_accuracy did not improve from 0.89726\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5207 - accuracy: 0.8758 - val_loss: 0.3397 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5076 - accuracy: 0.8788\n",
      "Epoch 10: val_accuracy did not improve from 0.89726\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.5076 - accuracy: 0.8783 - val_loss: 0.3065 - val_accuracy: 0.8790 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5076 - accuracy: 0.8831\n",
      "Epoch 11: val_accuracy improved from 0.89726 to 0.91553, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5077 - accuracy: 0.8831 - val_loss: 0.2879 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4773 - accuracy: 0.8883\n",
      "Epoch 12: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4782 - accuracy: 0.8882 - val_loss: 0.2811 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4659 - accuracy: 0.8859\n",
      "Epoch 13: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4653 - accuracy: 0.8862 - val_loss: 0.3096 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.8918\n",
      "Epoch 14: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4705 - accuracy: 0.8918 - val_loss: 0.3573 - val_accuracy: 0.8813 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4576 - accuracy: 0.8915\n",
      "Epoch 15: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4576 - accuracy: 0.8915 - val_loss: 0.2773 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4766 - accuracy: 0.8844\n",
      "Epoch 16: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4766 - accuracy: 0.8844 - val_loss: 0.3150 - val_accuracy: 0.8836 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4583 - accuracy: 0.8892\n",
      "Epoch 17: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4587 - accuracy: 0.8892 - val_loss: 0.2850 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4923 - accuracy: 0.8950\n",
      "Epoch 18: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4916 - accuracy: 0.8953 - val_loss: 0.4214 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4945 - accuracy: 0.8891\n",
      "Epoch 19: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.4940 - accuracy: 0.8892 - val_loss: 0.3204 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.8928\n",
      "Epoch 20: val_accuracy improved from 0.91553 to 0.92009, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.4801 - accuracy: 0.8928 - val_loss: 0.3093 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.8976\n",
      "Epoch 21: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4665 - accuracy: 0.8973 - val_loss: 0.3092 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4601 - accuracy: 0.9027\n",
      "Epoch 22: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4602 - accuracy: 0.9027 - val_loss: 0.2925 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4350 - accuracy: 0.9080\n",
      "Epoch 23: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4345 - accuracy: 0.9082 - val_loss: 0.2909 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4525 - accuracy: 0.9032\n",
      "Epoch 24: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4528 - accuracy: 0.9029 - val_loss: 0.2921 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4434 - accuracy: 0.9080\n",
      "Epoch 25: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4427 - accuracy: 0.9082 - val_loss: 0.3188 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.9042\n",
      "Epoch 26: val_accuracy improved from 0.92009 to 0.92237, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4435 - accuracy: 0.9047 - val_loss: 0.2910 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4371 - accuracy: 0.9085\n",
      "Epoch 27: val_accuracy improved from 0.92237 to 0.92466, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4364 - accuracy: 0.9087 - val_loss: 0.3018 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4576 - accuracy: 0.8966\n",
      "Epoch 28: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4573 - accuracy: 0.8966 - val_loss: 0.2951 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4238 - accuracy: 0.9121\n",
      "Epoch 29: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4242 - accuracy: 0.9120 - val_loss: 0.2733 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.9113\n",
      "Epoch 30: val_accuracy improved from 0.92466 to 0.92922, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4206 - accuracy: 0.9115 - val_loss: 0.2893 - val_accuracy: 0.9292 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3939 - accuracy: 0.9207\n",
      "Epoch 31: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 3s 27ms/step - loss: 0.3933 - accuracy: 0.9209 - val_loss: 0.2795 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4005 - accuracy: 0.9164\n",
      "Epoch 32: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4009 - accuracy: 0.9163 - val_loss: 0.2741 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.9144\n",
      "Epoch 33: val_accuracy improved from 0.92922 to 0.93151, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4078 - accuracy: 0.9143 - val_loss: 0.2621 - val_accuracy: 0.9315 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.9200\n",
      "Epoch 34: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3869 - accuracy: 0.9199 - val_loss: 0.2932 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4062 - accuracy: 0.9150\n",
      "Epoch 35: val_accuracy improved from 0.93151 to 0.93379, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4073 - accuracy: 0.9148 - val_loss: 0.2669 - val_accuracy: 0.9338 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.9198\n",
      "Epoch 36: val_accuracy did not improve from 0.93379\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3968 - accuracy: 0.9202 - val_loss: 0.2803 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.9190\n",
      "Epoch 37: val_accuracy did not improve from 0.93379\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3934 - accuracy: 0.9189 - val_loss: 0.2895 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3746 - accuracy: 0.9260\n",
      "Epoch 38: val_accuracy did not improve from 0.93379\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3746 - accuracy: 0.9260 - val_loss: 0.2654 - val_accuracy: 0.9338 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3818 - accuracy: 0.9220\n",
      "Epoch 39: val_accuracy did not improve from 0.93379\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3812 - accuracy: 0.9222 - val_loss: 0.2688 - val_accuracy: 0.9315 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.9253\n",
      "Epoch 40: val_accuracy improved from 0.93379 to 0.93607, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3853 - accuracy: 0.9255 - val_loss: 0.2561 - val_accuracy: 0.9361 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.9339\n",
      "Epoch 41: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3550 - accuracy: 0.9338 - val_loss: 0.2628 - val_accuracy: 0.9338 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3585 - accuracy: 0.9308\n",
      "Epoch 42: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3613 - accuracy: 0.9300 - val_loss: 0.2585 - val_accuracy: 0.9338 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.9268\n",
      "Epoch 43: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3626 - accuracy: 0.9270 - val_loss: 0.2654 - val_accuracy: 0.9315 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3529 - accuracy: 0.9299\n",
      "Epoch 44: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3525 - accuracy: 0.9300 - val_loss: 0.2600 - val_accuracy: 0.9315 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.9336\n",
      "Epoch 45: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3484 - accuracy: 0.9336 - val_loss: 0.2593 - val_accuracy: 0.9338 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3647 - accuracy: 0.9293\n",
      "Epoch 46: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3647 - accuracy: 0.9293 - val_loss: 0.2636 - val_accuracy: 0.9361 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.9347\n",
      "Epoch 47: val_accuracy improved from 0.93607 to 0.93836, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3500 - accuracy: 0.9346 - val_loss: 0.2592 - val_accuracy: 0.9384 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.9303\n",
      "Epoch 48: val_accuracy did not improve from 0.93836\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3648 - accuracy: 0.9303 - val_loss: 0.2690 - val_accuracy: 0.9361 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3469 - accuracy: 0.9356\n",
      "Epoch 49: val_accuracy did not improve from 0.93836\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3469 - accuracy: 0.9356 - val_loss: 0.2700 - val_accuracy: 0.9361 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.9392\n",
      "Epoch 50: val_accuracy improved from 0.93836 to 0.94064, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 0.3401 - accuracy: 0.9392 - val_loss: 0.2584 - val_accuracy: 0.9406 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.9329\n",
      "Epoch 51: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3480 - accuracy: 0.9331 - val_loss: 0.2559 - val_accuracy: 0.9384 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.9351\n",
      "Epoch 52: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3517 - accuracy: 0.9351 - val_loss: 0.2635 - val_accuracy: 0.9315 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3460 - accuracy: 0.9355\n",
      "Epoch 53: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3448 - accuracy: 0.9359 - val_loss: 0.2589 - val_accuracy: 0.9384 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.9334\n",
      "Epoch 54: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3408 - accuracy: 0.9333 - val_loss: 0.2607 - val_accuracy: 0.9361 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3356 - accuracy: 0.9370\n",
      "Epoch 55: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3361 - accuracy: 0.9369 - val_loss: 0.2590 - val_accuracy: 0.9361 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.9366\n",
      "Epoch 56: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3354 - accuracy: 0.9366 - val_loss: 0.2591 - val_accuracy: 0.9384 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.9384\n",
      "Epoch 57: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3278 - accuracy: 0.9384 - val_loss: 0.2544 - val_accuracy: 0.9361 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.9375\n",
      "Epoch 58: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3195 - accuracy: 0.9374 - val_loss: 0.2588 - val_accuracy: 0.9269 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.9372\n",
      "Epoch 59: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3212 - accuracy: 0.9371 - val_loss: 0.2492 - val_accuracy: 0.9338 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.9366\n",
      "Epoch 60: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3291 - accuracy: 0.9366 - val_loss: 0.2455 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.9440\n",
      "Epoch 61: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2935 - accuracy: 0.9440 - val_loss: 0.2392 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.9372\n",
      "Epoch 62: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3145 - accuracy: 0.9374 - val_loss: 0.2386 - val_accuracy: 0.9315 - lr: 1.5625e-04\n",
      "Epoch 63/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.9379\n",
      "Epoch 63: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3151 - accuracy: 0.9379 - val_loss: 0.2403 - val_accuracy: 0.9338 - lr: 1.5625e-04\n",
      "Epoch 64/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.9375\n",
      "Epoch 64: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3194 - accuracy: 0.9376 - val_loss: 0.2451 - val_accuracy: 0.9338 - lr: 1.5625e-04\n",
      "Epoch 65/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9424\n",
      "Epoch 65: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2920 - accuracy: 0.9427 - val_loss: 0.2411 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 66/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3180 - accuracy: 0.9369\n",
      "Epoch 66: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3180 - accuracy: 0.9369 - val_loss: 0.2372 - val_accuracy: 0.9338 - lr: 1.5625e-04\n",
      "Epoch 67/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.9393\n",
      "Epoch 67: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3095 - accuracy: 0.9397 - val_loss: 0.2396 - val_accuracy: 0.9315 - lr: 1.5625e-04\n",
      "Epoch 68/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.9408\n",
      "Epoch 68: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3074 - accuracy: 0.9412 - val_loss: 0.2361 - val_accuracy: 0.9315 - lr: 1.5625e-04\n",
      "Epoch 69/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.9412\n",
      "Epoch 69: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3048 - accuracy: 0.9412 - val_loss: 0.2371 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 70/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.9393Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3055 - accuracy: 0.9394 - val_loss: 0.2371 - val_accuracy: 0.9361 - lr: 7.8125e-05\n",
      "Epoch 70: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 8.0872 - accuracy: 0.8084\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26941, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 31ms/step - loss: 8.0074 - accuracy: 0.8094 - val_loss: 10.7685 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.8776\n",
      "Epoch 2: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4741 - accuracy: 0.8776 - val_loss: 9.3302 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy: 0.8806\n",
      "Epoch 3: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4570 - accuracy: 0.8801 - val_loss: 4.9126 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4374 - accuracy: 0.8918\n",
      "Epoch 4: val_accuracy improved from 0.26941 to 0.29452, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.4370 - accuracy: 0.8920 - val_loss: 3.0825 - val_accuracy: 0.2945 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4194 - accuracy: 0.8958\n",
      "Epoch 5: val_accuracy improved from 0.29452 to 0.90411, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4194 - accuracy: 0.8958 - val_loss: 0.2540 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4383 - accuracy: 0.8880\n",
      "Epoch 6: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4383 - accuracy: 0.8880 - val_loss: 0.2621 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4140 - accuracy: 0.8882\n",
      "Epoch 7: val_accuracy improved from 0.90411 to 0.90868, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4139 - accuracy: 0.8880 - val_loss: 0.2109 - val_accuracy: 0.9087 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.8940\n",
      "Epoch 8: val_accuracy improved from 0.90868 to 0.91096, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4242 - accuracy: 0.8935 - val_loss: 0.2849 - val_accuracy: 0.9110 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8934\n",
      "Epoch 9: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.4180 - accuracy: 0.8928 - val_loss: 0.2784 - val_accuracy: 0.8790 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3946 - accuracy: 0.9004\n",
      "Epoch 10: val_accuracy improved from 0.91096 to 0.91324, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3945 - accuracy: 0.9004 - val_loss: 0.2374 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.9027\n",
      "Epoch 11: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3703 - accuracy: 0.9027 - val_loss: 0.2300 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.9062\n",
      "Epoch 12: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3690 - accuracy: 0.9057 - val_loss: 0.2257 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.9101\n",
      "Epoch 13: val_accuracy improved from 0.91324 to 0.91781, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3530 - accuracy: 0.9100 - val_loss: 0.1989 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.9090\n",
      "Epoch 14: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3495 - accuracy: 0.9093 - val_loss: 0.2155 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.9136\n",
      "Epoch 15: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3365 - accuracy: 0.9136 - val_loss: 0.2174 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.9221\n",
      "Epoch 16: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3082 - accuracy: 0.9219 - val_loss: 0.2409 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.9197\n",
      "Epoch 17: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.3217 - accuracy: 0.9196 - val_loss: 0.2247 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.9172\n",
      "Epoch 18: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3137 - accuracy: 0.9171 - val_loss: 0.1967 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.9212\n",
      "Epoch 19: val_accuracy improved from 0.91781 to 0.92694, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3040 - accuracy: 0.9212 - val_loss: 0.1896 - val_accuracy: 0.9269 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.9293\n",
      "Epoch 20: val_accuracy improved from 0.92694 to 0.93151, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.2810 - accuracy: 0.9298 - val_loss: 0.1805 - val_accuracy: 0.9315 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.9327\n",
      "Epoch 21: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2720 - accuracy: 0.9323 - val_loss: 0.1938 - val_accuracy: 0.9315 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2536 - accuracy: 0.9347\n",
      "Epoch 22: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2544 - accuracy: 0.9346 - val_loss: 0.1990 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2458 - accuracy: 0.9408\n",
      "Epoch 23: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2458 - accuracy: 0.9407 - val_loss: 0.1926 - val_accuracy: 0.9269 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9376\n",
      "Epoch 24: val_accuracy improved from 0.93151 to 0.93607, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.2369 - accuracy: 0.9376 - val_loss: 0.1713 - val_accuracy: 0.9361 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.9380\n",
      "Epoch 25: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2518 - accuracy: 0.9381 - val_loss: 0.1773 - val_accuracy: 0.9269 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2336 - accuracy: 0.9420\n",
      "Epoch 26: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2336 - accuracy: 0.9420 - val_loss: 0.1955 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.9362\n",
      "Epoch 27: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2391 - accuracy: 0.9359 - val_loss: 0.1878 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9417\n",
      "Epoch 28: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2301 - accuracy: 0.9417 - val_loss: 0.1858 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9425\n",
      "Epoch 29: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2317 - accuracy: 0.9425 - val_loss: 0.1913 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9489\n",
      "Epoch 30: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2013 - accuracy: 0.9488 - val_loss: 0.1893 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9506\n",
      "Epoch 31: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.1870 - accuracy: 0.9506 - val_loss: 0.1885 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1809 - accuracy: 0.9569\n",
      "Epoch 32: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1809 - accuracy: 0.9569 - val_loss: 0.1869 - val_accuracy: 0.9361 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9526\n",
      "Epoch 33: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1856 - accuracy: 0.9526 - val_loss: 0.1960 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1761 - accuracy: 0.9510\n",
      "Epoch 34: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1759 - accuracy: 0.9511 - val_loss: 0.1883 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9518\n",
      "Epoch 35: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1805 - accuracy: 0.9516 - val_loss: 0.1800 - val_accuracy: 0.9338 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9545\n",
      "Epoch 36: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1732 - accuracy: 0.9544 - val_loss: 0.1970 - val_accuracy: 0.9315 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1863 - accuracy: 0.9531\n",
      "Epoch 37: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1863 - accuracy: 0.9531 - val_loss: 0.2290 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9558\n",
      "Epoch 38: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1829 - accuracy: 0.9559 - val_loss: 0.1953 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1730 - accuracy: 0.9515\n",
      "Epoch 39: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1728 - accuracy: 0.9516 - val_loss: 0.1876 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9639\n",
      "Epoch 40: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1392 - accuracy: 0.9640 - val_loss: 0.1933 - val_accuracy: 0.9292 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9596\n",
      "Epoch 41: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1534 - accuracy: 0.9594 - val_loss: 0.1994 - val_accuracy: 0.9292 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9605\n",
      "Epoch 42: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.1508 - accuracy: 0.9605 - val_loss: 0.2083 - val_accuracy: 0.9269 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.9626\n",
      "Epoch 43: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1465 - accuracy: 0.9622 - val_loss: 0.2010 - val_accuracy: 0.9224 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9632Restoring model weights from the end of the best epoch: 24.\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.1370 - accuracy: 0.9632 - val_loss: 0.2240 - val_accuracy: 0.9315 - lr: 6.2500e-04\n",
      "Epoch 44: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.9703 - accuracy: 0.8035\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23744, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 32ms/step - loss: 6.9703 - accuracy: 0.8035 - val_loss: 6.4038 - val_accuracy: 0.2374 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5271 - accuracy: 0.8689\n",
      "Epoch 2: val_accuracy did not improve from 0.23744\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5265 - accuracy: 0.8689 - val_loss: 7.4317 - val_accuracy: 0.2374 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.8700\n",
      "Epoch 3: val_accuracy did not improve from 0.23744\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5228 - accuracy: 0.8700 - val_loss: 2.5078 - val_accuracy: 0.2374 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5227 - accuracy: 0.8722\n",
      "Epoch 4: val_accuracy improved from 0.23744 to 0.87215, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.5271 - accuracy: 0.8710 - val_loss: 0.5268 - val_accuracy: 0.8721 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5281 - accuracy: 0.8791\n",
      "Epoch 5: val_accuracy improved from 0.87215 to 0.91096, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.5289 - accuracy: 0.8788 - val_loss: 0.3754 - val_accuracy: 0.9110 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5509 - accuracy: 0.8709\n",
      "Epoch 6: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5514 - accuracy: 0.8705 - val_loss: 0.3330 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5438 - accuracy: 0.8735\n",
      "Epoch 7: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5433 - accuracy: 0.8738 - val_loss: 0.3469 - val_accuracy: 0.9018 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5458 - accuracy: 0.8798\n",
      "Epoch 8: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5458 - accuracy: 0.8798 - val_loss: 0.3682 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.8773\n",
      "Epoch 9: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5348 - accuracy: 0.8773 - val_loss: 0.3233 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.8935\n",
      "Epoch 10: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4870 - accuracy: 0.8935 - val_loss: 0.3179 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4557 - accuracy: 0.8984\n",
      "Epoch 11: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4559 - accuracy: 0.8984 - val_loss: 0.3589 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4559 - accuracy: 0.8909\n",
      "Epoch 12: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4563 - accuracy: 0.8910 - val_loss: 0.3918 - val_accuracy: 0.8790 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.8902\n",
      "Epoch 13: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4606 - accuracy: 0.8897 - val_loss: 0.3750 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4548 - accuracy: 0.8946\n",
      "Epoch 14: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4552 - accuracy: 0.8946 - val_loss: 0.3174 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4392 - accuracy: 0.9011\n",
      "Epoch 15: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4375 - accuracy: 0.9019 - val_loss: 0.3625 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4406 - accuracy: 0.8953\n",
      "Epoch 16: val_accuracy improved from 0.91096 to 0.91553, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 0.4407 - accuracy: 0.8953 - val_loss: 0.3103 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4377 - accuracy: 0.9042\n",
      "Epoch 17: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 33ms/step - loss: 0.4383 - accuracy: 0.9039 - val_loss: 0.3465 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.8981\n",
      "Epoch 18: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4613 - accuracy: 0.8981 - val_loss: 0.3251 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4486 - accuracy: 0.8956\n",
      "Epoch 19: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4486 - accuracy: 0.8956 - val_loss: 0.3006 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3987 - accuracy: 0.9100\n",
      "Epoch 20: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 0.3987 - accuracy: 0.9100 - val_loss: 0.3190 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.9078\n",
      "Epoch 21: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 0.3974 - accuracy: 0.9075 - val_loss: 0.3267 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3934 - accuracy: 0.9108\n",
      "Epoch 22: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.3937 - accuracy: 0.9108 - val_loss: 0.3476 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3755 - accuracy: 0.9165\n",
      "Epoch 23: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3759 - accuracy: 0.9163 - val_loss: 0.3002 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3831 - accuracy: 0.9120\n",
      "Epoch 24: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3831 - accuracy: 0.9120 - val_loss: 0.4397 - val_accuracy: 0.8950 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.9189\n",
      "Epoch 25: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 0.3596 - accuracy: 0.9189 - val_loss: 0.3519 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.9205\n",
      "Epoch 26: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3627 - accuracy: 0.9207 - val_loss: 0.3428 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.9232\n",
      "Epoch 27: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3607 - accuracy: 0.9229 - val_loss: 0.3477 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.9167\n",
      "Epoch 28: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3785 - accuracy: 0.9166 - val_loss: 0.2861 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.9206\n",
      "Epoch 29: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3582 - accuracy: 0.9204 - val_loss: 0.3002 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.9289\n",
      "Epoch 30: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3308 - accuracy: 0.9290 - val_loss: 0.3436 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.9290\n",
      "Epoch 31: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3268 - accuracy: 0.9290 - val_loss: 0.3680 - val_accuracy: 0.8950 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3252 - accuracy: 0.9290\n",
      "Epoch 32: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3252 - accuracy: 0.9290 - val_loss: 0.3675 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3178 - accuracy: 0.9314\n",
      "Epoch 33: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3168 - accuracy: 0.9316 - val_loss: 0.3519 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.9311\n",
      "Epoch 34: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3234 - accuracy: 0.9313 - val_loss: 0.3957 - val_accuracy: 0.9041 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3248 - accuracy: 0.9342\n",
      "Epoch 35: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3242 - accuracy: 0.9343 - val_loss: 0.3597 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3211 - accuracy: 0.9299Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.3215 - accuracy: 0.9298 - val_loss: 0.4019 - val_accuracy: 0.8995 - lr: 0.0012\n",
      "Epoch 36: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 8.5097 - accuracy: 0.8102\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26941, saving model to best_model_480.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 5s 31ms/step - loss: 8.4266 - accuracy: 0.8109 - val_loss: 4.7920 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5115 - accuracy: 0.8416\n",
      "Epoch 2: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.5115 - accuracy: 0.8416 - val_loss: 4.5237 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5080 - accuracy: 0.8440\n",
      "Epoch 3: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5083 - accuracy: 0.8439 - val_loss: 3.1446 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4841 - accuracy: 0.8443\n",
      "Epoch 4: val_accuracy improved from 0.26941 to 0.42922, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.4843 - accuracy: 0.8441 - val_loss: 0.8563 - val_accuracy: 0.4292 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5150 - accuracy: 0.8645\n",
      "Epoch 5: val_accuracy improved from 0.42922 to 0.63470, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.5148 - accuracy: 0.8644 - val_loss: 0.8003 - val_accuracy: 0.6347 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.8639\n",
      "Epoch 6: val_accuracy improved from 0.63470 to 0.89269, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.5042 - accuracy: 0.8639 - val_loss: 0.3993 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.5285 - accuracy: 0.8701\n",
      "Epoch 7: val_accuracy did not improve from 0.89269\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5276 - accuracy: 0.8702 - val_loss: 0.3348 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4950 - accuracy: 0.8798\n",
      "Epoch 8: val_accuracy improved from 0.89269 to 0.90411, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.4950 - accuracy: 0.8798 - val_loss: 0.3607 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5139 - accuracy: 0.8709\n",
      "Epoch 9: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.5133 - accuracy: 0.8712 - val_loss: 0.3165 - val_accuracy: 0.8721 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4793 - accuracy: 0.8803\n",
      "Epoch 10: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4791 - accuracy: 0.8804 - val_loss: 0.3349 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.8845\n",
      "Epoch 11: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4784 - accuracy: 0.8844 - val_loss: 0.3007 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4467 - accuracy: 0.8924\n",
      "Epoch 12: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4475 - accuracy: 0.8918 - val_loss: 0.2818 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4456 - accuracy: 0.8892\n",
      "Epoch 13: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4450 - accuracy: 0.8895 - val_loss: 0.3719 - val_accuracy: 0.8950 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4233 - accuracy: 0.8952\n",
      "Epoch 14: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4230 - accuracy: 0.8953 - val_loss: 0.3016 - val_accuracy: 0.8950 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.8882\n",
      "Epoch 15: val_accuracy improved from 0.90411 to 0.90639, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 32ms/step - loss: 0.4429 - accuracy: 0.8882 - val_loss: 0.3061 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4360 - accuracy: 0.8919\n",
      "Epoch 16: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.4330 - accuracy: 0.8930 - val_loss: 0.3121 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4155 - accuracy: 0.8953\n",
      "Epoch 17: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4157 - accuracy: 0.8951 - val_loss: 0.2935 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4380 - accuracy: 0.8922\n",
      "Epoch 18: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.4350 - accuracy: 0.8930 - val_loss: 0.2893 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4329 - accuracy: 0.8938\n",
      "Epoch 19: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.4325 - accuracy: 0.8940 - val_loss: 0.2890 - val_accuracy: 0.8858 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3781 - accuracy: 0.9096\n",
      "Epoch 20: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3783 - accuracy: 0.9095 - val_loss: 0.3222 - val_accuracy: 0.8858 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.9024\n",
      "Epoch 21: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3866 - accuracy: 0.9024 - val_loss: 0.2969 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.9088\n",
      "Epoch 22: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3729 - accuracy: 0.9075 - val_loss: 0.4022 - val_accuracy: 0.8790 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3584 - accuracy: 0.9088\n",
      "Epoch 23: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3585 - accuracy: 0.9087 - val_loss: 0.3125 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.9077\n",
      "Epoch 24: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3629 - accuracy: 0.9077 - val_loss: 0.3223 - val_accuracy: 0.8927 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3561 - accuracy: 0.9080\n",
      "Epoch 25: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3561 - accuracy: 0.9080 - val_loss: 0.2865 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3669 - accuracy: 0.9019\n",
      "Epoch 26: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3669 - accuracy: 0.9019 - val_loss: 0.3240 - val_accuracy: 0.9041 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.9093\n",
      "Epoch 27: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3704 - accuracy: 0.9087 - val_loss: 0.2900 - val_accuracy: 0.8950 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.9149\n",
      "Epoch 28: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3469 - accuracy: 0.9148 - val_loss: 0.2992 - val_accuracy: 0.9018 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3531 - accuracy: 0.9085\n",
      "Epoch 29: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3534 - accuracy: 0.9082 - val_loss: 0.3021 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.9143\n",
      "Epoch 30: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.3308 - accuracy: 0.9143 - val_loss: 0.3512 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.9188\n",
      "Epoch 31: val_accuracy improved from 0.90639 to 0.91096, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.3267 - accuracy: 0.9184 - val_loss: 0.3366 - val_accuracy: 0.9110 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.9224\n",
      "Epoch 32: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3064 - accuracy: 0.9224 - val_loss: 0.3327 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.9170\n",
      "Epoch 33: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3206 - accuracy: 0.9171 - val_loss: 0.3498 - val_accuracy: 0.8973 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3192 - accuracy: 0.9136\n",
      "Epoch 34: val_accuracy improved from 0.91096 to 0.91324, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.3192 - accuracy: 0.9136 - val_loss: 0.3087 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.9255\n",
      "Epoch 35: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2953 - accuracy: 0.9252 - val_loss: 0.3440 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.9234\n",
      "Epoch 36: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3002 - accuracy: 0.9240 - val_loss: 0.3339 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.9150\n",
      "Epoch 37: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3095 - accuracy: 0.9148 - val_loss: 0.3437 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.9207\n",
      "Epoch 38: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2984 - accuracy: 0.9207 - val_loss: 0.3698 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.9187\n",
      "Epoch 39: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.3061 - accuracy: 0.9189 - val_loss: 0.3354 - val_accuracy: 0.8973 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9252\n",
      "Epoch 40: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2920 - accuracy: 0.9257 - val_loss: 0.3437 - val_accuracy: 0.8995 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.9308\n",
      "Epoch 41: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2796 - accuracy: 0.9308 - val_loss: 0.3572 - val_accuracy: 0.9064 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.9276\n",
      "Epoch 42: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2760 - accuracy: 0.9278 - val_loss: 0.3926 - val_accuracy: 0.9041 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2817 - accuracy: 0.9260\n",
      "Epoch 43: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2801 - accuracy: 0.9265 - val_loss: 0.3699 - val_accuracy: 0.9018 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2705 - accuracy: 0.9245\n",
      "Epoch 44: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2709 - accuracy: 0.9245 - val_loss: 0.3357 - val_accuracy: 0.8973 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2671 - accuracy: 0.9306\n",
      "Epoch 45: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2675 - accuracy: 0.9305 - val_loss: 0.3447 - val_accuracy: 0.9018 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2667 - accuracy: 0.9278\n",
      "Epoch 46: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.2677 - accuracy: 0.9275 - val_loss: 0.3606 - val_accuracy: 0.9110 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.9268\n",
      "Epoch 47: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2710 - accuracy: 0.9265 - val_loss: 0.3877 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2549 - accuracy: 0.9285\n",
      "Epoch 48: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2538 - accuracy: 0.9290 - val_loss: 0.3480 - val_accuracy: 0.9018 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2531 - accuracy: 0.9311\n",
      "Epoch 49: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2529 - accuracy: 0.9313 - val_loss: 0.3943 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.9357\n",
      "Epoch 50: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2516 - accuracy: 0.9359 - val_loss: 0.3914 - val_accuracy: 0.9087 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2432 - accuracy: 0.9319\n",
      "Epoch 51: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 30ms/step - loss: 0.2462 - accuracy: 0.9316 - val_loss: 0.3686 - val_accuracy: 0.9110 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9334\n",
      "Epoch 52: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2462 - accuracy: 0.9331 - val_loss: 0.3793 - val_accuracy: 0.9110 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9313\n",
      "Epoch 53: val_accuracy improved from 0.91324 to 0.91781, saving model to best_model_480.h5\n",
      "124/124 [==============================] - 4s 31ms/step - loss: 0.2494 - accuracy: 0.9313 - val_loss: 0.3760 - val_accuracy: 0.9178 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.9346\n",
      "Epoch 54: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2458 - accuracy: 0.9346 - val_loss: 0.4005 - val_accuracy: 0.9064 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2432 - accuracy: 0.9319\n",
      "Epoch 55: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2441 - accuracy: 0.9316 - val_loss: 0.3815 - val_accuracy: 0.9041 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2372 - accuracy: 0.9378\n",
      "Epoch 56: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2369 - accuracy: 0.9379 - val_loss: 0.3894 - val_accuracy: 0.9087 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.9305\n",
      "Epoch 57: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2540 - accuracy: 0.9305 - val_loss: 0.3891 - val_accuracy: 0.8995 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9383\n",
      "Epoch 58: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2315 - accuracy: 0.9381 - val_loss: 0.3910 - val_accuracy: 0.9132 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2409 - accuracy: 0.9337\n",
      "Epoch 59: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2410 - accuracy: 0.9331 - val_loss: 0.3996 - val_accuracy: 0.9064 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.9378\n",
      "Epoch 60: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2442 - accuracy: 0.9374 - val_loss: 0.3927 - val_accuracy: 0.9064 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9375\n",
      "Epoch 61: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2352 - accuracy: 0.9376 - val_loss: 0.3998 - val_accuracy: 0.9064 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.9385\n",
      "Epoch 62: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2298 - accuracy: 0.9384 - val_loss: 0.3933 - val_accuracy: 0.9087 - lr: 1.5625e-04\n",
      "Epoch 63/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2231 - accuracy: 0.9383\n",
      "Epoch 63: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2227 - accuracy: 0.9384 - val_loss: 0.4164 - val_accuracy: 0.9087 - lr: 1.5625e-04\n",
      "Epoch 64/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2288 - accuracy: 0.9339\n",
      "Epoch 64: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2286 - accuracy: 0.9341 - val_loss: 0.4123 - val_accuracy: 0.9110 - lr: 1.5625e-04\n",
      "Epoch 65/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9397\n",
      "Epoch 65: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2283 - accuracy: 0.9397 - val_loss: 0.4035 - val_accuracy: 0.9087 - lr: 1.5625e-04\n",
      "Epoch 66/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2254 - accuracy: 0.9390\n",
      "Epoch 66: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2250 - accuracy: 0.9392 - val_loss: 0.3964 - val_accuracy: 0.9041 - lr: 1.5625e-04\n",
      "Epoch 67/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.9357\n",
      "Epoch 67: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2354 - accuracy: 0.9359 - val_loss: 0.3982 - val_accuracy: 0.9087 - lr: 1.5625e-04\n",
      "Epoch 68/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9381\n",
      "Epoch 68: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 3s 28ms/step - loss: 0.2211 - accuracy: 0.9381 - val_loss: 0.4072 - val_accuracy: 0.9087 - lr: 1.5625e-04\n",
      "Epoch 69/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2255 - accuracy: 0.9413\n",
      "Epoch 69: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2258 - accuracy: 0.9412 - val_loss: 0.3838 - val_accuracy: 0.9064 - lr: 1.5625e-04\n",
      "Epoch 70/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9417\n",
      "Epoch 70: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2218 - accuracy: 0.9417 - val_loss: 0.4003 - val_accuracy: 0.9155 - lr: 7.8125e-05\n",
      "Epoch 71/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.9344\n",
      "Epoch 71: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2276 - accuracy: 0.9349 - val_loss: 0.3975 - val_accuracy: 0.9132 - lr: 7.8125e-05\n",
      "Epoch 72/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2203 - accuracy: 0.9388\n",
      "Epoch 72: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2207 - accuracy: 0.9387 - val_loss: 0.3979 - val_accuracy: 0.9132 - lr: 7.8125e-05\n",
      "Epoch 73/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2201 - accuracy: 0.9355Restoring model weights from the end of the best epoch: 53.\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 4s 29ms/step - loss: 0.2205 - accuracy: 0.9354 - val_loss: 0.4072 - val_accuracy: 0.9155 - lr: 7.8125e-05\n",
      "Epoch 73: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Average Accuracy: 0.9269858853142781\n",
      "Average Balanced Accuracy: 0.885105721918787\n",
      "Average Sensitivity (Sn): 0.7999822410978339\n",
      "Average Specificity (Sp): 0.9702292027397403\n",
      "Average MCC: 0.8030046532320221\n",
      "Average AUC: 0.944398561289252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import math\n",
    "\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "\n",
    "# Convert back to DataFrames if needed (assuming X_train is still a numpy array)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "# Result collection lists\n",
    "ACC_collection = []\n",
    "BACC_collection = []\n",
    "Sn_collection = []\n",
    "Sp_collection = []\n",
    "MCC_collection = []\n",
    "AUC_collection = []\n",
    "\n",
    "# Function to train the model for each fold\n",
    "def ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV):\n",
    "    # Train the model with the training fold\n",
    "    model = train_model(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
    "    return model\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(y_train):\n",
    "    X_train_CV, X_valid_CV = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_CV, y_valid_CV = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Train the model for this fold\n",
    "    model = ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
    "    \n",
    "    # Load the best model\n",
    "    saved_model = load_model('best_model_480.h5')\n",
    "    \n",
    "    # Predict probabilities\n",
    "    predicted_probabilities = saved_model.predict(X_valid_CV, batch_size=1)\n",
    "    \n",
    "    # Convert probabilities to class predictions\n",
    "    predicted_class = (predicted_probabilities > 0.5).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    y_true = y_valid_CV.values\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Sn = TP / (TP + FN)\n",
    "    Sp = TN / (TN + FP)\n",
    "    MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "    BACC = 0.5 * (Sn + Sp)\n",
    "    AUC = roc_auc_score(y_true, predicted_probabilities)\n",
    "    \n",
    "    # Append metrics to collection lists\n",
    "    ACC_collection.append(ACC)\n",
    "    Sn_collection.append(Sn)\n",
    "    Sp_collection.append(Sp)\n",
    "    MCC_collection.append(MCC)\n",
    "    BACC_collection.append(BACC)\n",
    "    AUC_collection.append(AUC)\n",
    "\n",
    "# Display the results for each fold\n",
    "print(\"Average Accuracy:\", np.mean(ACC_collection))\n",
    "print(\"Average Balanced Accuracy:\", np.mean(BACC_collection))\n",
    "print(\"Average Sensitivity (Sn):\", np.mean(Sn_collection))\n",
    "print(\"Average Specificity (Sp):\", np.mean(Sp_collection))\n",
    "print(\"Average MCC:\", np.mean(MCC_collection))\n",
    "print(\"Average AUC:\", np.mean(AUC_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e511ff0-55ce-4265-b9d0-e1f6c3aaccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Mean = 0.9269858853142781 , Std = 0.012611263722130587\n",
      "Balanced Accuracy: Mean = 0.885105721918787 , Std = 0.02909201008647401\n",
      "Sensitivity (Sn): Mean = 0.7999822410978339 , Std = 0.06272242087528321\n",
      "Specificity (Sp): Mean = 0.9702292027397403 , Std = 0.008385087470655537\n",
      "MCC: Mean = 0.8030046532320221 , Std = 0.03787945473188189\n",
      "AUC: Mean = 0.944398561289252 , Std = 0.016112717510871573\n"
     ]
    }
   ],
   "source": [
    "# Display the results for each fold with mean and standard deviation\n",
    "print(\"Accuracy: Mean =\", np.mean(ACC_collection), \", Std =\", np.std(ACC_collection))\n",
    "print(\"Balanced Accuracy: Mean =\", np.mean(BACC_collection), \", Std =\", np.std(BACC_collection))\n",
    "print(\"Sensitivity (Sn): Mean =\", np.mean(Sn_collection), \", Std =\", np.std(Sn_collection))\n",
    "print(\"Specificity (Sp): Mean =\", np.mean(Sp_collection), \", Std =\", np.std(Sp_collection))\n",
    "print(\"MCC: Mean =\", np.mean(MCC_collection), \", Std =\", np.std(MCC_collection))\n",
    "print(\"AUC: Mean =\", np.mean(AUC_collection), \", Std =\", np.std(AUC_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5141fb51-a6ef-4f76-91b2-ac2fb5a3d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 12ms/step\n",
      "\n",
      "Optimized Test Dataset Results:\n",
      "Accuracy (ACC): 0.9306569343065694\n",
      "Balanced Accuracy (BACC): 0.9072399781885983\n",
      "Sensitivity (Sn): 0.8597122302158273\n",
      "Specificity (Sp): 0.9547677261613692\n",
      "MCC: 0.8164281748282612\n",
      "AUC: 0.9072399781885981\n",
      "True Positives (TP): 239\n",
      "False Positives (FP): 37\n",
      "True Negatives (TN): 781\n",
      "False Negatives (FN): 39\n",
      "Total Positive: 278\n",
      "Total Negative: 818\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test dataset\n",
    "predicted_probas_test = saved_model.predict(X_test, batch_size=32)\n",
    "best_threshold_test, best_mcc_test = optimize_threshold(y_test, predicted_probas_test)\n",
    "predicted_classes_test = (predicted_probas_test > best_threshold_test).astype(int)\n",
    "\n",
    "# Calculate metrics for the test dataset with optimized threshold\n",
    "accuracy_test = accuracy_score(y_test, predicted_classes_test)\n",
    "sensitivity_test = recall_score(y_test, predicted_classes_test)  # Sensitivity (Recall)\n",
    "TN_test, FP_test, FN_test, TP_test = confusion_matrix(y_test, predicted_classes_test).ravel()\n",
    "specificity_test = TN_test / (TN_test + FP_test)  # Corrected Specificity calculation\n",
    "MCC_test = matthews_corrcoef(y_test, predicted_classes_test)\n",
    "auc_test = roc_auc_score(y_test, predicted_classes_test)\n",
    "\n",
    "# Compute the correct balanced accuracy\n",
    "balanced_accuracy_test = (sensitivity_test + specificity_test) / 2\n",
    "\n",
    "# Print the adjusted results for the test dataset\n",
    "print(\"\\nOptimized Test Dataset Results:\")\n",
    "print(f\"Accuracy (ACC): {accuracy_test}\")\n",
    "print(f\"Balanced Accuracy (BACC): {balanced_accuracy_test}\")\n",
    "print(f\"Sensitivity (Sn): {sensitivity_test}\")\n",
    "print(f\"Specificity (Sp): {specificity_test}\")\n",
    "print(f\"MCC: {MCC_test}\")\n",
    "print(f\"AUC: {auc_test}\")\n",
    "print(f\"True Positives (TP): {TP_test}\")\n",
    "print(f\"False Positives (FP): {FP_test}\")\n",
    "print(f\"True Negatives (TN): {TN_test}\")\n",
    "print(f\"False Negatives (FN): {FN_test}\")\n",
    "\n",
    "# Print the total positive and total negative\n",
    "total_positive = np.sum(y_test)\n",
    "total_negative = len(y_test) - total_positive\n",
    "print(f\"Total Positive: {total_positive}\")\n",
    "print(f\"Total Negative: {total_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0232f1-1e5f-4a1a-9c9c-46b4dbede579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 11ms/step\n",
      "\n",
      "Optimized External Dataset (KELM) Results:\n",
      "Accuracy (ACC): 0.8645833333333334\n",
      "Balanced Accuracy (BACC): 0.8645833333333334\n",
      "Sensitivity (Sn): 0.8020833333333334\n",
      "Specificity (Sp): 0.9270833333333334\n",
      "MCC: 0.7349309197401641\n",
      "AUC: 0.8645833333333335\n",
      "True Positives (TP): 77\n",
      "False Positives (FP): 7\n",
      "True Negatives (TN): 89\n",
      "False Negatives (FN): 19\n",
      "Total Positive: 96\n",
      "Total Negative: 96\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the external dataset (KELM)\n",
    "dataset_external = pd.read_csv('kelm_dataset.csv', na_filter=False)\n",
    "X_external_data_name = 'kelm_dataset_esm2_t12_35M_UR50D_unified_480_dimension.csv'\n",
    "X_external_data = pd.read_csv(X_external_data_name, header=0, index_col=0, delimiter=',')\n",
    "X_external = np.array(X_external_data)\n",
    "y_external = np.array(dataset_external['label'])\n",
    "\n",
    "# Normalize the external dataset\n",
    "X_external_normalized = scaler.transform(X_external)\n",
    "\n",
    "# Predict probabilities for external dataset\n",
    "predicted_probas_ext = saved_model.predict(X_external_normalized, batch_size=32)\n",
    "best_threshold_ext, best_mcc_ext = optimize_threshold(y_external, predicted_probas_ext)\n",
    "predicted_classes_ext = (predicted_probas_ext > best_threshold_ext).astype(int)\n",
    "\n",
    "# Calculate metrics for the external dataset with optimized threshold\n",
    "accuracy_ext = accuracy_score(y_external, predicted_classes_ext)\n",
    "sensitivity_ext = recall_score(y_external, predicted_classes_ext)  # Sensitivity (Recall)\n",
    "TN_ext, FP_ext, FN_ext, TP_ext = confusion_matrix(y_external, predicted_classes_ext).ravel()\n",
    "specificity_ext = TN_ext / (TN_ext + FP_ext)  # Corrected Specificity calculation\n",
    "MCC_ext = matthews_corrcoef(y_external, predicted_classes_ext)\n",
    "auc_ext = roc_auc_score(y_external, predicted_classes_ext)\n",
    "\n",
    "# Compute the correct balanced accuracy\n",
    "balanced_accuracy_ext = (sensitivity_ext + specificity_ext) / 2\n",
    "\n",
    "# Print the adjusted results for the external dataset\n",
    "print(\"\\nOptimized External Dataset (KELM) Results:\")\n",
    "print(f\"Accuracy (ACC): {accuracy_ext}\")\n",
    "print(f\"Balanced Accuracy (BACC): {balanced_accuracy_ext}\")\n",
    "print(f\"Sensitivity (Sn): {sensitivity_ext}\")\n",
    "print(f\"Specificity (Sp): {specificity_ext}\")\n",
    "print(f\"MCC: {MCC_ext}\")\n",
    "print(f\"AUC: {auc_ext}\")\n",
    "print(f\"True Positives (TP): {TP_ext}\")\n",
    "print(f\"False Positives (FP): {FP_ext}\")\n",
    "print(f\"True Negatives (TN): {TN_ext}\")\n",
    "print(f\"False Negatives (FN): {FN_ext}\")\n",
    "\n",
    "# Print the total positive and total negative\n",
    "total_positive_ext = np.sum(y_external)\n",
    "total_negative_ext = len(y_external) - total_positive_ext\n",
    "print(f\"Total Positive: {total_positive_ext}\")\n",
    "print(f\"Total Negative: {total_negative_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012045d6-990b-4b33-899d-b04f9e7af60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_true, y_pred_probas, title='ROC Curve'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_probas)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curve for the test dataset\n",
    "plot_roc_curve(y_test, predicted_probas_test, title='ROC Curve for Test Dataset')\n",
    "\n",
    "# Plot ROC curve for the external dataset\n",
    "plot_roc_curve(y_external, predicted_probas_ext, title='ROC Curve for External Dataset (KELM)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffa0556-1630-41e4-88c8-513ab1baedf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fpr_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Example saving for the test dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse Positive Rate\u001b[39m\u001b[38;5;124m'\u001b[39m: fpr_test,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Positive Rate\u001b[39m\u001b[38;5;124m'\u001b[39m: tpr_test,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROC AUC\u001b[39m\u001b[38;5;124m'\u001b[39m: [roc_auc_test] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(fpr_test)  \u001b[38;5;66;03m# AUC is constant across the curve\u001b[39;00m\n\u001b[0;32m      8\u001b[0m })\n\u001b[0;32m      9\u001b[0m df_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mESM-480_test_dataset_roc_values.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Example saving for the external dataset\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fpr_test' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example saving for the test dataset\n",
    "df_test = pd.DataFrame({\n",
    "    'False Positive Rate': fpr_test,\n",
    "    'True Positive Rate': tpr_test,\n",
    "    'ROC AUC': [roc_auc_test] * len(fpr_test)  # AUC is constant across the curve\n",
    "})\n",
    "df_test.to_csv('ESM-480_test_dataset_roc_values.csv', index=False)\n",
    "\n",
    "# Example saving for the external dataset\n",
    "df_ext = pd.DataFrame({\n",
    "    'False Positive Rate': fpr_ext,\n",
    "    'True Positive Rate': tpr_ext,\n",
    "    'ROC AUC': [roc_auc_ext] * len(fpr_ext)  # AUC is constant across the curve\n",
    "})\n",
    "df_ext.to_csv('ESM-480_external_dataset_roc_values.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fa1bc-2509-4415-b86f-4e2791b97d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfcb92b-e770-43aa-9868-ec0d32ee819a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
