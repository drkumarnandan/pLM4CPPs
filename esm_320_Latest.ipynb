{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6105a668-7319-45df-add9-f47fd3f90b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "135/137 [============================>.] - ETA: 0s - loss: 5.3555 - accuracy: 0.8153\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25365, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 4s 20ms/step - loss: 5.2847 - accuracy: 0.8166 - val_loss: 4.9741 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "  8/137 [>.............................] - ETA: 2s - loss: 0.5241 - accuracy: 0.8359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/137 [============================>.] - ETA: 0s - loss: 0.4768 - accuracy: 0.8750\n",
      "Epoch 2: val_accuracy did not improve from 0.25365\n",
      "137/137 [==============================] - 3s 18ms/step - loss: 0.4759 - accuracy: 0.8750 - val_loss: 3.5656 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.8759\n",
      "Epoch 3: val_accuracy did not improve from 0.25365\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.4795 - accuracy: 0.8759 - val_loss: 2.1860 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.4690 - accuracy: 0.8757\n",
      "Epoch 4: val_accuracy improved from 0.25365 to 0.40237, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.4681 - accuracy: 0.8759 - val_loss: 1.3882 - val_accuracy: 0.4024 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.4681 - accuracy: 0.8720\n",
      "Epoch 5: val_accuracy improved from 0.40237 to 0.64690, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.4663 - accuracy: 0.8725 - val_loss: 0.8126 - val_accuracy: 0.6469 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.8785\n",
      "Epoch 6: val_accuracy improved from 0.64690 to 0.89964, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.4566 - accuracy: 0.8777 - val_loss: 0.2974 - val_accuracy: 0.8996 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8852\n",
      "Epoch 7: val_accuracy did not improve from 0.89964\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.4440 - accuracy: 0.8852 - val_loss: 0.2924 - val_accuracy: 0.8768 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.8873\n",
      "Epoch 8: val_accuracy did not improve from 0.89964\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.4432 - accuracy: 0.8871 - val_loss: 0.3276 - val_accuracy: 0.8522 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.8778\n",
      "Epoch 9: val_accuracy did not improve from 0.89964\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.4476 - accuracy: 0.8775 - val_loss: 0.3023 - val_accuracy: 0.8704 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.4120 - accuracy: 0.8932\n",
      "Epoch 10: val_accuracy did not improve from 0.89964\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.4123 - accuracy: 0.8930 - val_loss: 0.2924 - val_accuracy: 0.8750 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3921 - accuracy: 0.9010\n",
      "Epoch 11: val_accuracy improved from 0.89964 to 0.90146, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.3927 - accuracy: 0.9010 - val_loss: 0.3077 - val_accuracy: 0.9015 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.9028\n",
      "Epoch 12: val_accuracy did not improve from 0.90146\n",
      "137/137 [==============================] - 3s 18ms/step - loss: 0.3751 - accuracy: 0.9037 - val_loss: 0.2802 - val_accuracy: 0.8987 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3741 - accuracy: 0.9033\n",
      "Epoch 13: val_accuracy did not improve from 0.90146\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.3740 - accuracy: 0.9033 - val_loss: 0.2902 - val_accuracy: 0.8841 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3763 - accuracy: 0.9037\n",
      "Epoch 14: val_accuracy did not improve from 0.90146\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.3764 - accuracy: 0.9035 - val_loss: 0.3178 - val_accuracy: 0.8978 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3849 - accuracy: 0.9037\n",
      "Epoch 15: val_accuracy did not improve from 0.90146\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.3827 - accuracy: 0.9044 - val_loss: 0.2853 - val_accuracy: 0.8978 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.9086\n",
      "Epoch 16: val_accuracy improved from 0.90146 to 0.90511, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.3814 - accuracy: 0.9074 - val_loss: 0.3203 - val_accuracy: 0.9051 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3608 - accuracy: 0.9116\n",
      "Epoch 17: val_accuracy did not improve from 0.90511\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.3683 - accuracy: 0.9115 - val_loss: 0.2827 - val_accuracy: 0.8850 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3717 - accuracy: 0.9056\n",
      "Epoch 18: val_accuracy did not improve from 0.90511\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.3701 - accuracy: 0.9060 - val_loss: 0.3267 - val_accuracy: 0.8631 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.9095\n",
      "Epoch 19: val_accuracy did not improve from 0.90511\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.3702 - accuracy: 0.9087 - val_loss: 0.2726 - val_accuracy: 0.8951 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3267 - accuracy: 0.9236\n",
      "Epoch 20: val_accuracy improved from 0.90511 to 0.90602, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.3284 - accuracy: 0.9224 - val_loss: 0.2852 - val_accuracy: 0.9060 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3146 - accuracy: 0.9237\n",
      "Epoch 21: val_accuracy did not improve from 0.90602\n",
      "137/137 [==============================] - 3s 18ms/step - loss: 0.3139 - accuracy: 0.9238 - val_loss: 0.2986 - val_accuracy: 0.9033 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3140 - accuracy: 0.9253\n",
      "Epoch 22: val_accuracy improved from 0.90602 to 0.91241, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.3147 - accuracy: 0.9247 - val_loss: 0.2922 - val_accuracy: 0.9124 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.9228\n",
      "Epoch 23: val_accuracy did not improve from 0.91241\n",
      "137/137 [==============================] - 3s 18ms/step - loss: 0.2993 - accuracy: 0.9227 - val_loss: 0.3003 - val_accuracy: 0.9097 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.9254\n",
      "Epoch 24: val_accuracy did not improve from 0.91241\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 0.3085 - accuracy: 0.9254 - val_loss: 0.2855 - val_accuracy: 0.9051 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.9259\n",
      "Epoch 25: val_accuracy did not improve from 0.91241\n",
      "137/137 [==============================] - 3s 18ms/step - loss: 0.3002 - accuracy: 0.9263 - val_loss: 0.3011 - val_accuracy: 0.9078 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9292\n",
      "Epoch 26: val_accuracy did not improve from 0.91241\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2867 - accuracy: 0.9293 - val_loss: 0.2821 - val_accuracy: 0.9097 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9293\n",
      "Epoch 27: val_accuracy did not improve from 0.91241\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2864 - accuracy: 0.9293 - val_loss: 0.2651 - val_accuracy: 0.9088 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9281\n",
      "Epoch 28: val_accuracy did not improve from 0.91241\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2861 - accuracy: 0.9281 - val_loss: 0.2973 - val_accuracy: 0.9042 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9287\n",
      "Epoch 29: val_accuracy improved from 0.91241 to 0.91423, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.2928 - accuracy: 0.9284 - val_loss: 0.3093 - val_accuracy: 0.9142 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.9352\n",
      "Epoch 30: val_accuracy did not improve from 0.91423\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2625 - accuracy: 0.9354 - val_loss: 0.3582 - val_accuracy: 0.9133 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2599 - accuracy: 0.9366\n",
      "Epoch 31: val_accuracy improved from 0.91423 to 0.91880, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.2587 - accuracy: 0.9370 - val_loss: 0.2948 - val_accuracy: 0.9188 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2488 - accuracy: 0.9387\n",
      "Epoch 32: val_accuracy did not improve from 0.91880\n",
      "137/137 [==============================] - 3s 18ms/step - loss: 0.2493 - accuracy: 0.9386 - val_loss: 0.3147 - val_accuracy: 0.9170 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9442\n",
      "Epoch 33: val_accuracy did not improve from 0.91880\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2355 - accuracy: 0.9443 - val_loss: 0.3476 - val_accuracy: 0.9133 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2393 - accuracy: 0.9428\n",
      "Epoch 34: val_accuracy did not improve from 0.91880\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2389 - accuracy: 0.9430 - val_loss: 0.2998 - val_accuracy: 0.9142 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.9405\n",
      "Epoch 35: val_accuracy improved from 0.91880 to 0.91971, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.2454 - accuracy: 0.9407 - val_loss: 0.3104 - val_accuracy: 0.9197 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.9439\n",
      "Epoch 36: val_accuracy did not improve from 0.91971\n",
      "137/137 [==============================] - 3s 18ms/step - loss: 0.2316 - accuracy: 0.9443 - val_loss: 0.3078 - val_accuracy: 0.9161 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9433\n",
      "Epoch 37: val_accuracy did not improve from 0.91971\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2381 - accuracy: 0.9432 - val_loss: 0.3166 - val_accuracy: 0.9179 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2299 - accuracy: 0.9468\n",
      "Epoch 38: val_accuracy did not improve from 0.91971\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2284 - accuracy: 0.9468 - val_loss: 0.3135 - val_accuracy: 0.9170 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2242 - accuracy: 0.9456\n",
      "Epoch 39: val_accuracy did not improve from 0.91971\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2264 - accuracy: 0.9455 - val_loss: 0.3198 - val_accuracy: 0.9188 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9535\n",
      "Epoch 40: val_accuracy improved from 0.91971 to 0.92153, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.2078 - accuracy: 0.9530 - val_loss: 0.3567 - val_accuracy: 0.9215 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2152 - accuracy: 0.9495\n",
      "Epoch 41: val_accuracy did not improve from 0.92153\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2143 - accuracy: 0.9496 - val_loss: 0.3448 - val_accuracy: 0.9206 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.1950 - accuracy: 0.9553\n",
      "Epoch 42: val_accuracy did not improve from 0.92153\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1955 - accuracy: 0.9553 - val_loss: 0.3459 - val_accuracy: 0.9197 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.9494\n",
      "Epoch 43: val_accuracy did not improve from 0.92153\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2164 - accuracy: 0.9491 - val_loss: 0.3881 - val_accuracy: 0.9151 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.9530\n",
      "Epoch 44: val_accuracy did not improve from 0.92153\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2045 - accuracy: 0.9530 - val_loss: 0.3623 - val_accuracy: 0.9206 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9512\n",
      "Epoch 45: val_accuracy did not improve from 0.92153\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2115 - accuracy: 0.9512 - val_loss: 0.3554 - val_accuracy: 0.9179 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9557\n",
      "Epoch 46: val_accuracy did not improve from 0.92153\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1966 - accuracy: 0.9560 - val_loss: 0.3564 - val_accuracy: 0.9215 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.1981 - accuracy: 0.9553\n",
      "Epoch 47: val_accuracy improved from 0.92153 to 0.92427, saving model to best_model_320.h5\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.1971 - accuracy: 0.9557 - val_loss: 0.3596 - val_accuracy: 0.9243 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9540\n",
      "Epoch 48: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.2025 - accuracy: 0.9537 - val_loss: 0.3651 - val_accuracy: 0.9197 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1927 - accuracy: 0.9552\n",
      "Epoch 49: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1938 - accuracy: 0.9551 - val_loss: 0.4235 - val_accuracy: 0.9161 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1906 - accuracy: 0.9566\n",
      "Epoch 50: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1906 - accuracy: 0.9567 - val_loss: 0.3716 - val_accuracy: 0.9234 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9610\n",
      "Epoch 51: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1759 - accuracy: 0.9610 - val_loss: 0.3641 - val_accuracy: 0.9243 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1867 - accuracy: 0.9568\n",
      "Epoch 52: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1867 - accuracy: 0.9567 - val_loss: 0.3990 - val_accuracy: 0.9215 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9591\n",
      "Epoch 53: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1847 - accuracy: 0.9589 - val_loss: 0.3795 - val_accuracy: 0.9243 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9580\n",
      "Epoch 54: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1906 - accuracy: 0.9580 - val_loss: 0.3802 - val_accuracy: 0.9215 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9597\n",
      "Epoch 55: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1822 - accuracy: 0.9596 - val_loss: 0.4031 - val_accuracy: 0.9206 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9617\n",
      "Epoch 56: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1718 - accuracy: 0.9617 - val_loss: 0.4043 - val_accuracy: 0.9234 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1772 - accuracy: 0.9596\n",
      "Epoch 57: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1772 - accuracy: 0.9596 - val_loss: 0.4271 - val_accuracy: 0.9243 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9616\n",
      "Epoch 58: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1837 - accuracy: 0.9619 - val_loss: 0.4034 - val_accuracy: 0.9206 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9569\n",
      "Epoch 59: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.1857 - accuracy: 0.9569 - val_loss: 0.4013 - val_accuracy: 0.9188 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9601\n",
      "Epoch 60: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.1723 - accuracy: 0.9601 - val_loss: 0.4252 - val_accuracy: 0.9206 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.1715 - accuracy: 0.9655\n",
      "Epoch 61: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.1717 - accuracy: 0.9658 - val_loss: 0.4221 - val_accuracy: 0.9197 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9626\n",
      "Epoch 62: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.1680 - accuracy: 0.9626 - val_loss: 0.4186 - val_accuracy: 0.9215 - lr: 1.5625e-04\n",
      "Epoch 63/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1728 - accuracy: 0.9609\n",
      "Epoch 63: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 22ms/step - loss: 0.1731 - accuracy: 0.9610 - val_loss: 0.4272 - val_accuracy: 0.9215 - lr: 1.5625e-04\n",
      "Epoch 64/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9590\n",
      "Epoch 64: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 0.1770 - accuracy: 0.9594 - val_loss: 0.4129 - val_accuracy: 0.9215 - lr: 1.5625e-04\n",
      "Epoch 65/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9614\n",
      "Epoch 65: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1673 - accuracy: 0.9614 - val_loss: 0.4103 - val_accuracy: 0.9234 - lr: 1.5625e-04\n",
      "Epoch 66/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9612\n",
      "Epoch 66: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.1726 - accuracy: 0.9612 - val_loss: 0.4254 - val_accuracy: 0.9234 - lr: 1.5625e-04\n",
      "Epoch 67/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9642Restoring model weights from the end of the best epoch: 47.\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.92427\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 0.1692 - accuracy: 0.9642 - val_loss: 0.4446 - val_accuracy: 0.9188 - lr: 1.5625e-04\n",
      "Epoch 67: early stopping\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_excel('Final_non_redundant_sequences.xlsx', na_filter=False)\n",
    "X_data_name = 'whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_data = pd.read_csv(X_data_name, header=0, index_col=0, delimiter=',')\n",
    "X = np.array(X_data)\n",
    "y = np.array(dataset['label'])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input = Input(input_shape)\n",
    "    x = Conv1D(64, 5, strides=1, padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv1D(128, 5, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr * np.power(drop, np.floor((1 + epoch) / epochs_drop))\n",
    "    return lr\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    input_shape = (320, 1)\n",
    "    model = build_model(input_shape)\n",
    "    \n",
    "    # Optimizer\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, restore_best_weights=True)\n",
    "    mc = ModelCheckpoint('best_model_320.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    callbacks_list = [lrate, early_stop, mc]\n",
    "    \n",
    "    class_weight = {0: 1, 1: 2}  # Adjust the weights as needed\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, callbacks=callbacks_list, batch_size=32, class_weight=class_weight)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Load the best model\n",
    "saved_model = load_model('best_model_320.h5')\n",
    "\n",
    "# Function to optimize threshold based on MCC\n",
    "def optimize_threshold(y_true, y_pred_probas):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    best_mcc = -1\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_probas > threshold).astype(int)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        \n",
    "        if mcc > best_mcc:\n",
    "            best_mcc = mcc\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cef867-1118-44d9-8433-b22212417815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 5.4320 - accuracy: 0.8147\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24601, saving model to best_model_320.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 3s 21ms/step - loss: 5.4320 - accuracy: 0.8147 - val_loss: 3.0115 - val_accuracy: 0.2460 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4590 - accuracy: 0.8758\n",
      "Epoch 2: val_accuracy did not improve from 0.24601\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4587 - accuracy: 0.8760 - val_loss: 4.0877 - val_accuracy: 0.2460 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.8768\n",
      "Epoch 3: val_accuracy did not improve from 0.24601\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4528 - accuracy: 0.8770 - val_loss: 4.1471 - val_accuracy: 0.2460 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4592 - accuracy: 0.8737\n",
      "Epoch 4: val_accuracy improved from 0.24601 to 0.24829, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.4591 - accuracy: 0.8742 - val_loss: 1.7433 - val_accuracy: 0.2483 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4517 - accuracy: 0.8770\n",
      "Epoch 5: val_accuracy improved from 0.24829 to 0.89749, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.4514 - accuracy: 0.8773 - val_loss: 0.2763 - val_accuracy: 0.8975 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4400 - accuracy: 0.8842\n",
      "Epoch 6: val_accuracy did not improve from 0.89749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4385 - accuracy: 0.8844 - val_loss: 0.3077 - val_accuracy: 0.8679 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8829\n",
      "Epoch 7: val_accuracy improved from 0.89749 to 0.90888, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4216 - accuracy: 0.8831 - val_loss: 0.2266 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4107 - accuracy: 0.8934\n",
      "Epoch 8: val_accuracy improved from 0.90888 to 0.91572, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4088 - accuracy: 0.8945 - val_loss: 0.2656 - val_accuracy: 0.9157 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8934\n",
      "Epoch 9: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3979 - accuracy: 0.8935 - val_loss: 0.2457 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3606 - accuracy: 0.9039\n",
      "Epoch 10: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3606 - accuracy: 0.9039 - val_loss: 0.2499 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.9086\n",
      "Epoch 11: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3443 - accuracy: 0.9090 - val_loss: 0.2619 - val_accuracy: 0.8975 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.9098\n",
      "Epoch 12: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3373 - accuracy: 0.9097 - val_loss: 0.2632 - val_accuracy: 0.9112 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3480 - accuracy: 0.9068\n",
      "Epoch 13: val_accuracy improved from 0.91572 to 0.91800, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3513 - accuracy: 0.9067 - val_loss: 0.2338 - val_accuracy: 0.9180 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.9111\n",
      "Epoch 14: val_accuracy improved from 0.91800 to 0.92027, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3377 - accuracy: 0.9113 - val_loss: 0.2299 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.9129\n",
      "Epoch 15: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3183 - accuracy: 0.9130 - val_loss: 0.2213 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.9137\n",
      "Epoch 16: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3119 - accuracy: 0.9135 - val_loss: 0.2606 - val_accuracy: 0.9066 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.9134\n",
      "Epoch 17: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3203 - accuracy: 0.9140 - val_loss: 0.2489 - val_accuracy: 0.9021 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.9226\n",
      "Epoch 18: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3125 - accuracy: 0.9224 - val_loss: 0.2658 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.9211\n",
      "Epoch 19: val_accuracy improved from 0.92027 to 0.92711, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2968 - accuracy: 0.9206 - val_loss: 0.2249 - val_accuracy: 0.9271 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.9318\n",
      "Epoch 20: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2669 - accuracy: 0.9318 - val_loss: 0.2378 - val_accuracy: 0.9203 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.9322\n",
      "Epoch 21: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2634 - accuracy: 0.9320 - val_loss: 0.2379 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.9305\n",
      "Epoch 22: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2544 - accuracy: 0.9305 - val_loss: 0.2488 - val_accuracy: 0.9112 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9380\n",
      "Epoch 23: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2379 - accuracy: 0.9379 - val_loss: 0.2767 - val_accuracy: 0.9043 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.9320\n",
      "Epoch 24: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2397 - accuracy: 0.9320 - val_loss: 0.2585 - val_accuracy: 0.9112 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.9388\n",
      "Epoch 25: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2389 - accuracy: 0.9389 - val_loss: 0.2414 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2289 - accuracy: 0.9357\n",
      "Epoch 26: val_accuracy improved from 0.92711 to 0.93166, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2291 - accuracy: 0.9356 - val_loss: 0.2385 - val_accuracy: 0.9317 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9447\n",
      "Epoch 27: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2161 - accuracy: 0.9445 - val_loss: 0.2811 - val_accuracy: 0.9134 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.9431\n",
      "Epoch 28: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2134 - accuracy: 0.9432 - val_loss: 0.2434 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2174 - accuracy: 0.9416\n",
      "Epoch 29: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2194 - accuracy: 0.9414 - val_loss: 0.2564 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9470\n",
      "Epoch 30: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2037 - accuracy: 0.9473 - val_loss: 0.2563 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9460\n",
      "Epoch 31: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1894 - accuracy: 0.9460 - val_loss: 0.2858 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.9517\n",
      "Epoch 32: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1814 - accuracy: 0.9518 - val_loss: 0.2994 - val_accuracy: 0.9203 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9540\n",
      "Epoch 33: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1840 - accuracy: 0.9539 - val_loss: 0.2858 - val_accuracy: 0.9157 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.9498\n",
      "Epoch 34: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1870 - accuracy: 0.9498 - val_loss: 0.3432 - val_accuracy: 0.9112 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9526\n",
      "Epoch 35: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1760 - accuracy: 0.9526 - val_loss: 0.3090 - val_accuracy: 0.9248 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1679 - accuracy: 0.9556\n",
      "Epoch 36: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1677 - accuracy: 0.9559 - val_loss: 0.2990 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9590\n",
      "Epoch 37: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1564 - accuracy: 0.9587 - val_loss: 0.3069 - val_accuracy: 0.9226 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9551\n",
      "Epoch 38: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1734 - accuracy: 0.9551 - val_loss: 0.2981 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1582 - accuracy: 0.9566\n",
      "Epoch 39: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1601 - accuracy: 0.9561 - val_loss: 0.3037 - val_accuracy: 0.9157 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1440 - accuracy: 0.9593\n",
      "Epoch 40: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1438 - accuracy: 0.9594 - val_loss: 0.3005 - val_accuracy: 0.9226 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9621\n",
      "Epoch 41: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1417 - accuracy: 0.9617 - val_loss: 0.3155 - val_accuracy: 0.9248 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.9606\n",
      "Epoch 42: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1402 - accuracy: 0.9604 - val_loss: 0.3602 - val_accuracy: 0.9112 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9636\n",
      "Epoch 43: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1433 - accuracy: 0.9642 - val_loss: 0.3319 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1342 - accuracy: 0.9647\n",
      "Epoch 44: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1344 - accuracy: 0.9645 - val_loss: 0.3452 - val_accuracy: 0.9157 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.9620\n",
      "Epoch 45: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1377 - accuracy: 0.9620 - val_loss: 0.3329 - val_accuracy: 0.9134 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9618Restoring model weights from the end of the best epoch: 26.\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1354 - accuracy: 0.9620 - val_loss: 0.3356 - val_accuracy: 0.9226 - lr: 6.2500e-04\n",
      "Epoch 46: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 4.4206 - accuracy: 0.7908\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26424, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 4.3528 - accuracy: 0.7893 - val_loss: 2.5047 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "  1/124 [..............................] - ETA: 2s - loss: 0.3758 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/124 [============================>.] - ETA: 0s - loss: 0.5432 - accuracy: 0.8388\n",
      "Epoch 2: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.5432 - accuracy: 0.8395 - val_loss: 1.9420 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.5098 - accuracy: 0.8745\n",
      "Epoch 3: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.5083 - accuracy: 0.8750 - val_loss: 2.7231 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.5089 - accuracy: 0.8696\n",
      "Epoch 4: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.5082 - accuracy: 0.8697 - val_loss: 1.3453 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.8730\n",
      "Epoch 5: val_accuracy improved from 0.26424 to 0.76310, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.5056 - accuracy: 0.8730 - val_loss: 0.4908 - val_accuracy: 0.7631 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4880 - accuracy: 0.8776\n",
      "Epoch 6: val_accuracy improved from 0.76310 to 0.88383, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4873 - accuracy: 0.8773 - val_loss: 0.3035 - val_accuracy: 0.8838 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4983 - accuracy: 0.8765\n",
      "Epoch 7: val_accuracy did not improve from 0.88383\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4977 - accuracy: 0.8765 - val_loss: 0.2949 - val_accuracy: 0.8702 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4886 - accuracy: 0.8742\n",
      "Epoch 8: val_accuracy improved from 0.88383 to 0.89294, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4900 - accuracy: 0.8737 - val_loss: 0.4001 - val_accuracy: 0.8929 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4924 - accuracy: 0.8778\n",
      "Epoch 9: val_accuracy did not improve from 0.89294\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4922 - accuracy: 0.8778 - val_loss: 0.3862 - val_accuracy: 0.8884 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4685 - accuracy: 0.8862\n",
      "Epoch 10: val_accuracy improved from 0.89294 to 0.90888, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4685 - accuracy: 0.8862 - val_loss: 0.3067 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4343 - accuracy: 0.8958\n",
      "Epoch 11: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4339 - accuracy: 0.8960 - val_loss: 0.2914 - val_accuracy: 0.8998 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4347 - accuracy: 0.8976\n",
      "Epoch 12: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4358 - accuracy: 0.8976 - val_loss: 0.2734 - val_accuracy: 0.9021 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4434 - accuracy: 0.8958\n",
      "Epoch 13: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4442 - accuracy: 0.8955 - val_loss: 0.3670 - val_accuracy: 0.8815 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4249 - accuracy: 0.9004\n",
      "Epoch 14: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4240 - accuracy: 0.9006 - val_loss: 0.2743 - val_accuracy: 0.8793 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.9019\n",
      "Epoch 15: val_accuracy improved from 0.90888 to 0.91116, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4222 - accuracy: 0.9019 - val_loss: 0.2765 - val_accuracy: 0.9112 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.9068\n",
      "Epoch 16: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4309 - accuracy: 0.9077 - val_loss: 0.2837 - val_accuracy: 0.8838 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4129 - accuracy: 0.9055\n",
      "Epoch 17: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4124 - accuracy: 0.9057 - val_loss: 0.3005 - val_accuracy: 0.8656 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.9004\n",
      "Epoch 18: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4223 - accuracy: 0.9006 - val_loss: 0.2378 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4071 - accuracy: 0.9013\n",
      "Epoch 19: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4060 - accuracy: 0.9016 - val_loss: 0.2417 - val_accuracy: 0.9043 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3743 - accuracy: 0.9157\n",
      "Epoch 20: val_accuracy did not improve from 0.91116\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3761 - accuracy: 0.9153 - val_loss: 0.2512 - val_accuracy: 0.8952 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.9182\n",
      "Epoch 21: val_accuracy improved from 0.91116 to 0.91344, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3650 - accuracy: 0.9181 - val_loss: 0.2381 - val_accuracy: 0.9134 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.9150\n",
      "Epoch 22: val_accuracy did not improve from 0.91344\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3739 - accuracy: 0.9151 - val_loss: 0.2348 - val_accuracy: 0.9089 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3523 - accuracy: 0.9171\n",
      "Epoch 23: val_accuracy did not improve from 0.91344\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3573 - accuracy: 0.9158 - val_loss: 0.2386 - val_accuracy: 0.9112 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3628 - accuracy: 0.9150\n",
      "Epoch 24: val_accuracy improved from 0.91344 to 0.91572, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3647 - accuracy: 0.9146 - val_loss: 0.2548 - val_accuracy: 0.9157 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.9186\n",
      "Epoch 25: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3382 - accuracy: 0.9186 - val_loss: 0.2384 - val_accuracy: 0.9112 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.9212\n",
      "Epoch 26: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3406 - accuracy: 0.9211 - val_loss: 0.2612 - val_accuracy: 0.9134 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.9262\n",
      "Epoch 27: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3184 - accuracy: 0.9262 - val_loss: 0.2360 - val_accuracy: 0.9089 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.9164\n",
      "Epoch 28: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3373 - accuracy: 0.9166 - val_loss: 0.2238 - val_accuracy: 0.9112 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3398 - accuracy: 0.9192\n",
      "Epoch 29: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3396 - accuracy: 0.9194 - val_loss: 0.2600 - val_accuracy: 0.9043 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3183 - accuracy: 0.9249\n",
      "Epoch 30: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3172 - accuracy: 0.9252 - val_loss: 0.2255 - val_accuracy: 0.9157 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.9313\n",
      "Epoch 31: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2964 - accuracy: 0.9308 - val_loss: 0.2378 - val_accuracy: 0.9157 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.9293\n",
      "Epoch 32: val_accuracy did not improve from 0.91572\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2939 - accuracy: 0.9290 - val_loss: 0.2516 - val_accuracy: 0.9157 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9311\n",
      "Epoch 33: val_accuracy improved from 0.91572 to 0.92483, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2942 - accuracy: 0.9298 - val_loss: 0.2495 - val_accuracy: 0.9248 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.9357\n",
      "Epoch 34: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2761 - accuracy: 0.9356 - val_loss: 0.2454 - val_accuracy: 0.9043 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9310\n",
      "Epoch 35: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2906 - accuracy: 0.9310 - val_loss: 0.2377 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9366\n",
      "Epoch 36: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2863 - accuracy: 0.9366 - val_loss: 0.2808 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.9326\n",
      "Epoch 37: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2983 - accuracy: 0.9323 - val_loss: 0.2480 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2624 - accuracy: 0.9428\n",
      "Epoch 38: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2632 - accuracy: 0.9424 - val_loss: 0.2482 - val_accuracy: 0.9021 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9326\n",
      "Epoch 39: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2925 - accuracy: 0.9326 - val_loss: 0.2496 - val_accuracy: 0.9157 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2733 - accuracy: 0.9414\n",
      "Epoch 40: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2723 - accuracy: 0.9417 - val_loss: 0.2395 - val_accuracy: 0.9180 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2626 - accuracy: 0.9411\n",
      "Epoch 41: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2626 - accuracy: 0.9414 - val_loss: 0.2490 - val_accuracy: 0.9157 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2651 - accuracy: 0.9432\n",
      "Epoch 42: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2637 - accuracy: 0.9440 - val_loss: 0.2455 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.9440\n",
      "Epoch 43: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2558 - accuracy: 0.9440 - val_loss: 0.2396 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2564 - accuracy: 0.9460\n",
      "Epoch 44: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2570 - accuracy: 0.9455 - val_loss: 0.2498 - val_accuracy: 0.9180 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9444\n",
      "Epoch 45: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2449 - accuracy: 0.9442 - val_loss: 0.2505 - val_accuracy: 0.9226 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9419\n",
      "Epoch 46: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2533 - accuracy: 0.9419 - val_loss: 0.2553 - val_accuracy: 0.9180 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.9501\n",
      "Epoch 47: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2391 - accuracy: 0.9501 - val_loss: 0.2556 - val_accuracy: 0.9226 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.9436\n",
      "Epoch 48: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2551 - accuracy: 0.9437 - val_loss: 0.2551 - val_accuracy: 0.9180 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9526\n",
      "Epoch 49: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2287 - accuracy: 0.9526 - val_loss: 0.2673 - val_accuracy: 0.9226 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2454 - accuracy: 0.9437\n",
      "Epoch 50: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2441 - accuracy: 0.9442 - val_loss: 0.2492 - val_accuracy: 0.9180 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2288 - accuracy: 0.9509\n",
      "Epoch 51: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2285 - accuracy: 0.9508 - val_loss: 0.2564 - val_accuracy: 0.9180 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.9481\n",
      "Epoch 52: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2334 - accuracy: 0.9483 - val_loss: 0.2666 - val_accuracy: 0.9180 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9444Restoring model weights from the end of the best epoch: 33.\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2443 - accuracy: 0.9442 - val_loss: 0.2657 - val_accuracy: 0.9180 - lr: 3.1250e-04\n",
      "Epoch 53: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 4.9164 - accuracy: 0.8110\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26424, saving model to best_model_320.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 3s 21ms/step - loss: 4.8708 - accuracy: 0.8116 - val_loss: 3.1184 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4942 - accuracy: 0.8484\n",
      "Epoch 2: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4922 - accuracy: 0.8491 - val_loss: 5.4101 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.8538\n",
      "Epoch 3: val_accuracy did not improve from 0.26424\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4795 - accuracy: 0.8522 - val_loss: 1.8094 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5030 - accuracy: 0.8323\n",
      "Epoch 4: val_accuracy improved from 0.26424 to 0.83144, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.5022 - accuracy: 0.8327 - val_loss: 0.4037 - val_accuracy: 0.8314 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.5007 - accuracy: 0.8745\n",
      "Epoch 5: val_accuracy improved from 0.83144 to 0.90205, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4987 - accuracy: 0.8740 - val_loss: 0.2652 - val_accuracy: 0.9021 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4796 - accuracy: 0.8735\n",
      "Epoch 6: val_accuracy did not improve from 0.90205\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4788 - accuracy: 0.8737 - val_loss: 0.2654 - val_accuracy: 0.8861 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.8804\n",
      "Epoch 7: val_accuracy did not improve from 0.90205\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4684 - accuracy: 0.8803 - val_loss: 0.3367 - val_accuracy: 0.8998 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4766 - accuracy: 0.8750\n",
      "Epoch 8: val_accuracy did not improve from 0.90205\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4766 - accuracy: 0.8750 - val_loss: 0.3015 - val_accuracy: 0.8952 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4655 - accuracy: 0.8725\n",
      "Epoch 9: val_accuracy did not improve from 0.90205\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4658 - accuracy: 0.8725 - val_loss: 0.2857 - val_accuracy: 0.8929 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8908\n",
      "Epoch 10: val_accuracy improved from 0.90205 to 0.90888, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4209 - accuracy: 0.8907 - val_loss: 0.2612 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8960\n",
      "Epoch 11: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3973 - accuracy: 0.8960 - val_loss: 0.2447 - val_accuracy: 0.9066 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8925\n",
      "Epoch 12: val_accuracy improved from 0.90888 to 0.91572, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.3956 - accuracy: 0.8925 - val_loss: 0.2594 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3947 - accuracy: 0.8965\n",
      "Epoch 13: val_accuracy improved from 0.91572 to 0.92027, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.3939 - accuracy: 0.8966 - val_loss: 0.2547 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3758 - accuracy: 0.8991\n",
      "Epoch 14: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3758 - accuracy: 0.8991 - val_loss: 0.2400 - val_accuracy: 0.9066 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.8993\n",
      "Epoch 15: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3963 - accuracy: 0.8993 - val_loss: 0.2502 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3900 - accuracy: 0.8965\n",
      "Epoch 16: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3903 - accuracy: 0.8968 - val_loss: 0.2653 - val_accuracy: 0.9134 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3795 - accuracy: 0.8928\n",
      "Epoch 17: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3790 - accuracy: 0.8930 - val_loss: 0.2544 - val_accuracy: 0.9021 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3772 - accuracy: 0.9014\n",
      "Epoch 18: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3760 - accuracy: 0.9019 - val_loss: 0.2600 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.9052\n",
      "Epoch 19: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3576 - accuracy: 0.9052 - val_loss: 0.2598 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3429 - accuracy: 0.9070\n",
      "Epoch 20: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3428 - accuracy: 0.9069 - val_loss: 0.2585 - val_accuracy: 0.9203 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3350 - accuracy: 0.9134\n",
      "Epoch 21: val_accuracy improved from 0.92027 to 0.92255, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3360 - accuracy: 0.9133 - val_loss: 0.2515 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3249 - accuracy: 0.9153\n",
      "Epoch 22: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3249 - accuracy: 0.9153 - val_loss: 0.2533 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.9144\n",
      "Epoch 23: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3182 - accuracy: 0.9146 - val_loss: 0.2482 - val_accuracy: 0.9203 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.9090\n",
      "Epoch 24: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3266 - accuracy: 0.9092 - val_loss: 0.2693 - val_accuracy: 0.9203 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.9179\n",
      "Epoch 25: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3042 - accuracy: 0.9186 - val_loss: 0.2721 - val_accuracy: 0.9089 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.9146\n",
      "Epoch 26: val_accuracy did not improve from 0.92255\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3181 - accuracy: 0.9143 - val_loss: 0.2597 - val_accuracy: 0.9157 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.9219\n",
      "Epoch 27: val_accuracy improved from 0.92255 to 0.92483, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3029 - accuracy: 0.9219 - val_loss: 0.3031 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.9221\n",
      "Epoch 28: val_accuracy improved from 0.92483 to 0.92711, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3113 - accuracy: 0.9222 - val_loss: 0.2499 - val_accuracy: 0.9271 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.9159\n",
      "Epoch 29: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3072 - accuracy: 0.9156 - val_loss: 0.2582 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.9267\n",
      "Epoch 30: val_accuracy did not improve from 0.92711\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2884 - accuracy: 0.9272 - val_loss: 0.2715 - val_accuracy: 0.9271 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9261\n",
      "Epoch 31: val_accuracy improved from 0.92711 to 0.93622, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2611 - accuracy: 0.9262 - val_loss: 0.2504 - val_accuracy: 0.9362 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.9310\n",
      "Epoch 32: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2622 - accuracy: 0.9305 - val_loss: 0.2587 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9257\n",
      "Epoch 33: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2893 - accuracy: 0.9257 - val_loss: 0.2570 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2741 - accuracy: 0.9292\n",
      "Epoch 34: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2738 - accuracy: 0.9293 - val_loss: 0.2711 - val_accuracy: 0.9248 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.9331\n",
      "Epoch 35: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2679 - accuracy: 0.9326 - val_loss: 0.3535 - val_accuracy: 0.9066 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2553 - accuracy: 0.9313\n",
      "Epoch 36: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2551 - accuracy: 0.9318 - val_loss: 0.2905 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2486 - accuracy: 0.9344\n",
      "Epoch 37: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2497 - accuracy: 0.9338 - val_loss: 0.2653 - val_accuracy: 0.9317 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9345\n",
      "Epoch 38: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2449 - accuracy: 0.9346 - val_loss: 0.2683 - val_accuracy: 0.9226 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2406 - accuracy: 0.9362\n",
      "Epoch 39: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2429 - accuracy: 0.9348 - val_loss: 0.2959 - val_accuracy: 0.9248 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.9385\n",
      "Epoch 40: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2342 - accuracy: 0.9386 - val_loss: 0.2838 - val_accuracy: 0.9339 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9403\n",
      "Epoch 41: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2232 - accuracy: 0.9407 - val_loss: 0.3284 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9389\n",
      "Epoch 42: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2276 - accuracy: 0.9389 - val_loss: 0.2827 - val_accuracy: 0.9339 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2325 - accuracy: 0.9403\n",
      "Epoch 43: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2307 - accuracy: 0.9409 - val_loss: 0.3076 - val_accuracy: 0.9294 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.9406\n",
      "Epoch 44: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2211 - accuracy: 0.9402 - val_loss: 0.3055 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9455\n",
      "Epoch 45: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2187 - accuracy: 0.9452 - val_loss: 0.3189 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2193 - accuracy: 0.9398\n",
      "Epoch 46: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2191 - accuracy: 0.9399 - val_loss: 0.2929 - val_accuracy: 0.9339 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2120 - accuracy: 0.9444\n",
      "Epoch 47: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 3s 22ms/step - loss: 0.2112 - accuracy: 0.9447 - val_loss: 0.3111 - val_accuracy: 0.9271 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9431\n",
      "Epoch 48: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2044 - accuracy: 0.9435 - val_loss: 0.3129 - val_accuracy: 0.9294 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9475\n",
      "Epoch 49: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.2028 - accuracy: 0.9473 - val_loss: 0.3274 - val_accuracy: 0.9226 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9468\n",
      "Epoch 50: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2133 - accuracy: 0.9468 - val_loss: 0.3283 - val_accuracy: 0.9317 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9451Restoring model weights from the end of the best epoch: 31.\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.93622\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2058 - accuracy: 0.9452 - val_loss: 0.3182 - val_accuracy: 0.9294 - lr: 3.1250e-04\n",
      "Epoch 51: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 6.1868 - accuracy: 0.8204\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25799, saving model to best_model_320.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 3s 21ms/step - loss: 6.1297 - accuracy: 0.8208 - val_loss: 0.8740 - val_accuracy: 0.2580 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4559 - accuracy: 0.8842\n",
      "Epoch 2: val_accuracy did not improve from 0.25799\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4538 - accuracy: 0.8849 - val_loss: 1.8604 - val_accuracy: 0.2443 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.8846\n",
      "Epoch 3: val_accuracy did not improve from 0.25799\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4328 - accuracy: 0.8857 - val_loss: 2.6062 - val_accuracy: 0.2443 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8824\n",
      "Epoch 4: val_accuracy did not improve from 0.25799\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4266 - accuracy: 0.8826 - val_loss: 3.5831 - val_accuracy: 0.2443 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4094 - accuracy: 0.8892\n",
      "Epoch 5: val_accuracy improved from 0.25799 to 0.76256, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4093 - accuracy: 0.8892 - val_loss: 0.4677 - val_accuracy: 0.7626 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3994 - accuracy: 0.8864\n",
      "Epoch 6: val_accuracy improved from 0.76256 to 0.89269, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4007 - accuracy: 0.8857 - val_loss: 0.2736 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3921 - accuracy: 0.8930\n",
      "Epoch 7: val_accuracy improved from 0.89269 to 0.89954, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3918 - accuracy: 0.8933 - val_loss: 0.2457 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8892\n",
      "Epoch 8: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3913 - accuracy: 0.8895 - val_loss: 0.2730 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3688 - accuracy: 0.8984\n",
      "Epoch 9: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3684 - accuracy: 0.8986 - val_loss: 0.2737 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.9114\n",
      "Epoch 10: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3444 - accuracy: 0.9115 - val_loss: 0.2476 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.9096\n",
      "Epoch 11: val_accuracy improved from 0.89954 to 0.90868, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3321 - accuracy: 0.9103 - val_loss: 0.2438 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3314 - accuracy: 0.9124\n",
      "Epoch 12: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3295 - accuracy: 0.9125 - val_loss: 0.2414 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.9137\n",
      "Epoch 13: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3187 - accuracy: 0.9141 - val_loss: 0.2639 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.9144\n",
      "Epoch 14: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3086 - accuracy: 0.9138 - val_loss: 0.2572 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.9128\n",
      "Epoch 15: val_accuracy improved from 0.90868 to 0.91324, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3019 - accuracy: 0.9128 - val_loss: 0.2540 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3021 - accuracy: 0.9184\n",
      "Epoch 16: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3011 - accuracy: 0.9179 - val_loss: 0.2492 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.9192\n",
      "Epoch 17: val_accuracy improved from 0.91324 to 0.92922, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2922 - accuracy: 0.9191 - val_loss: 0.2699 - val_accuracy: 0.9292 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.9205\n",
      "Epoch 18: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2986 - accuracy: 0.9207 - val_loss: 0.2887 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9211\n",
      "Epoch 19: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2863 - accuracy: 0.9212 - val_loss: 0.2852 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.9277\n",
      "Epoch 20: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2633 - accuracy: 0.9285 - val_loss: 0.2397 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9285\n",
      "Epoch 21: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2501 - accuracy: 0.9293 - val_loss: 0.2746 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.9380\n",
      "Epoch 22: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2305 - accuracy: 0.9379 - val_loss: 0.2730 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.9357\n",
      "Epoch 23: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2386 - accuracy: 0.9359 - val_loss: 0.2433 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9324\n",
      "Epoch 24: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2435 - accuracy: 0.9326 - val_loss: 0.2778 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2219 - accuracy: 0.9365\n",
      "Epoch 25: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2236 - accuracy: 0.9364 - val_loss: 0.2565 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2182 - accuracy: 0.9380\n",
      "Epoch 26: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2180 - accuracy: 0.9381 - val_loss: 0.2528 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9388\n",
      "Epoch 27: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2101 - accuracy: 0.9394 - val_loss: 0.2768 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2111 - accuracy: 0.9400\n",
      "Epoch 28: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2112 - accuracy: 0.9399 - val_loss: 0.2707 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9419\n",
      "Epoch 29: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2070 - accuracy: 0.9422 - val_loss: 0.2671 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2004 - accuracy: 0.9439\n",
      "Epoch 30: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2005 - accuracy: 0.9437 - val_loss: 0.2585 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1857 - accuracy: 0.9497\n",
      "Epoch 31: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1858 - accuracy: 0.9496 - val_loss: 0.3007 - val_accuracy: 0.8995 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9511\n",
      "Epoch 32: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1685 - accuracy: 0.9513 - val_loss: 0.2566 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1686 - accuracy: 0.9533\n",
      "Epoch 33: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1688 - accuracy: 0.9531 - val_loss: 0.3080 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9551\n",
      "Epoch 34: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1562 - accuracy: 0.9551 - val_loss: 0.2972 - val_accuracy: 0.9110 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9530\n",
      "Epoch 35: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1600 - accuracy: 0.9534 - val_loss: 0.3358 - val_accuracy: 0.8995 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1593 - accuracy: 0.9557\n",
      "Epoch 36: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1596 - accuracy: 0.9556 - val_loss: 0.2962 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9548Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1559 - accuracy: 0.9549 - val_loss: 0.2900 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 37: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 4.4220 - accuracy: 0.8130\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23973, saving model to best_model_320.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 3s 21ms/step - loss: 4.4128 - accuracy: 0.8134 - val_loss: 7.1312 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4742 - accuracy: 0.8729\n",
      "Epoch 2: val_accuracy did not improve from 0.23973\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4758 - accuracy: 0.8725 - val_loss: 6.3968 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.8631\n",
      "Epoch 3: val_accuracy did not improve from 0.23973\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4644 - accuracy: 0.8631 - val_loss: 4.9569 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4597 - accuracy: 0.8717\n",
      "Epoch 4: val_accuracy did not improve from 0.23973\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4602 - accuracy: 0.8712 - val_loss: 2.2330 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.8758\n",
      "Epoch 5: val_accuracy improved from 0.23973 to 0.59589, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4623 - accuracy: 0.8758 - val_loss: 0.6517 - val_accuracy: 0.5959 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4632 - accuracy: 0.8778\n",
      "Epoch 6: val_accuracy improved from 0.59589 to 0.87900, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.4634 - accuracy: 0.8778 - val_loss: 0.2966 - val_accuracy: 0.8790 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4481 - accuracy: 0.8872\n",
      "Epoch 7: val_accuracy improved from 0.87900 to 0.88584, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4496 - accuracy: 0.8872 - val_loss: 0.3061 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4594 - accuracy: 0.8888\n",
      "Epoch 8: val_accuracy improved from 0.88584 to 0.89498, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4602 - accuracy: 0.8887 - val_loss: 0.4270 - val_accuracy: 0.8950 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4592 - accuracy: 0.8819\n",
      "Epoch 9: val_accuracy did not improve from 0.89498\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4592 - accuracy: 0.8819 - val_loss: 0.3728 - val_accuracy: 0.8493 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.9001\n",
      "Epoch 10: val_accuracy did not improve from 0.89498\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3971 - accuracy: 0.8996 - val_loss: 0.2837 - val_accuracy: 0.8836 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3897 - accuracy: 0.9044\n",
      "Epoch 11: val_accuracy improved from 0.89498 to 0.89726, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3903 - accuracy: 0.9042 - val_loss: 0.3056 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.9078\n",
      "Epoch 12: val_accuracy improved from 0.89726 to 0.90868, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3671 - accuracy: 0.9077 - val_loss: 0.2724 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.9116\n",
      "Epoch 13: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3637 - accuracy: 0.9123 - val_loss: 0.2896 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3633 - accuracy: 0.9131\n",
      "Epoch 14: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3633 - accuracy: 0.9131 - val_loss: 0.3098 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3608 - accuracy: 0.9121\n",
      "Epoch 15: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3608 - accuracy: 0.9120 - val_loss: 0.2780 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.9080\n",
      "Epoch 16: val_accuracy improved from 0.90868 to 0.91324, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 22ms/step - loss: 0.3666 - accuracy: 0.9085 - val_loss: 0.2731 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3442 - accuracy: 0.9155\n",
      "Epoch 17: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3433 - accuracy: 0.9156 - val_loss: 0.2839 - val_accuracy: 0.8927 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3306 - accuracy: 0.9184\n",
      "Epoch 18: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3304 - accuracy: 0.9186 - val_loss: 0.2874 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.9146\n",
      "Epoch 19: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3508 - accuracy: 0.9146 - val_loss: 0.2879 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.9204\n",
      "Epoch 20: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3073 - accuracy: 0.9204 - val_loss: 0.2845 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9263\n",
      "Epoch 21: val_accuracy improved from 0.91324 to 0.92009, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2910 - accuracy: 0.9265 - val_loss: 0.2761 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2819 - accuracy: 0.9303\n",
      "Epoch 22: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2851 - accuracy: 0.9298 - val_loss: 0.2765 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.9339\n",
      "Epoch 23: val_accuracy improved from 0.92009 to 0.92237, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 24ms/step - loss: 0.2714 - accuracy: 0.9341 - val_loss: 0.2679 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.9334\n",
      "Epoch 24: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2826 - accuracy: 0.9338 - val_loss: 0.2758 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9313\n",
      "Epoch 25: val_accuracy improved from 0.92237 to 0.92466, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 23ms/step - loss: 0.2775 - accuracy: 0.9313 - val_loss: 0.2733 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.9296\n",
      "Epoch 26: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2722 - accuracy: 0.9295 - val_loss: 0.3334 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2724 - accuracy: 0.9352\n",
      "Epoch 27: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2708 - accuracy: 0.9354 - val_loss: 0.3168 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2650 - accuracy: 0.9342\n",
      "Epoch 28: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2646 - accuracy: 0.9343 - val_loss: 0.2757 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.9398\n",
      "Epoch 29: val_accuracy improved from 0.92466 to 0.92694, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.2606 - accuracy: 0.9389 - val_loss: 0.2656 - val_accuracy: 0.9269 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9430\n",
      "Epoch 30: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2405 - accuracy: 0.9430 - val_loss: 0.2613 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.9388\n",
      "Epoch 31: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2437 - accuracy: 0.9387 - val_loss: 0.2801 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9437\n",
      "Epoch 32: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2278 - accuracy: 0.9437 - val_loss: 0.2842 - val_accuracy: 0.9110 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9455\n",
      "Epoch 33: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2279 - accuracy: 0.9455 - val_loss: 0.2936 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2207 - accuracy: 0.9494\n",
      "Epoch 34: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2206 - accuracy: 0.9496 - val_loss: 0.2717 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9458\n",
      "Epoch 35: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2260 - accuracy: 0.9458 - val_loss: 0.2749 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.9437\n",
      "Epoch 36: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2224 - accuracy: 0.9437 - val_loss: 0.3128 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9515\n",
      "Epoch 37: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2034 - accuracy: 0.9516 - val_loss: 0.3019 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9543\n",
      "Epoch 38: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2104 - accuracy: 0.9536 - val_loss: 0.3023 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2113 - accuracy: 0.9490\n",
      "Epoch 39: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2113 - accuracy: 0.9490 - val_loss: 0.2800 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1937 - accuracy: 0.9538\n",
      "Epoch 40: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1933 - accuracy: 0.9539 - val_loss: 0.2861 - val_accuracy: 0.9155 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1917 - accuracy: 0.9533\n",
      "Epoch 41: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1925 - accuracy: 0.9529 - val_loss: 0.3111 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1925 - accuracy: 0.9535\n",
      "Epoch 42: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1931 - accuracy: 0.9534 - val_loss: 0.3039 - val_accuracy: 0.9064 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9549\n",
      "Epoch 43: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1954 - accuracy: 0.9544 - val_loss: 0.3336 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9597\n",
      "Epoch 44: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1857 - accuracy: 0.9597 - val_loss: 0.3054 - val_accuracy: 0.9064 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9582\n",
      "Epoch 45: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1823 - accuracy: 0.9584 - val_loss: 0.3155 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9603\n",
      "Epoch 46: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1768 - accuracy: 0.9605 - val_loss: 0.3406 - val_accuracy: 0.9018 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1655 - accuracy: 0.9654\n",
      "Epoch 47: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1654 - accuracy: 0.9655 - val_loss: 0.3515 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9569\n",
      "Epoch 48: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1828 - accuracy: 0.9569 - val_loss: 0.3351 - val_accuracy: 0.9110 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9622Restoring model weights from the end of the best epoch: 29.\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.92694\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1647 - accuracy: 0.9622 - val_loss: 0.3324 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 49: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 4.8442 - accuracy: 0.8046\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26256, saving model to best_model_320.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 4s 23ms/step - loss: 4.8442 - accuracy: 0.8046 - val_loss: 7.2600 - val_accuracy: 0.2626 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4784 - accuracy: 0.8641\n",
      "Epoch 2: val_accuracy did not improve from 0.26256\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4784 - accuracy: 0.8641 - val_loss: 5.3609 - val_accuracy: 0.2626 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4696 - accuracy: 0.8707\n",
      "Epoch 3: val_accuracy did not improve from 0.26256\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4687 - accuracy: 0.8710 - val_loss: 3.7110 - val_accuracy: 0.2626 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4965 - accuracy: 0.8689\n",
      "Epoch 4: val_accuracy improved from 0.26256 to 0.48402, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4959 - accuracy: 0.8692 - val_loss: 0.7708 - val_accuracy: 0.4840 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4790 - accuracy: 0.8758\n",
      "Epoch 5: val_accuracy improved from 0.48402 to 0.89269, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4801 - accuracy: 0.8758 - val_loss: 0.3559 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.8840\n",
      "Epoch 6: val_accuracy improved from 0.89269 to 0.89498, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4475 - accuracy: 0.8842 - val_loss: 0.3899 - val_accuracy: 0.8950 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4881 - accuracy: 0.8841\n",
      "Epoch 7: val_accuracy did not improve from 0.89498\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4876 - accuracy: 0.8844 - val_loss: 0.3834 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4593 - accuracy: 0.8867\n",
      "Epoch 8: val_accuracy improved from 0.89498 to 0.90183, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 22ms/step - loss: 0.4586 - accuracy: 0.8869 - val_loss: 0.2985 - val_accuracy: 0.9018 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4355 - accuracy: 0.8870\n",
      "Epoch 9: val_accuracy did not improve from 0.90183\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4351 - accuracy: 0.8872 - val_loss: 0.3489 - val_accuracy: 0.8813 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.8953\n",
      "Epoch 10: val_accuracy improved from 0.90183 to 0.90868, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4175 - accuracy: 0.8953 - val_loss: 0.2986 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.9037\n",
      "Epoch 11: val_accuracy improved from 0.90868 to 0.91553, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.4084 - accuracy: 0.9037 - val_loss: 0.2910 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8991\n",
      "Epoch 12: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3995 - accuracy: 0.8991 - val_loss: 0.2731 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.8986\n",
      "Epoch 13: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3970 - accuracy: 0.8986 - val_loss: 0.2974 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3812 - accuracy: 0.9027\n",
      "Epoch 14: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3814 - accuracy: 0.9027 - val_loss: 0.2722 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3871 - accuracy: 0.9004\n",
      "Epoch 15: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3864 - accuracy: 0.9006 - val_loss: 0.2988 - val_accuracy: 0.8858 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.9137\n",
      "Epoch 16: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3680 - accuracy: 0.9136 - val_loss: 0.2657 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.9037\n",
      "Epoch 17: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3773 - accuracy: 0.9042 - val_loss: 0.2776 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.9142\n",
      "Epoch 18: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3574 - accuracy: 0.9131 - val_loss: 0.2709 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.9109\n",
      "Epoch 19: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3535 - accuracy: 0.9100 - val_loss: 0.2739 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.9179\n",
      "Epoch 20: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3417 - accuracy: 0.9171 - val_loss: 0.2842 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.9149\n",
      "Epoch 21: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3361 - accuracy: 0.9151 - val_loss: 0.3269 - val_accuracy: 0.8881 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.9226\n",
      "Epoch 22: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3134 - accuracy: 0.9232 - val_loss: 0.2661 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.9174\n",
      "Epoch 23: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3355 - accuracy: 0.9176 - val_loss: 0.2891 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.9217\n",
      "Epoch 24: val_accuracy improved from 0.91553 to 0.92237, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3136 - accuracy: 0.9222 - val_loss: 0.2660 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.9209\n",
      "Epoch 25: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3083 - accuracy: 0.9209 - val_loss: 0.2956 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.9274\n",
      "Epoch 26: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3087 - accuracy: 0.9270 - val_loss: 0.2763 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3088 - accuracy: 0.9225\n",
      "Epoch 27: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3083 - accuracy: 0.9227 - val_loss: 0.3195 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.9210\n",
      "Epoch 28: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3153 - accuracy: 0.9209 - val_loss: 0.3424 - val_accuracy: 0.8630 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9334\n",
      "Epoch 29: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2892 - accuracy: 0.9333 - val_loss: 0.2942 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.9333\n",
      "Epoch 30: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2723 - accuracy: 0.9333 - val_loss: 0.3022 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.9347\n",
      "Epoch 31: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2800 - accuracy: 0.9346 - val_loss: 0.2972 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2733 - accuracy: 0.9326\n",
      "Epoch 32: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2717 - accuracy: 0.9328 - val_loss: 0.2954 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.9347\n",
      "Epoch 33: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2691 - accuracy: 0.9346 - val_loss: 0.2874 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9413\n",
      "Epoch 34: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2570 - accuracy: 0.9409 - val_loss: 0.3185 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2626 - accuracy: 0.9360\n",
      "Epoch 35: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2629 - accuracy: 0.9359 - val_loss: 0.3096 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9406\n",
      "Epoch 36: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2622 - accuracy: 0.9407 - val_loss: 0.2818 - val_accuracy: 0.9110 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2589 - accuracy: 0.9362\n",
      "Epoch 37: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2619 - accuracy: 0.9351 - val_loss: 0.2999 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.9347\n",
      "Epoch 38: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2611 - accuracy: 0.9349 - val_loss: 0.3194 - val_accuracy: 0.9110 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.9422\n",
      "Epoch 39: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2432 - accuracy: 0.9422 - val_loss: 0.3675 - val_accuracy: 0.8858 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9419\n",
      "Epoch 40: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2361 - accuracy: 0.9420 - val_loss: 0.3280 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9452\n",
      "Epoch 41: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2345 - accuracy: 0.9452 - val_loss: 0.3428 - val_accuracy: 0.8973 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.9412\n",
      "Epoch 42: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2404 - accuracy: 0.9412 - val_loss: 0.3367 - val_accuracy: 0.9064 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2384 - accuracy: 0.9396\n",
      "Epoch 43: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2382 - accuracy: 0.9397 - val_loss: 0.3368 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9454Restoring model weights from the end of the best epoch: 24.\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2334 - accuracy: 0.9458 - val_loss: 0.3469 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 44: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 6.3835 - accuracy: 0.8218\n",
      "Epoch 1: val_accuracy improved from -inf to 0.47260, saving model to best_model_320.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 3s 21ms/step - loss: 6.2731 - accuracy: 0.8233 - val_loss: 0.6773 - val_accuracy: 0.4726 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4673 - accuracy: 0.8768\n",
      "Epoch 2: val_accuracy did not improve from 0.47260\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4669 - accuracy: 0.8768 - val_loss: 1.2610 - val_accuracy: 0.2603 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4558 - accuracy: 0.8755\n",
      "Epoch 3: val_accuracy did not improve from 0.47260\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4553 - accuracy: 0.8755 - val_loss: 1.0549 - val_accuracy: 0.2877 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4570 - accuracy: 0.8858\n",
      "Epoch 4: val_accuracy improved from 0.47260 to 0.76484, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4552 - accuracy: 0.8859 - val_loss: 0.4865 - val_accuracy: 0.7648 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8835\n",
      "Epoch 5: val_accuracy improved from 0.76484 to 0.84247, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4286 - accuracy: 0.8831 - val_loss: 0.4024 - val_accuracy: 0.8425 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4471 - accuracy: 0.8854\n",
      "Epoch 6: val_accuracy improved from 0.84247 to 0.89954, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4471 - accuracy: 0.8854 - val_loss: 0.3144 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4302 - accuracy: 0.8908\n",
      "Epoch 7: val_accuracy improved from 0.89954 to 0.90183, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4300 - accuracy: 0.8907 - val_loss: 0.2645 - val_accuracy: 0.9018 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4026 - accuracy: 0.8954\n",
      "Epoch 8: val_accuracy improved from 0.90183 to 0.91553, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4033 - accuracy: 0.8948 - val_loss: 0.2740 - val_accuracy: 0.9155 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4136 - accuracy: 0.8910\n",
      "Epoch 9: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4136 - accuracy: 0.8910 - val_loss: 0.2748 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.9045\n",
      "Epoch 10: val_accuracy improved from 0.91553 to 0.92009, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.3712 - accuracy: 0.9042 - val_loss: 0.2510 - val_accuracy: 0.9201 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.9052\n",
      "Epoch 11: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3592 - accuracy: 0.9047 - val_loss: 0.2654 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3542 - accuracy: 0.9050\n",
      "Epoch 12: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3547 - accuracy: 0.9052 - val_loss: 0.2578 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.9098\n",
      "Epoch 13: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3427 - accuracy: 0.9098 - val_loss: 0.2507 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.9155\n",
      "Epoch 14: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3331 - accuracy: 0.9141 - val_loss: 0.2581 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.9119\n",
      "Epoch 15: val_accuracy improved from 0.92009 to 0.92237, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3238 - accuracy: 0.9131 - val_loss: 0.2976 - val_accuracy: 0.9224 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.9114\n",
      "Epoch 16: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3268 - accuracy: 0.9123 - val_loss: 0.2488 - val_accuracy: 0.9201 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.9150\n",
      "Epoch 17: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3179 - accuracy: 0.9146 - val_loss: 0.2538 - val_accuracy: 0.9201 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.9146\n",
      "Epoch 18: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3169 - accuracy: 0.9148 - val_loss: 0.2447 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.9118\n",
      "Epoch 19: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3419 - accuracy: 0.9120 - val_loss: 0.2734 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.9212\n",
      "Epoch 20: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2914 - accuracy: 0.9212 - val_loss: 0.2460 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.9256\n",
      "Epoch 21: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2778 - accuracy: 0.9257 - val_loss: 0.2448 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9273\n",
      "Epoch 22: val_accuracy improved from 0.92237 to 0.92466, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2588 - accuracy: 0.9275 - val_loss: 0.2549 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.9308\n",
      "Epoch 23: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2669 - accuracy: 0.9313 - val_loss: 0.2597 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2523 - accuracy: 0.9332\n",
      "Epoch 24: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2518 - accuracy: 0.9333 - val_loss: 0.2873 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.9268\n",
      "Epoch 25: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2733 - accuracy: 0.9270 - val_loss: 0.2712 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9380\n",
      "Epoch 26: val_accuracy improved from 0.92466 to 0.93151, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 22ms/step - loss: 0.2360 - accuracy: 0.9381 - val_loss: 0.2571 - val_accuracy: 0.9315 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9388\n",
      "Epoch 27: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2407 - accuracy: 0.9381 - val_loss: 0.2648 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9321\n",
      "Epoch 28: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2602 - accuracy: 0.9321 - val_loss: 0.2848 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2369 - accuracy: 0.9375\n",
      "Epoch 29: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2375 - accuracy: 0.9371 - val_loss: 0.2671 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9395\n",
      "Epoch 30: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2177 - accuracy: 0.9394 - val_loss: 0.2758 - val_accuracy: 0.9292 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2138 - accuracy: 0.9470\n",
      "Epoch 31: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2135 - accuracy: 0.9468 - val_loss: 0.2745 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.9424\n",
      "Epoch 32: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2057 - accuracy: 0.9430 - val_loss: 0.2657 - val_accuracy: 0.9292 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1938 - accuracy: 0.9483\n",
      "Epoch 33: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1945 - accuracy: 0.9480 - val_loss: 0.2823 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1947 - accuracy: 0.9524\n",
      "Epoch 34: val_accuracy improved from 0.93151 to 0.93607, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.1955 - accuracy: 0.9521 - val_loss: 0.2663 - val_accuracy: 0.9361 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9498\n",
      "Epoch 35: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1963 - accuracy: 0.9501 - val_loss: 0.2705 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9477\n",
      "Epoch 36: val_accuracy did not improve from 0.93607\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1829 - accuracy: 0.9475 - val_loss: 0.3002 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9497\n",
      "Epoch 37: val_accuracy improved from 0.93607 to 0.94064, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1880 - accuracy: 0.9496 - val_loss: 0.2923 - val_accuracy: 0.9406 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1883 - accuracy: 0.9516\n",
      "Epoch 38: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1883 - accuracy: 0.9516 - val_loss: 0.2845 - val_accuracy: 0.9361 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.9487\n",
      "Epoch 39: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1883 - accuracy: 0.9485 - val_loss: 0.2980 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9595\n",
      "Epoch 40: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1645 - accuracy: 0.9597 - val_loss: 0.2891 - val_accuracy: 0.9247 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1690 - accuracy: 0.9544\n",
      "Epoch 41: val_accuracy did not improve from 0.94064\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1693 - accuracy: 0.9544 - val_loss: 0.3012 - val_accuracy: 0.9384 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9577\n",
      "Epoch 42: val_accuracy improved from 0.94064 to 0.94292, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.1670 - accuracy: 0.9577 - val_loss: 0.3123 - val_accuracy: 0.9429 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9597\n",
      "Epoch 43: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1476 - accuracy: 0.9597 - val_loss: 0.3061 - val_accuracy: 0.9384 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9559\n",
      "Epoch 44: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1601 - accuracy: 0.9561 - val_loss: 0.3005 - val_accuracy: 0.9315 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1444 - accuracy: 0.9647\n",
      "Epoch 45: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1447 - accuracy: 0.9645 - val_loss: 0.3101 - val_accuracy: 0.9406 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9688\n",
      "Epoch 46: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1390 - accuracy: 0.9686 - val_loss: 0.3239 - val_accuracy: 0.9315 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1460 - accuracy: 0.9634\n",
      "Epoch 47: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1457 - accuracy: 0.9632 - val_loss: 0.3330 - val_accuracy: 0.9247 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1361 - accuracy: 0.9637\n",
      "Epoch 48: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1358 - accuracy: 0.9638 - val_loss: 0.3233 - val_accuracy: 0.9292 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.9647\n",
      "Epoch 49: val_accuracy improved from 0.94292 to 0.94749, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.1345 - accuracy: 0.9648 - val_loss: 0.3368 - val_accuracy: 0.9475 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9677\n",
      "Epoch 50: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1282 - accuracy: 0.9678 - val_loss: 0.3438 - val_accuracy: 0.9406 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9629\n",
      "Epoch 51: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1379 - accuracy: 0.9630 - val_loss: 0.3440 - val_accuracy: 0.9361 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9673\n",
      "Epoch 52: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1213 - accuracy: 0.9673 - val_loss: 0.3486 - val_accuracy: 0.9315 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9672\n",
      "Epoch 53: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1363 - accuracy: 0.9673 - val_loss: 0.3379 - val_accuracy: 0.9292 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9680\n",
      "Epoch 54: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1280 - accuracy: 0.9681 - val_loss: 0.3402 - val_accuracy: 0.9338 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9695\n",
      "Epoch 55: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1229 - accuracy: 0.9691 - val_loss: 0.3579 - val_accuracy: 0.9361 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9677\n",
      "Epoch 56: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1259 - accuracy: 0.9678 - val_loss: 0.3611 - val_accuracy: 0.9384 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9669\n",
      "Epoch 57: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1253 - accuracy: 0.9670 - val_loss: 0.3584 - val_accuracy: 0.9406 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9675\n",
      "Epoch 58: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1232 - accuracy: 0.9670 - val_loss: 0.3649 - val_accuracy: 0.9338 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9673\n",
      "Epoch 59: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1249 - accuracy: 0.9673 - val_loss: 0.3761 - val_accuracy: 0.9338 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9752\n",
      "Epoch 60: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1146 - accuracy: 0.9752 - val_loss: 0.3715 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9731\n",
      "Epoch 61: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1107 - accuracy: 0.9731 - val_loss: 0.3819 - val_accuracy: 0.9338 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9682\n",
      "Epoch 62: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1136 - accuracy: 0.9681 - val_loss: 0.3827 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 63/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9706\n",
      "Epoch 63: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1191 - accuracy: 0.9706 - val_loss: 0.3835 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 64/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 0.9675\n",
      "Epoch 64: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1153 - accuracy: 0.9676 - val_loss: 0.3917 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 65/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9721\n",
      "Epoch 65: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1156 - accuracy: 0.9721 - val_loss: 0.3902 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 66/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9705\n",
      "Epoch 66: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1141 - accuracy: 0.9703 - val_loss: 0.3874 - val_accuracy: 0.9315 - lr: 1.5625e-04\n",
      "Epoch 67/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9711\n",
      "Epoch 67: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1138 - accuracy: 0.9711 - val_loss: 0.3889 - val_accuracy: 0.9315 - lr: 1.5625e-04\n",
      "Epoch 68/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9741\n",
      "Epoch 68: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1088 - accuracy: 0.9741 - val_loss: 0.3887 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 69/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9685Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.94749\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1174 - accuracy: 0.9686 - val_loss: 0.3879 - val_accuracy: 0.9361 - lr: 1.5625e-04\n",
      "Epoch 69: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 6.9213 - accuracy: 0.8230\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26941, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 22ms/step - loss: 6.8529 - accuracy: 0.8238 - val_loss: 2.6162 - val_accuracy: 0.2694 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4945 - accuracy: 0.8722\n",
      "Epoch 2: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4924 - accuracy: 0.8728 - val_loss: 3.0877 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4639 - accuracy: 0.8857\n",
      "Epoch 3: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4636 - accuracy: 0.8854 - val_loss: 3.1931 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8906\n",
      "Epoch 4: val_accuracy improved from 0.26941 to 0.50685, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4221 - accuracy: 0.8905 - val_loss: 0.8458 - val_accuracy: 0.5068 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4361 - accuracy: 0.8821\n",
      "Epoch 5: val_accuracy improved from 0.50685 to 0.79680, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.4358 - accuracy: 0.8819 - val_loss: 0.3705 - val_accuracy: 0.7968 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4010 - accuracy: 0.8936\n",
      "Epoch 6: val_accuracy improved from 0.79680 to 0.90411, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4018 - accuracy: 0.8923 - val_loss: 0.2622 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4137 - accuracy: 0.8847\n",
      "Epoch 7: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4137 - accuracy: 0.8849 - val_loss: 0.2488 - val_accuracy: 0.9018 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8957\n",
      "Epoch 8: val_accuracy improved from 0.90411 to 0.90639, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3997 - accuracy: 0.8953 - val_loss: 0.2550 - val_accuracy: 0.9064 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4403 - accuracy: 0.8844\n",
      "Epoch 9: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4403 - accuracy: 0.8844 - val_loss: 0.2879 - val_accuracy: 0.8539 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.8988\n",
      "Epoch 10: val_accuracy improved from 0.90639 to 0.90868, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3870 - accuracy: 0.8996 - val_loss: 0.2244 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3816 - accuracy: 0.9027\n",
      "Epoch 11: val_accuracy improved from 0.90868 to 0.91781, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3808 - accuracy: 0.9027 - val_loss: 0.2103 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.9037\n",
      "Epoch 12: val_accuracy improved from 0.91781 to 0.92466, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3730 - accuracy: 0.9019 - val_loss: 0.2140 - val_accuracy: 0.9247 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3545 - accuracy: 0.9024\n",
      "Epoch 13: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3558 - accuracy: 0.9014 - val_loss: 0.2282 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3600 - accuracy: 0.9106\n",
      "Epoch 14: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3601 - accuracy: 0.9105 - val_loss: 0.2053 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3425 - accuracy: 0.9139\n",
      "Epoch 15: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3426 - accuracy: 0.9138 - val_loss: 0.2160 - val_accuracy: 0.9201 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.9119\n",
      "Epoch 16: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3401 - accuracy: 0.9113 - val_loss: 0.2968 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3618 - accuracy: 0.9035\n",
      "Epoch 17: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3619 - accuracy: 0.9034 - val_loss: 0.2321 - val_accuracy: 0.8836 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.9135\n",
      "Epoch 18: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.3444 - accuracy: 0.9141 - val_loss: 0.2077 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.9153\n",
      "Epoch 19: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3307 - accuracy: 0.9141 - val_loss: 0.2108 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.9161\n",
      "Epoch 20: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3212 - accuracy: 0.9156 - val_loss: 0.2644 - val_accuracy: 0.8950 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.9234\n",
      "Epoch 21: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2909 - accuracy: 0.9234 - val_loss: 0.2133 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9327\n",
      "Epoch 22: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2840 - accuracy: 0.9328 - val_loss: 0.2251 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.9243\n",
      "Epoch 23: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2822 - accuracy: 0.9245 - val_loss: 0.1904 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2730 - accuracy: 0.9260\n",
      "Epoch 24: val_accuracy improved from 0.92466 to 0.92922, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2722 - accuracy: 0.9265 - val_loss: 0.2059 - val_accuracy: 0.9292 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.9314\n",
      "Epoch 25: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2720 - accuracy: 0.9318 - val_loss: 0.2178 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2806 - accuracy: 0.9267\n",
      "Epoch 26: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2798 - accuracy: 0.9270 - val_loss: 0.2140 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.9269\n",
      "Epoch 27: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2779 - accuracy: 0.9260 - val_loss: 0.2088 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.9311\n",
      "Epoch 28: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2572 - accuracy: 0.9311 - val_loss: 0.2063 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9263\n",
      "Epoch 29: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2591 - accuracy: 0.9262 - val_loss: 0.2186 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.9393\n",
      "Epoch 30: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2359 - accuracy: 0.9394 - val_loss: 0.2084 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9355\n",
      "Epoch 31: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2411 - accuracy: 0.9354 - val_loss: 0.2153 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9374\n",
      "Epoch 32: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2291 - accuracy: 0.9374 - val_loss: 0.2090 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9380\n",
      "Epoch 33: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2169 - accuracy: 0.9389 - val_loss: 0.1980 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9436\n",
      "Epoch 34: val_accuracy improved from 0.92922 to 0.93151, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2094 - accuracy: 0.9437 - val_loss: 0.2093 - val_accuracy: 0.9315 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9475\n",
      "Epoch 35: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2120 - accuracy: 0.9475 - val_loss: 0.2062 - val_accuracy: 0.9315 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9452\n",
      "Epoch 36: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2181 - accuracy: 0.9452 - val_loss: 0.2027 - val_accuracy: 0.9292 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9475\n",
      "Epoch 37: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2075 - accuracy: 0.9475 - val_loss: 0.2070 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2105 - accuracy: 0.9476\n",
      "Epoch 38: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2102 - accuracy: 0.9478 - val_loss: 0.2235 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9452\n",
      "Epoch 39: val_accuracy did not improve from 0.93151\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2093 - accuracy: 0.9458 - val_loss: 0.2267 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1879 - accuracy: 0.9514\n",
      "Epoch 40: val_accuracy improved from 0.93151 to 0.93379, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.1858 - accuracy: 0.9521 - val_loss: 0.2186 - val_accuracy: 0.9338 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1826 - accuracy: 0.9529\n",
      "Epoch 41: val_accuracy did not improve from 0.93379\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1827 - accuracy: 0.9529 - val_loss: 0.2240 - val_accuracy: 0.9269 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1815 - accuracy: 0.9538\n",
      "Epoch 42: val_accuracy did not improve from 0.93379\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1813 - accuracy: 0.9539 - val_loss: 0.2158 - val_accuracy: 0.9292 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1664 - accuracy: 0.9550\n",
      "Epoch 43: val_accuracy did not improve from 0.93379\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1661 - accuracy: 0.9551 - val_loss: 0.2220 - val_accuracy: 0.9338 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9585\n",
      "Epoch 44: val_accuracy improved from 0.93379 to 0.94292, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1779 - accuracy: 0.9587 - val_loss: 0.2102 - val_accuracy: 0.9429 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9540\n",
      "Epoch 45: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1745 - accuracy: 0.9541 - val_loss: 0.2188 - val_accuracy: 0.9384 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9546\n",
      "Epoch 46: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1797 - accuracy: 0.9546 - val_loss: 0.2154 - val_accuracy: 0.9361 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9579\n",
      "Epoch 47: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1746 - accuracy: 0.9582 - val_loss: 0.2188 - val_accuracy: 0.9361 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9548\n",
      "Epoch 48: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1731 - accuracy: 0.9551 - val_loss: 0.2126 - val_accuracy: 0.9338 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1633 - accuracy: 0.9593\n",
      "Epoch 49: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1632 - accuracy: 0.9592 - val_loss: 0.2232 - val_accuracy: 0.9361 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.9612\n",
      "Epoch 50: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1558 - accuracy: 0.9612 - val_loss: 0.2321 - val_accuracy: 0.9269 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9591\n",
      "Epoch 51: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1538 - accuracy: 0.9589 - val_loss: 0.2126 - val_accuracy: 0.9361 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.9626\n",
      "Epoch 52: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1599 - accuracy: 0.9627 - val_loss: 0.2146 - val_accuracy: 0.9406 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9632\n",
      "Epoch 53: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1455 - accuracy: 0.9632 - val_loss: 0.2246 - val_accuracy: 0.9338 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1588 - accuracy: 0.9582\n",
      "Epoch 54: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1574 - accuracy: 0.9587 - val_loss: 0.2230 - val_accuracy: 0.9384 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.9629\n",
      "Epoch 55: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1400 - accuracy: 0.9630 - val_loss: 0.2357 - val_accuracy: 0.9292 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9627\n",
      "Epoch 56: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1438 - accuracy: 0.9627 - val_loss: 0.2418 - val_accuracy: 0.9269 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.9667\n",
      "Epoch 57: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1374 - accuracy: 0.9663 - val_loss: 0.2263 - val_accuracy: 0.9338 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9615\n",
      "Epoch 58: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1501 - accuracy: 0.9612 - val_loss: 0.2257 - val_accuracy: 0.9338 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1471 - accuracy: 0.9606\n",
      "Epoch 59: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.1476 - accuracy: 0.9602 - val_loss: 0.2306 - val_accuracy: 0.9292 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.9647\n",
      "Epoch 60: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1389 - accuracy: 0.9648 - val_loss: 0.2310 - val_accuracy: 0.9315 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9653\n",
      "Epoch 61: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1351 - accuracy: 0.9653 - val_loss: 0.2323 - val_accuracy: 0.9338 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9673\n",
      "Epoch 62: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1339 - accuracy: 0.9673 - val_loss: 0.2439 - val_accuracy: 0.9269 - lr: 1.5625e-04\n",
      "Epoch 63/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1352 - accuracy: 0.9649\n",
      "Epoch 63: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1350 - accuracy: 0.9650 - val_loss: 0.2473 - val_accuracy: 0.9269 - lr: 1.5625e-04\n",
      "Epoch 64/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1291 - accuracy: 0.9649Restoring model weights from the end of the best epoch: 44.\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.94292\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1301 - accuracy: 0.9645 - val_loss: 0.2306 - val_accuracy: 0.9338 - lr: 1.5625e-04\n",
      "Epoch 64: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 5.9240 - accuracy: 0.8063\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23744, saving model to best_model_320.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 3s 22ms/step - loss: 5.8271 - accuracy: 0.8066 - val_loss: 3.5293 - val_accuracy: 0.2374 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4838 - accuracy: 0.8634\n",
      "Epoch 2: val_accuracy did not improve from 0.23744\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4838 - accuracy: 0.8634 - val_loss: 4.8512 - val_accuracy: 0.2374 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4372 - accuracy: 0.8755\n",
      "Epoch 3: val_accuracy did not improve from 0.23744\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4376 - accuracy: 0.8753 - val_loss: 2.6175 - val_accuracy: 0.2374 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.8750\n",
      "Epoch 4: val_accuracy improved from 0.23744 to 0.25114, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4252 - accuracy: 0.8750 - val_loss: 2.2214 - val_accuracy: 0.2511 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4241 - accuracy: 0.8707\n",
      "Epoch 5: val_accuracy improved from 0.25114 to 0.73288, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.4241 - accuracy: 0.8707 - val_loss: 0.5929 - val_accuracy: 0.7329 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4335 - accuracy: 0.8675\n",
      "Epoch 6: val_accuracy improved from 0.73288 to 0.91096, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.4332 - accuracy: 0.8672 - val_loss: 0.3075 - val_accuracy: 0.9110 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4031 - accuracy: 0.8834\n",
      "Epoch 7: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4031 - accuracy: 0.8834 - val_loss: 0.4705 - val_accuracy: 0.8082 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8891\n",
      "Epoch 8: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3891 - accuracy: 0.8895 - val_loss: 0.2792 - val_accuracy: 0.9110 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.8895\n",
      "Epoch 9: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3938 - accuracy: 0.8890 - val_loss: 0.3348 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.9029\n",
      "Epoch 10: val_accuracy improved from 0.91096 to 0.91553, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3784 - accuracy: 0.9032 - val_loss: 0.2927 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.9035\n",
      "Epoch 11: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3619 - accuracy: 0.9037 - val_loss: 0.2538 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.9049\n",
      "Epoch 12: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3402 - accuracy: 0.9049 - val_loss: 0.2753 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.9144\n",
      "Epoch 13: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3221 - accuracy: 0.9148 - val_loss: 0.2820 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3347 - accuracy: 0.9103\n",
      "Epoch 14: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3366 - accuracy: 0.9098 - val_loss: 0.2861 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.9093\n",
      "Epoch 15: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3270 - accuracy: 0.9093 - val_loss: 0.2874 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.9160\n",
      "Epoch 16: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3085 - accuracy: 0.9156 - val_loss: 0.2751 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.9123\n",
      "Epoch 17: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3157 - accuracy: 0.9123 - val_loss: 0.2674 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9204\n",
      "Epoch 18: val_accuracy improved from 0.91553 to 0.91781, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2834 - accuracy: 0.9204 - val_loss: 0.2737 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.9247\n",
      "Epoch 19: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2887 - accuracy: 0.9247 - val_loss: 0.2919 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2701 - accuracy: 0.9298\n",
      "Epoch 20: val_accuracy improved from 0.91781 to 0.92009, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.2731 - accuracy: 0.9295 - val_loss: 0.2656 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2702 - accuracy: 0.9308\n",
      "Epoch 21: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.2695 - accuracy: 0.9308 - val_loss: 0.3008 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2517 - accuracy: 0.9305\n",
      "Epoch 22: val_accuracy improved from 0.92009 to 0.92922, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2515 - accuracy: 0.9300 - val_loss: 0.2640 - val_accuracy: 0.9292 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.9316\n",
      "Epoch 23: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2500 - accuracy: 0.9316 - val_loss: 0.3028 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9296\n",
      "Epoch 24: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2445 - accuracy: 0.9298 - val_loss: 0.3126 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9379\n",
      "Epoch 25: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2419 - accuracy: 0.9379 - val_loss: 0.3270 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9300\n",
      "Epoch 26: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2419 - accuracy: 0.9300 - val_loss: 0.3113 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9380\n",
      "Epoch 27: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2262 - accuracy: 0.9376 - val_loss: 0.3073 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2201 - accuracy: 0.9411\n",
      "Epoch 28: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2224 - accuracy: 0.9407 - val_loss: 0.3042 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9362\n",
      "Epoch 29: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2398 - accuracy: 0.9366 - val_loss: 0.3416 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.2090 - accuracy: 0.9406\n",
      "Epoch 30: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2081 - accuracy: 0.9412 - val_loss: 0.3014 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2089 - accuracy: 0.9429\n",
      "Epoch 31: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2101 - accuracy: 0.9425 - val_loss: 0.3332 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9468\n",
      "Epoch 32: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1917 - accuracy: 0.9468 - val_loss: 0.3524 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9450\n",
      "Epoch 33: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1922 - accuracy: 0.9447 - val_loss: 0.3381 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1883 - accuracy: 0.9494\n",
      "Epoch 34: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1879 - accuracy: 0.9496 - val_loss: 0.3983 - val_accuracy: 0.8881 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1813 - accuracy: 0.9470\n",
      "Epoch 35: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1808 - accuracy: 0.9470 - val_loss: 0.3409 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1738 - accuracy: 0.9527\n",
      "Epoch 36: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1736 - accuracy: 0.9529 - val_loss: 0.3868 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.9508\n",
      "Epoch 37: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1717 - accuracy: 0.9501 - val_loss: 0.3694 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1646 - accuracy: 0.9506\n",
      "Epoch 38: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1633 - accuracy: 0.9511 - val_loss: 0.3723 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9465\n",
      "Epoch 39: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1856 - accuracy: 0.9473 - val_loss: 0.3835 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9544\n",
      "Epoch 40: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1549 - accuracy: 0.9549 - val_loss: 0.3721 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9572\n",
      "Epoch 41: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1550 - accuracy: 0.9572 - val_loss: 0.3782 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1492 - accuracy: 0.9563Restoring model weights from the end of the best epoch: 22.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1489 - accuracy: 0.9564 - val_loss: 0.4048 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 42: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 6.5775 - accuracy: 0.8166\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26941, saving model to best_model_320.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 3s 21ms/step - loss: 6.5135 - accuracy: 0.8175 - val_loss: 1.5755 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4666 - accuracy: 0.8673\n",
      "Epoch 2: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 2s 18ms/step - loss: 0.4647 - accuracy: 0.8677 - val_loss: 1.7113 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.4434 - accuracy: 0.8856\n",
      "Epoch 3: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4454 - accuracy: 0.8854 - val_loss: 1.3745 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.8811\n",
      "Epoch 4: val_accuracy improved from 0.26941 to 0.63470, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4221 - accuracy: 0.8814 - val_loss: 0.6563 - val_accuracy: 0.6347 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4183 - accuracy: 0.8811\n",
      "Epoch 5: val_accuracy improved from 0.63470 to 0.88813, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4178 - accuracy: 0.8814 - val_loss: 0.3330 - val_accuracy: 0.8881 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.4148 - accuracy: 0.8957\n",
      "Epoch 6: val_accuracy improved from 0.88813 to 0.89954, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.4162 - accuracy: 0.8953 - val_loss: 0.3357 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8953\n",
      "Epoch 7: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.4118 - accuracy: 0.8953 - val_loss: 0.2835 - val_accuracy: 0.8973 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8947\n",
      "Epoch 8: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3990 - accuracy: 0.8946 - val_loss: 0.3166 - val_accuracy: 0.8950 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3978 - accuracy: 0.9045\n",
      "Epoch 9: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3999 - accuracy: 0.9034 - val_loss: 0.2798 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.9037\n",
      "Epoch 10: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3726 - accuracy: 0.9039 - val_loss: 0.2688 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3481 - accuracy: 0.9109\n",
      "Epoch 11: val_accuracy improved from 0.89954 to 0.90639, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 20ms/step - loss: 0.3501 - accuracy: 0.9103 - val_loss: 0.2962 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.9117\n",
      "Epoch 12: val_accuracy improved from 0.90639 to 0.90868, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.3278 - accuracy: 0.9113 - val_loss: 0.3041 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.9188\n",
      "Epoch 13: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3243 - accuracy: 0.9191 - val_loss: 0.2772 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.9131\n",
      "Epoch 14: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3364 - accuracy: 0.9133 - val_loss: 0.2752 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.3145 - accuracy: 0.9137\n",
      "Epoch 15: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3153 - accuracy: 0.9133 - val_loss: 0.2832 - val_accuracy: 0.8927 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3137 - accuracy: 0.9153\n",
      "Epoch 16: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3137 - accuracy: 0.9153 - val_loss: 0.2786 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3102 - accuracy: 0.9190\n",
      "Epoch 17: val_accuracy improved from 0.90868 to 0.91324, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.3108 - accuracy: 0.9186 - val_loss: 0.2580 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.9260\n",
      "Epoch 18: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.3009 - accuracy: 0.9260 - val_loss: 0.2917 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.9234\n",
      "Epoch 19: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2988 - accuracy: 0.9234 - val_loss: 0.2904 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2789 - accuracy: 0.9285\n",
      "Epoch 20: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2776 - accuracy: 0.9285 - val_loss: 0.2681 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9293\n",
      "Epoch 21: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2745 - accuracy: 0.9293 - val_loss: 0.2754 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9331\n",
      "Epoch 22: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2634 - accuracy: 0.9331 - val_loss: 0.2640 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9352\n",
      "Epoch 23: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2504 - accuracy: 0.9354 - val_loss: 0.3011 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2460 - accuracy: 0.9365\n",
      "Epoch 24: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2439 - accuracy: 0.9371 - val_loss: 0.2893 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.9329\n",
      "Epoch 25: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2518 - accuracy: 0.9333 - val_loss: 0.3055 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9364\n",
      "Epoch 26: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2369 - accuracy: 0.9364 - val_loss: 0.2899 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9326\n",
      "Epoch 27: val_accuracy improved from 0.91324 to 0.92237, saving model to best_model_320.h5\n",
      "124/124 [==============================] - 3s 21ms/step - loss: 0.2585 - accuracy: 0.9326 - val_loss: 0.3218 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2361 - accuracy: 0.9341\n",
      "Epoch 28: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2361 - accuracy: 0.9341 - val_loss: 0.2765 - val_accuracy: 0.9041 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2337 - accuracy: 0.9388\n",
      "Epoch 29: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2331 - accuracy: 0.9389 - val_loss: 0.3112 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.9420\n",
      "Epoch 30: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2080 - accuracy: 0.9420 - val_loss: 0.3165 - val_accuracy: 0.9110 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.2035 - accuracy: 0.9472\n",
      "Epoch 31: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.2048 - accuracy: 0.9470 - val_loss: 0.3112 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1867 - accuracy: 0.9499\n",
      "Epoch 32: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1866 - accuracy: 0.9501 - val_loss: 0.3527 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2095 - accuracy: 0.9456\n",
      "Epoch 33: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2095 - accuracy: 0.9455 - val_loss: 0.3509 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1998 - accuracy: 0.9465\n",
      "Epoch 34: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.2007 - accuracy: 0.9460 - val_loss: 0.3285 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9506\n",
      "Epoch 35: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1851 - accuracy: 0.9506 - val_loss: 0.3227 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9548\n",
      "Epoch 36: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1825 - accuracy: 0.9541 - val_loss: 0.3636 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9503\n",
      "Epoch 37: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1948 - accuracy: 0.9503 - val_loss: 0.3283 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9521\n",
      "Epoch 38: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1815 - accuracy: 0.9521 - val_loss: 0.3428 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1837 - accuracy: 0.9520\n",
      "Epoch 39: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1835 - accuracy: 0.9521 - val_loss: 0.3729 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9607\n",
      "Epoch 40: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1577 - accuracy: 0.9607 - val_loss: 0.4010 - val_accuracy: 0.9155 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1517 - accuracy: 0.9609\n",
      "Epoch 41: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1515 - accuracy: 0.9610 - val_loss: 0.4046 - val_accuracy: 0.9110 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1512 - accuracy: 0.9606\n",
      "Epoch 42: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1512 - accuracy: 0.9607 - val_loss: 0.4019 - val_accuracy: 0.9155 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9610\n",
      "Epoch 43: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1523 - accuracy: 0.9612 - val_loss: 0.3779 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1467 - accuracy: 0.9616\n",
      "Epoch 44: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1474 - accuracy: 0.9615 - val_loss: 0.4207 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "121/124 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9633\n",
      "Epoch 45: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1379 - accuracy: 0.9638 - val_loss: 0.4220 - val_accuracy: 0.9110 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1387 - accuracy: 0.9642\n",
      "Epoch 46: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 19ms/step - loss: 0.1385 - accuracy: 0.9643 - val_loss: 0.4223 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9637Restoring model weights from the end of the best epoch: 27.\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 2s 20ms/step - loss: 0.1343 - accuracy: 0.9638 - val_loss: 0.4452 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 47: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Average Accuracy: 0.931325865135582\n",
      "Average Balanced Accuracy: 0.8983585266866427\n",
      "Average Sensitivity (Sn): 0.8313957317099219\n",
      "Average Specificity (Sp): 0.9653213216633635\n",
      "Average MCC: 0.8163026396414297\n",
      "Average AUC: 0.9512400644160725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import math\n",
    "\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "\n",
    "# Convert back to DataFrames if needed (assuming X_train is still a numpy array)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "# Result collection lists\n",
    "ACC_collection = []\n",
    "BACC_collection = []\n",
    "Sn_collection = []\n",
    "Sp_collection = []\n",
    "MCC_collection = []\n",
    "AUC_collection = []\n",
    "\n",
    "# Function to train the model for each fold\n",
    "def ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV):\n",
    "    # Train the model with the training fold\n",
    "    model = train_model(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
    "    return model\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(y_train):\n",
    "    X_train_CV, X_valid_CV = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_CV, y_valid_CV = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Train the model for this fold\n",
    "    model = ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
    "    \n",
    "    # Load the best model\n",
    "    saved_model = load_model('best_model_320.h5')\n",
    "    \n",
    "    # Predict probabilities\n",
    "    predicted_probabilities = saved_model.predict(X_valid_CV, batch_size=1)\n",
    "    \n",
    "    # Convert probabilities to class predictions\n",
    "    predicted_class = (predicted_probabilities > 0.5).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    y_true = y_valid_CV.values\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Sn = TP / (TP + FN)\n",
    "    Sp = TN / (TN + FP)\n",
    "    MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "    BACC = 0.5 * (Sn + Sp)\n",
    "    AUC = roc_auc_score(y_true, predicted_probabilities)\n",
    "    \n",
    "    # Append metrics to collection lists\n",
    "    ACC_collection.append(ACC)\n",
    "    Sn_collection.append(Sn)\n",
    "    Sp_collection.append(Sp)\n",
    "    MCC_collection.append(MCC)\n",
    "    BACC_collection.append(BACC)\n",
    "    AUC_collection.append(AUC)\n",
    "\n",
    "# Display the results for each fold\n",
    "print(\"Average Accuracy:\", np.mean(ACC_collection))\n",
    "print(\"Average Balanced Accuracy:\", np.mean(BACC_collection))\n",
    "print(\"Average Sensitivity (Sn):\", np.mean(Sn_collection))\n",
    "print(\"Average Specificity (Sp):\", np.mean(Sp_collection))\n",
    "print(\"Average MCC:\", np.mean(MCC_collection))\n",
    "print(\"Average AUC:\", np.mean(AUC_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179d73b1-25b3-4ced-a0e4-a646707b0bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Mean = 0.931325865135582 , Std = 0.008064101787526264\n",
      "Balanced Accuracy: Mean = 0.8983585266866427 , Std = 0.01710341256283681\n",
      "Sensitivity (Sn): Mean = 0.8313957317099219 , Std = 0.03843485013365642\n",
      "Specificity (Sp): Mean = 0.9653213216633635 , Std = 0.009592965096919669\n",
      "MCC: Mean = 0.8163026396414297 , Std = 0.02363667115917629\n",
      "AUC: Mean = 0.9512400644160725 , Std = 0.01013474825958819\n"
     ]
    }
   ],
   "source": [
    "# Display the results for each fold with mean and standard deviation\n",
    "print(\"Accuracy: Mean =\", np.mean(ACC_collection), \", Std =\", np.std(ACC_collection))\n",
    "print(\"Balanced Accuracy: Mean =\", np.mean(BACC_collection), \", Std =\", np.std(BACC_collection))\n",
    "print(\"Sensitivity (Sn): Mean =\", np.mean(Sn_collection), \", Std =\", np.std(Sn_collection))\n",
    "print(\"Specificity (Sp): Mean =\", np.mean(Sp_collection), \", Std =\", np.std(Sp_collection))\n",
    "print(\"MCC: Mean =\", np.mean(MCC_collection), \", Std =\", np.std(MCC_collection))\n",
    "print(\"AUC: Mean =\", np.mean(AUC_collection), \", Std =\", np.std(AUC_collection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5141fb51-a6ef-4f76-91b2-ac2fb5a3d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 9ms/step\n",
      "\n",
      "Optimized Test Dataset Results:\n",
      "Accuracy (ACC): 0.9233576642335767\n",
      "Balanced Accuracy (BACC): 0.8928514889799652\n",
      "Sensitivity (Sn): 0.8309352517985612\n",
      "Specificity (Sp): 0.9547677261613692\n",
      "MCC: 0.7953803978466644\n",
      "AUC: 0.8928514889799651\n",
      "True Positives (TP): 231\n",
      "False Positives (FP): 37\n",
      "True Negatives (TN): 781\n",
      "False Negatives (FN): 47\n",
      "Total Positive: 278\n",
      "Total Negative: 818\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test dataset\n",
    "predicted_probas_test = saved_model.predict(X_test, batch_size=32)\n",
    "best_threshold_test, best_mcc_test = optimize_threshold(y_test, predicted_probas_test)\n",
    "predicted_classes_test = (predicted_probas_test > best_threshold_test).astype(int)\n",
    "\n",
    "# Calculate metrics for the test dataset with optimized threshold\n",
    "accuracy_test = accuracy_score(y_test, predicted_classes_test)\n",
    "sensitivity_test = recall_score(y_test, predicted_classes_test)  # Sensitivity (Recall)\n",
    "TN_test, FP_test, FN_test, TP_test = confusion_matrix(y_test, predicted_classes_test).ravel()\n",
    "specificity_test = TN_test / (TN_test + FP_test)  # Corrected Specificity calculation\n",
    "MCC_test = matthews_corrcoef(y_test, predicted_classes_test)\n",
    "auc_test = roc_auc_score(y_test, predicted_classes_test)\n",
    "\n",
    "# Compute the correct balanced accuracy\n",
    "balanced_accuracy_test = (sensitivity_test + specificity_test) / 2\n",
    "\n",
    "# Print the adjusted results for the test dataset\n",
    "print(\"\\nOptimized Test Dataset Results:\")\n",
    "print(f\"Accuracy (ACC): {accuracy_test}\")\n",
    "print(f\"Balanced Accuracy (BACC): {balanced_accuracy_test}\")\n",
    "print(f\"Sensitivity (Sn): {sensitivity_test}\")\n",
    "print(f\"Specificity (Sp): {specificity_test}\")\n",
    "print(f\"MCC: {MCC_test}\")\n",
    "print(f\"AUC: {auc_test}\")\n",
    "print(f\"True Positives (TP): {TP_test}\")\n",
    "print(f\"False Positives (FP): {FP_test}\")\n",
    "print(f\"True Negatives (TN): {TN_test}\")\n",
    "print(f\"False Negatives (FN): {FN_test}\")\n",
    "\n",
    "# Print the total positive and total negative\n",
    "total_positive = np.sum(y_test)\n",
    "total_negative = len(y_test) - total_positive\n",
    "print(f\"Total Positive: {total_positive}\")\n",
    "print(f\"Total Negative: {total_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc0232f1-1e5f-4a1a-9c9c-46b4dbede579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step\n",
      "\n",
      "Optimized External Dataset (KELM) Results:\n",
      "Accuracy (ACC): 0.8802083333333334\n",
      "Balanced Accuracy (BACC): 0.8802083333333334\n",
      "Sensitivity (Sn): 0.8958333333333334\n",
      "Specificity (Sp): 0.8645833333333334\n",
      "MCC: 0.7607882360348789\n",
      "AUC: 0.8802083333333334\n",
      "True Positives (TP): 86\n",
      "False Positives (FP): 13\n",
      "True Negatives (TN): 83\n",
      "False Negatives (FN): 10\n",
      "Total Positive: 96\n",
      "Total Negative: 96\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the external dataset (KELM)\n",
    "dataset_external = pd.read_csv('kelm_dataset.csv', na_filter=False)\n",
    "X_external_data_name = 'kelm_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_external_data = pd.read_csv(X_external_data_name, header=0, index_col=0, delimiter=',')\n",
    "X_external = np.array(X_external_data)\n",
    "y_external = np.array(dataset_external['label'])\n",
    "\n",
    "# Normalize the external dataset\n",
    "X_external_normalized = scaler.transform(X_external)\n",
    "\n",
    "# Predict probabilities for external dataset\n",
    "predicted_probas_ext = saved_model.predict(X_external_normalized, batch_size=32)\n",
    "best_threshold_ext, best_mcc_ext = optimize_threshold(y_external, predicted_probas_ext)\n",
    "predicted_classes_ext = (predicted_probas_ext > best_threshold_ext).astype(int)\n",
    "\n",
    "# Calculate metrics for the external dataset with optimized threshold\n",
    "accuracy_ext = accuracy_score(y_external, predicted_classes_ext)\n",
    "sensitivity_ext = recall_score(y_external, predicted_classes_ext)  # Sensitivity (Recall)\n",
    "TN_ext, FP_ext, FN_ext, TP_ext = confusion_matrix(y_external, predicted_classes_ext).ravel()\n",
    "specificity_ext = TN_ext / (TN_ext + FP_ext)  # Corrected Specificity calculation\n",
    "MCC_ext = matthews_corrcoef(y_external, predicted_classes_ext)\n",
    "auc_ext = roc_auc_score(y_external, predicted_classes_ext)\n",
    "\n",
    "# Compute the correct balanced accuracy\n",
    "balanced_accuracy_ext = (sensitivity_ext + specificity_ext) / 2\n",
    "\n",
    "# Print the adjusted results for the external dataset\n",
    "print(\"\\nOptimized External Dataset (KELM) Results:\")\n",
    "print(f\"Accuracy (ACC): {accuracy_ext}\")\n",
    "print(f\"Balanced Accuracy (BACC): {balanced_accuracy_ext}\")\n",
    "print(f\"Sensitivity (Sn): {sensitivity_ext}\")\n",
    "print(f\"Specificity (Sp): {specificity_ext}\")\n",
    "print(f\"MCC: {MCC_ext}\")\n",
    "print(f\"AUC: {auc_ext}\")\n",
    "print(f\"True Positives (TP): {TP_ext}\")\n",
    "print(f\"False Positives (FP): {FP_ext}\")\n",
    "print(f\"True Negatives (TN): {TN_ext}\")\n",
    "print(f\"False Negatives (FN): {FN_ext}\")\n",
    "\n",
    "# Print the total positive and total negative\n",
    "total_positive_ext = np.sum(y_external)\n",
    "total_negative_ext = len(y_external) - total_positive_ext\n",
    "print(f\"Total Positive: {total_positive_ext}\")\n",
    "print(f\"Total Negative: {total_negative_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbda3ef9-8f11-4702-8d06-ebec666eafcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_true, y_pred_probas, title='ROC Curve'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_probas)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curve for the test dataset\n",
    "plot_roc_curve(y_test, predicted_probas_test, title='ROC Curve for Test Dataset')\n",
    "\n",
    "# Plot ROC curve for the external dataset\n",
    "plot_roc_curve(y_external, predicted_probas_ext, title='ROC Curve for External Dataset (KELM)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69cf0dd4-6397-465e-b840-05a677fa79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example saving for the test dataset\n",
    "df_test = pd.DataFrame({\n",
    "    'False Positive Rate': fpr_test,\n",
    "    'True Positive Rate': tpr_test,\n",
    "    'ROC AUC': [roc_auc_test] * len(fpr_test)  # AUC is constant across the curve\n",
    "})\n",
    "df_test.to_csv('ESM-320_test_dataset_roc_values.csv', index=False)\n",
    "\n",
    "# Example saving for the external dataset\n",
    "df_ext = pd.DataFrame({\n",
    "    'False Positive Rate': fpr_ext,\n",
    "    'True Positive Rate': tpr_ext,\n",
    "    'ROC AUC': [roc_auc_ext] * len(fpr_ext)  # AUC is constant across the curve\n",
    "})\n",
    "df_ext.to_csv('ESM-320_external_dataset_roc_values.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da775c3-406f-425a-ab20-a26a2e6dd518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
