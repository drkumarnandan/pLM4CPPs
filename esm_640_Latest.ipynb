{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6105a668-7319-45df-add9-f47fd3f90b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "137/137 [==============================] - ETA: 0s - loss: 9.5649 - accuracy: 0.7990\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25365, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 7s 41ms/step - loss: 9.5649 - accuracy: 0.7990 - val_loss: 5.1860 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.8677\n",
      "Epoch 2: val_accuracy did not improve from 0.25365\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.5297 - accuracy: 0.8677 - val_loss: 3.4243 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4894 - accuracy: 0.8686\n",
      "Epoch 3: val_accuracy did not improve from 0.25365\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.4894 - accuracy: 0.8686 - val_loss: 1.6071 - val_accuracy: 0.2536 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.8811\n",
      "Epoch 4: val_accuracy improved from 0.25365 to 0.61953, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 40ms/step - loss: 0.4791 - accuracy: 0.8811 - val_loss: 0.7452 - val_accuracy: 0.6195 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4831 - accuracy: 0.8779\n",
      "Epoch 5: val_accuracy improved from 0.61953 to 0.88412, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 41ms/step - loss: 0.4831 - accuracy: 0.8779 - val_loss: 0.3250 - val_accuracy: 0.8841 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4851 - accuracy: 0.8795\n",
      "Epoch 6: val_accuracy improved from 0.88412 to 0.90328, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 41ms/step - loss: 0.4851 - accuracy: 0.8795 - val_loss: 0.3379 - val_accuracy: 0.9033 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4717 - accuracy: 0.8866\n",
      "Epoch 7: val_accuracy did not improve from 0.90328\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.4717 - accuracy: 0.8866 - val_loss: 0.3560 - val_accuracy: 0.9005 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.8937\n",
      "Epoch 8: val_accuracy did not improve from 0.90328\n",
      "137/137 [==============================] - 5s 37ms/step - loss: 0.4579 - accuracy: 0.8937 - val_loss: 0.3363 - val_accuracy: 0.8951 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.4512 - accuracy: 0.8927\n",
      "Epoch 9: val_accuracy improved from 0.90328 to 0.90693, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.4500 - accuracy: 0.8928 - val_loss: 0.2812 - val_accuracy: 0.9069 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.9055\n",
      "Epoch 10: val_accuracy did not improve from 0.90693\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.4089 - accuracy: 0.9055 - val_loss: 0.3447 - val_accuracy: 0.9051 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.9005\n",
      "Epoch 11: val_accuracy improved from 0.90693 to 0.91332, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 41ms/step - loss: 0.4246 - accuracy: 0.9005 - val_loss: 0.3202 - val_accuracy: 0.9133 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.8994\n",
      "Epoch 12: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.4065 - accuracy: 0.8994 - val_loss: 0.3329 - val_accuracy: 0.9115 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy: 0.9014\n",
      "Epoch 13: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.4008 - accuracy: 0.9019 - val_loss: 0.2903 - val_accuracy: 0.9060 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3948 - accuracy: 0.9023\n",
      "Epoch 14: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.3959 - accuracy: 0.9021 - val_loss: 0.3071 - val_accuracy: 0.8942 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3971 - accuracy: 0.9062\n",
      "Epoch 15: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.3971 - accuracy: 0.9062 - val_loss: 0.3440 - val_accuracy: 0.9015 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.9131\n",
      "Epoch 16: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.3731 - accuracy: 0.9131 - val_loss: 0.3003 - val_accuracy: 0.9033 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.9071\n",
      "Epoch 17: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.3910 - accuracy: 0.9071 - val_loss: 0.2695 - val_accuracy: 0.9060 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3672 - accuracy: 0.9101\n",
      "Epoch 18: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.3672 - accuracy: 0.9101 - val_loss: 0.3021 - val_accuracy: 0.9042 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.9122\n",
      "Epoch 19: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.3690 - accuracy: 0.9122 - val_loss: 0.2900 - val_accuracy: 0.9024 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3432 - accuracy: 0.9235\n",
      "Epoch 20: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.3422 - accuracy: 0.9238 - val_loss: 0.2963 - val_accuracy: 0.9115 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.9222\n",
      "Epoch 21: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.3240 - accuracy: 0.9222 - val_loss: 0.2941 - val_accuracy: 0.9115 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.9222\n",
      "Epoch 22: val_accuracy did not improve from 0.91332\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.3219 - accuracy: 0.9222 - val_loss: 0.2742 - val_accuracy: 0.9097 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3203 - accuracy: 0.9231\n",
      "Epoch 23: val_accuracy improved from 0.91332 to 0.91423, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 42ms/step - loss: 0.3203 - accuracy: 0.9231 - val_loss: 0.2871 - val_accuracy: 0.9142 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.9236\n",
      "Epoch 24: val_accuracy did not improve from 0.91423\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.3108 - accuracy: 0.9236 - val_loss: 0.3140 - val_accuracy: 0.9042 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3086 - accuracy: 0.9261\n",
      "Epoch 25: val_accuracy did not improve from 0.91423\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.3086 - accuracy: 0.9261 - val_loss: 0.3165 - val_accuracy: 0.8996 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.9272\n",
      "Epoch 26: val_accuracy improved from 0.91423 to 0.91788, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 41ms/step - loss: 0.2948 - accuracy: 0.9272 - val_loss: 0.2999 - val_accuracy: 0.9179 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.9261\n",
      "Epoch 27: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.2990 - accuracy: 0.9261 - val_loss: 0.2692 - val_accuracy: 0.9115 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.9233\n",
      "Epoch 28: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.3132 - accuracy: 0.9236 - val_loss: 0.2686 - val_accuracy: 0.9097 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.9214\n",
      "Epoch 29: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.3002 - accuracy: 0.9215 - val_loss: 0.2863 - val_accuracy: 0.9179 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.9368\n",
      "Epoch 30: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.2694 - accuracy: 0.9363 - val_loss: 0.3227 - val_accuracy: 0.9151 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9357\n",
      "Epoch 31: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2658 - accuracy: 0.9357 - val_loss: 0.3499 - val_accuracy: 0.9051 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9327\n",
      "Epoch 32: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2692 - accuracy: 0.9327 - val_loss: 0.3404 - val_accuracy: 0.9151 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9357\n",
      "Epoch 33: val_accuracy did not improve from 0.91788\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.2533 - accuracy: 0.9357 - val_loss: 0.3168 - val_accuracy: 0.9170 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.9373\n",
      "Epoch 34: val_accuracy improved from 0.91788 to 0.91880, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 40ms/step - loss: 0.2558 - accuracy: 0.9373 - val_loss: 0.3233 - val_accuracy: 0.9188 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9400\n",
      "Epoch 35: val_accuracy did not improve from 0.91880\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.2411 - accuracy: 0.9400 - val_loss: 0.3180 - val_accuracy: 0.9188 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9436\n",
      "Epoch 36: val_accuracy did not improve from 0.91880\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.2461 - accuracy: 0.9436 - val_loss: 0.3131 - val_accuracy: 0.9142 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9430\n",
      "Epoch 37: val_accuracy improved from 0.91880 to 0.92062, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 41ms/step - loss: 0.2421 - accuracy: 0.9430 - val_loss: 0.3519 - val_accuracy: 0.9206 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9420\n",
      "Epoch 38: val_accuracy did not improve from 0.92062\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2339 - accuracy: 0.9420 - val_loss: 0.3434 - val_accuracy: 0.9197 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.9420\n",
      "Epoch 39: val_accuracy did not improve from 0.92062\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.2399 - accuracy: 0.9420 - val_loss: 0.3610 - val_accuracy: 0.9161 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2199 - accuracy: 0.9474\n",
      "Epoch 40: val_accuracy did not improve from 0.92062\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2187 - accuracy: 0.9478 - val_loss: 0.3496 - val_accuracy: 0.9188 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9475\n",
      "Epoch 41: val_accuracy did not improve from 0.92062\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2205 - accuracy: 0.9475 - val_loss: 0.3404 - val_accuracy: 0.9179 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2062 - accuracy: 0.9529\n",
      "Epoch 42: val_accuracy improved from 0.92062 to 0.92245, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 41ms/step - loss: 0.2054 - accuracy: 0.9532 - val_loss: 0.3318 - val_accuracy: 0.9224 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9484\n",
      "Epoch 43: val_accuracy did not improve from 0.92245\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2076 - accuracy: 0.9484 - val_loss: 0.3696 - val_accuracy: 0.9215 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9516\n",
      "Epoch 44: val_accuracy did not improve from 0.92245\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.1951 - accuracy: 0.9516 - val_loss: 0.3777 - val_accuracy: 0.9215 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9531\n",
      "Epoch 45: val_accuracy did not improve from 0.92245\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2033 - accuracy: 0.9530 - val_loss: 0.3825 - val_accuracy: 0.9161 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2129 - accuracy: 0.9467\n",
      "Epoch 46: val_accuracy did not improve from 0.92245\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2120 - accuracy: 0.9468 - val_loss: 0.3660 - val_accuracy: 0.9206 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9493\n",
      "Epoch 47: val_accuracy did not improve from 0.92245\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.1977 - accuracy: 0.9493 - val_loss: 0.3864 - val_accuracy: 0.9215 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9487\n",
      "Epoch 48: val_accuracy improved from 0.92245 to 0.92336, saving model to best_model_640.h5\n",
      "137/137 [==============================] - 6s 42ms/step - loss: 0.2050 - accuracy: 0.9487 - val_loss: 0.3801 - val_accuracy: 0.9234 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9491\n",
      "Epoch 49: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.2033 - accuracy: 0.9491 - val_loss: 0.3736 - val_accuracy: 0.9215 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1910 - accuracy: 0.9551\n",
      "Epoch 50: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1910 - accuracy: 0.9551 - val_loss: 0.3884 - val_accuracy: 0.9197 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9582\n",
      "Epoch 51: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.1782 - accuracy: 0.9585 - val_loss: 0.4000 - val_accuracy: 0.9234 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9568\n",
      "Epoch 52: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1853 - accuracy: 0.9567 - val_loss: 0.4203 - val_accuracy: 0.9197 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9562\n",
      "Epoch 53: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1839 - accuracy: 0.9562 - val_loss: 0.4088 - val_accuracy: 0.9224 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9551\n",
      "Epoch 54: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1797 - accuracy: 0.9551 - val_loss: 0.4170 - val_accuracy: 0.9197 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9560\n",
      "Epoch 55: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1874 - accuracy: 0.9560 - val_loss: 0.4162 - val_accuracy: 0.9197 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9601\n",
      "Epoch 56: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1665 - accuracy: 0.9601 - val_loss: 0.4309 - val_accuracy: 0.9170 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1736 - accuracy: 0.9598\n",
      "Epoch 57: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1736 - accuracy: 0.9598 - val_loss: 0.4242 - val_accuracy: 0.9224 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9567\n",
      "Epoch 58: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.1744 - accuracy: 0.9567 - val_loss: 0.4468 - val_accuracy: 0.9206 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9601\n",
      "Epoch 59: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.1719 - accuracy: 0.9601 - val_loss: 0.4383 - val_accuracy: 0.9197 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9601\n",
      "Epoch 60: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.1649 - accuracy: 0.9601 - val_loss: 0.4552 - val_accuracy: 0.9170 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1636 - accuracy: 0.9616\n",
      "Epoch 61: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.1632 - accuracy: 0.9619 - val_loss: 0.4599 - val_accuracy: 0.9188 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9624\n",
      "Epoch 62: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.1649 - accuracy: 0.9624 - val_loss: 0.4707 - val_accuracy: 0.9179 - lr: 1.5625e-04\n",
      "Epoch 63/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9614\n",
      "Epoch 63: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1631 - accuracy: 0.9612 - val_loss: 0.4652 - val_accuracy: 0.9179 - lr: 1.5625e-04\n",
      "Epoch 64/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9601\n",
      "Epoch 64: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1622 - accuracy: 0.9601 - val_loss: 0.4576 - val_accuracy: 0.9170 - lr: 1.5625e-04\n",
      "Epoch 65/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9594\n",
      "Epoch 65: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1667 - accuracy: 0.9594 - val_loss: 0.4543 - val_accuracy: 0.9206 - lr: 1.5625e-04\n",
      "Epoch 66/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1520 - accuracy: 0.9644\n",
      "Epoch 66: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.1513 - accuracy: 0.9644 - val_loss: 0.4737 - val_accuracy: 0.9197 - lr: 1.5625e-04\n",
      "Epoch 67/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9624\n",
      "Epoch 67: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 5s 40ms/step - loss: 0.1641 - accuracy: 0.9624 - val_loss: 0.4655 - val_accuracy: 0.9197 - lr: 1.5625e-04\n",
      "Epoch 68/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9621Restoring model weights from the end of the best epoch: 48.\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.92336\n",
      "137/137 [==============================] - 6s 40ms/step - loss: 0.1608 - accuracy: 0.9624 - val_loss: 0.4661 - val_accuracy: 0.9197 - lr: 1.5625e-04\n",
      "Epoch 68: early stopping\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_excel('Final_non_redundant_sequences.xlsx', na_filter=False)\n",
    "X_data_name = 'whole_sample_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv'\n",
    "X_data = pd.read_csv(X_data_name, header=0, index_col=0, delimiter=',')\n",
    "X = np.array(X_data)\n",
    "y = np.array(dataset['label'])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "def build_model(input_shape):\n",
    "    input = Input(input_shape)\n",
    "    x = Conv1D(64, 5, strides=1, padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv1D(128, 5, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr * np.power(drop, np.floor((1 + epoch) / epochs_drop))\n",
    "    return lr\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    input_shape = (640, 1)\n",
    "    model = build_model(input_shape)\n",
    "    \n",
    "    # Optimizer\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, restore_best_weights=True)\n",
    "    mc = ModelCheckpoint('best_model_640.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    callbacks_list = [lrate, early_stop, mc]\n",
    "    \n",
    "    class_weight = {0: 1, 1: 2}  # Adjust the weights as needed\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, callbacks=callbacks_list, batch_size=32, class_weight=class_weight)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Load the best model\n",
    "saved_model = load_model('best_model_640.h5')\n",
    "\n",
    "# Function to optimize threshold based on MCC\n",
    "def optimize_threshold(y_true, y_pred_probas):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    best_mcc = -1\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_probas > threshold).astype(int)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        \n",
    "        if mcc > best_mcc:\n",
    "            best_mcc = mcc\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0cf6ed2-1920-49d8-b27d-aa1f64d7cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 8.3173 - accuracy: 0.8161\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24601, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 42ms/step - loss: 8.3012 - accuracy: 0.8162 - val_loss: 2.5564 - val_accuracy: 0.2460 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4652 - accuracy: 0.8697\n",
      "Epoch 2: val_accuracy did not improve from 0.24601\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4652 - accuracy: 0.8697 - val_loss: 4.2959 - val_accuracy: 0.2460 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4547 - accuracy: 0.8679\n",
      "Epoch 3: val_accuracy improved from 0.24601 to 0.25285, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4544 - accuracy: 0.8679 - val_loss: 1.8547 - val_accuracy: 0.2528 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4586 - accuracy: 0.8577\n",
      "Epoch 4: val_accuracy improved from 0.25285 to 0.79727, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4588 - accuracy: 0.8578 - val_loss: 0.5617 - val_accuracy: 0.7973 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4738 - accuracy: 0.8793\n",
      "Epoch 5: val_accuracy improved from 0.79727 to 0.88838, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4732 - accuracy: 0.8796 - val_loss: 0.2653 - val_accuracy: 0.8884 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4443 - accuracy: 0.8913\n",
      "Epoch 6: val_accuracy improved from 0.88838 to 0.90888, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4446 - accuracy: 0.8912 - val_loss: 0.2467 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4588 - accuracy: 0.8841\n",
      "Epoch 7: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4586 - accuracy: 0.8841 - val_loss: 0.2913 - val_accuracy: 0.9066 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4700 - accuracy: 0.8844\n",
      "Epoch 8: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4695 - accuracy: 0.8846 - val_loss: 0.2782 - val_accuracy: 0.8884 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4811 - accuracy: 0.8852\n",
      "Epoch 9: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4805 - accuracy: 0.8854 - val_loss: 0.3890 - val_accuracy: 0.8815 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4354 - accuracy: 0.8999\n",
      "Epoch 10: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4348 - accuracy: 0.9001 - val_loss: 0.3036 - val_accuracy: 0.8838 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3905 - accuracy: 0.9080\n",
      "Epoch 11: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3905 - accuracy: 0.9080 - val_loss: 0.2495 - val_accuracy: 0.9043 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4010 - accuracy: 0.9062\n",
      "Epoch 12: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4006 - accuracy: 0.9064 - val_loss: 0.2568 - val_accuracy: 0.8952 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3814 - accuracy: 0.9069\n",
      "Epoch 13: val_accuracy did not improve from 0.90888\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3814 - accuracy: 0.9069 - val_loss: 0.2574 - val_accuracy: 0.9021 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3738 - accuracy: 0.9096\n",
      "Epoch 14: val_accuracy improved from 0.90888 to 0.92027, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.3740 - accuracy: 0.9095 - val_loss: 0.2446 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.9129\n",
      "Epoch 15: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3596 - accuracy: 0.9128 - val_loss: 0.2429 - val_accuracy: 0.8952 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.9110\n",
      "Epoch 16: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3684 - accuracy: 0.9110 - val_loss: 0.2718 - val_accuracy: 0.9021 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.9153\n",
      "Epoch 17: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3527 - accuracy: 0.9153 - val_loss: 0.3768 - val_accuracy: 0.8770 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3469 - accuracy: 0.9158\n",
      "Epoch 18: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3469 - accuracy: 0.9158 - val_loss: 0.2905 - val_accuracy: 0.8952 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3346 - accuracy: 0.9182\n",
      "Epoch 19: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3341 - accuracy: 0.9181 - val_loss: 0.2843 - val_accuracy: 0.8929 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.9237\n",
      "Epoch 20: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3177 - accuracy: 0.9237 - val_loss: 0.2918 - val_accuracy: 0.8907 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.9243\n",
      "Epoch 21: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2992 - accuracy: 0.9244 - val_loss: 0.2761 - val_accuracy: 0.8907 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.9249\n",
      "Epoch 22: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2971 - accuracy: 0.9249 - val_loss: 0.2723 - val_accuracy: 0.8975 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2813 - accuracy: 0.9317\n",
      "Epoch 23: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2810 - accuracy: 0.9318 - val_loss: 0.2316 - val_accuracy: 0.9157 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9303\n",
      "Epoch 24: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2845 - accuracy: 0.9303 - val_loss: 0.2963 - val_accuracy: 0.8907 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.9296\n",
      "Epoch 25: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2706 - accuracy: 0.9298 - val_loss: 0.2934 - val_accuracy: 0.9066 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.9268\n",
      "Epoch 26: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2706 - accuracy: 0.9265 - val_loss: 0.2548 - val_accuracy: 0.9157 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2721 - accuracy: 0.9322\n",
      "Epoch 27: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2726 - accuracy: 0.9320 - val_loss: 0.2609 - val_accuracy: 0.9043 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9376\n",
      "Epoch 28: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2577 - accuracy: 0.9376 - val_loss: 0.3266 - val_accuracy: 0.8952 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2408 - accuracy: 0.9409\n",
      "Epoch 29: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2408 - accuracy: 0.9409 - val_loss: 0.2463 - val_accuracy: 0.9180 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2223 - accuracy: 0.9482\n",
      "Epoch 30: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2233 - accuracy: 0.9478 - val_loss: 0.2811 - val_accuracy: 0.9043 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2205 - accuracy: 0.9431\n",
      "Epoch 31: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2210 - accuracy: 0.9430 - val_loss: 0.3084 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.9462\n",
      "Epoch 32: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2192 - accuracy: 0.9462 - val_loss: 0.2493 - val_accuracy: 0.9203 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9472\n",
      "Epoch 33: val_accuracy improved from 0.92027 to 0.92483, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.2154 - accuracy: 0.9470 - val_loss: 0.2405 - val_accuracy: 0.9248 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2092 - accuracy: 0.9482\n",
      "Epoch 34: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2089 - accuracy: 0.9483 - val_loss: 0.2796 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9512\n",
      "Epoch 35: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2048 - accuracy: 0.9513 - val_loss: 0.3265 - val_accuracy: 0.9021 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2000 - accuracy: 0.9502\n",
      "Epoch 36: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2000 - accuracy: 0.9503 - val_loss: 0.3462 - val_accuracy: 0.9043 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1912 - accuracy: 0.9520\n",
      "Epoch 37: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1909 - accuracy: 0.9521 - val_loss: 0.3300 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9513\n",
      "Epoch 38: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1955 - accuracy: 0.9513 - val_loss: 0.3459 - val_accuracy: 0.9021 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9531\n",
      "Epoch 39: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1853 - accuracy: 0.9531 - val_loss: 0.3224 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1843 - accuracy: 0.9560\n",
      "Epoch 40: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1844 - accuracy: 0.9561 - val_loss: 0.3335 - val_accuracy: 0.9180 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9599\n",
      "Epoch 41: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1696 - accuracy: 0.9599 - val_loss: 0.3537 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9615\n",
      "Epoch 42: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1645 - accuracy: 0.9615 - val_loss: 0.3119 - val_accuracy: 0.9248 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1743 - accuracy: 0.9572\n",
      "Epoch 43: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1743 - accuracy: 0.9572 - val_loss: 0.3296 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1579 - accuracy: 0.9614\n",
      "Epoch 44: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1578 - accuracy: 0.9615 - val_loss: 0.3253 - val_accuracy: 0.9157 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9610\n",
      "Epoch 45: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1626 - accuracy: 0.9610 - val_loss: 0.3708 - val_accuracy: 0.9180 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1608 - accuracy: 0.9609\n",
      "Epoch 46: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1604 - accuracy: 0.9610 - val_loss: 0.3419 - val_accuracy: 0.9203 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9652\n",
      "Epoch 47: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1568 - accuracy: 0.9650 - val_loss: 0.3835 - val_accuracy: 0.9157 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1536 - accuracy: 0.9629\n",
      "Epoch 48: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1535 - accuracy: 0.9630 - val_loss: 0.3706 - val_accuracy: 0.9157 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9609\n",
      "Epoch 49: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1556 - accuracy: 0.9610 - val_loss: 0.3240 - val_accuracy: 0.9180 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.9670\n",
      "Epoch 50: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1445 - accuracy: 0.9670 - val_loss: 0.3905 - val_accuracy: 0.9089 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9649\n",
      "Epoch 51: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1486 - accuracy: 0.9650 - val_loss: 0.3589 - val_accuracy: 0.9157 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1450 - accuracy: 0.9665\n",
      "Epoch 52: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1450 - accuracy: 0.9665 - val_loss: 0.3847 - val_accuracy: 0.9157 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9625Restoring model weights from the end of the best epoch: 33.\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.92483\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1531 - accuracy: 0.9625 - val_loss: 0.3928 - val_accuracy: 0.9226 - lr: 3.1250e-04\n",
      "Epoch 53: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 13.3321 - accuracy: 0.8048\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88383, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 41ms/step - loss: 13.3321 - accuracy: 0.8048 - val_loss: 0.6229 - val_accuracy: 0.8838 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4774 - accuracy: 0.8737\n",
      "Epoch 2: val_accuracy did not improve from 0.88383\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4775 - accuracy: 0.8735 - val_loss: 0.5534 - val_accuracy: 0.8451 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4581 - accuracy: 0.8745\n",
      "Epoch 3: val_accuracy did not improve from 0.88383\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4584 - accuracy: 0.8745 - val_loss: 0.3422 - val_accuracy: 0.8679 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.8829\n",
      "Epoch 4: val_accuracy improved from 0.88383 to 0.88610, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4465 - accuracy: 0.8829 - val_loss: 0.3189 - val_accuracy: 0.8861 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4366 - accuracy: 0.8849\n",
      "Epoch 5: val_accuracy improved from 0.88610 to 0.89294, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4365 - accuracy: 0.8849 - val_loss: 0.3315 - val_accuracy: 0.8929 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4516 - accuracy: 0.8869\n",
      "Epoch 6: val_accuracy did not improve from 0.89294\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4510 - accuracy: 0.8872 - val_loss: 0.3665 - val_accuracy: 0.8838 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4471 - accuracy: 0.8895\n",
      "Epoch 7: val_accuracy did not improve from 0.89294\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.4467 - accuracy: 0.8895 - val_loss: 0.2927 - val_accuracy: 0.8747 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4517 - accuracy: 0.8935\n",
      "Epoch 8: val_accuracy did not improve from 0.89294\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4525 - accuracy: 0.8933 - val_loss: 0.2882 - val_accuracy: 0.8861 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4510 - accuracy: 0.8908\n",
      "Epoch 9: val_accuracy did not improve from 0.89294\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4512 - accuracy: 0.8907 - val_loss: 0.3154 - val_accuracy: 0.8815 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4378 - accuracy: 0.8925\n",
      "Epoch 10: val_accuracy did not improve from 0.89294\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4372 - accuracy: 0.8927 - val_loss: 0.2601 - val_accuracy: 0.8929 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.8968\n",
      "Epoch 11: val_accuracy improved from 0.89294 to 0.91572, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3938 - accuracy: 0.8971 - val_loss: 0.2532 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3967 - accuracy: 0.8976\n",
      "Epoch 12: val_accuracy improved from 0.91572 to 0.91800, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.3964 - accuracy: 0.8976 - val_loss: 0.2422 - val_accuracy: 0.9180 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3851 - accuracy: 0.9002\n",
      "Epoch 13: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3845 - accuracy: 0.9004 - val_loss: 0.2528 - val_accuracy: 0.9112 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3737 - accuracy: 0.9062\n",
      "Epoch 14: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3737 - accuracy: 0.9062 - val_loss: 0.2661 - val_accuracy: 0.8907 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.9047\n",
      "Epoch 15: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3672 - accuracy: 0.9049 - val_loss: 0.2595 - val_accuracy: 0.9021 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3518 - accuracy: 0.9096\n",
      "Epoch 16: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3523 - accuracy: 0.9090 - val_loss: 0.2652 - val_accuracy: 0.9089 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.9042\n",
      "Epoch 17: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3795 - accuracy: 0.9039 - val_loss: 0.2352 - val_accuracy: 0.9157 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.9111\n",
      "Epoch 18: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3595 - accuracy: 0.9110 - val_loss: 0.2595 - val_accuracy: 0.8975 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.9144\n",
      "Epoch 19: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3379 - accuracy: 0.9143 - val_loss: 0.2460 - val_accuracy: 0.9021 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3283 - accuracy: 0.9162\n",
      "Epoch 20: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3279 - accuracy: 0.9163 - val_loss: 0.2372 - val_accuracy: 0.9180 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3154 - accuracy: 0.9240\n",
      "Epoch 21: val_accuracy improved from 0.91800 to 0.93166, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.3167 - accuracy: 0.9239 - val_loss: 0.2047 - val_accuracy: 0.9317 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.9291\n",
      "Epoch 22: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3044 - accuracy: 0.9290 - val_loss: 0.2403 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.9305\n",
      "Epoch 23: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2972 - accuracy: 0.9305 - val_loss: 0.2128 - val_accuracy: 0.9180 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.9215\n",
      "Epoch 24: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3062 - accuracy: 0.9214 - val_loss: 0.2086 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.9219\n",
      "Epoch 25: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2960 - accuracy: 0.9219 - val_loss: 0.2250 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9261\n",
      "Epoch 26: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2870 - accuracy: 0.9260 - val_loss: 0.2131 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2744 - accuracy: 0.9342\n",
      "Epoch 27: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2744 - accuracy: 0.9341 - val_loss: 0.2464 - val_accuracy: 0.9226 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9256\n",
      "Epoch 28: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2942 - accuracy: 0.9255 - val_loss: 0.2325 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2734 - accuracy: 0.9286\n",
      "Epoch 29: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2740 - accuracy: 0.9282 - val_loss: 0.2096 - val_accuracy: 0.9294 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2553 - accuracy: 0.9347\n",
      "Epoch 30: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2548 - accuracy: 0.9348 - val_loss: 0.2133 - val_accuracy: 0.9271 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.9390\n",
      "Epoch 31: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2407 - accuracy: 0.9386 - val_loss: 0.2084 - val_accuracy: 0.9317 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.9376\n",
      "Epoch 32: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2452 - accuracy: 0.9376 - val_loss: 0.2077 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.9436\n",
      "Epoch 33: val_accuracy did not improve from 0.93166\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2190 - accuracy: 0.9437 - val_loss: 0.2162 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9442\n",
      "Epoch 34: val_accuracy improved from 0.93166 to 0.93394, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.2171 - accuracy: 0.9442 - val_loss: 0.2157 - val_accuracy: 0.9339 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9452\n",
      "Epoch 35: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.2145 - accuracy: 0.9452 - val_loss: 0.2068 - val_accuracy: 0.9317 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2336 - accuracy: 0.9384\n",
      "Epoch 36: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2336 - accuracy: 0.9384 - val_loss: 0.2038 - val_accuracy: 0.9317 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2258 - accuracy: 0.9417\n",
      "Epoch 37: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2258 - accuracy: 0.9417 - val_loss: 0.2230 - val_accuracy: 0.9180 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.9479\n",
      "Epoch 38: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2164 - accuracy: 0.9478 - val_loss: 0.2022 - val_accuracy: 0.9294 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2134 - accuracy: 0.9454\n",
      "Epoch 39: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2131 - accuracy: 0.9455 - val_loss: 0.2194 - val_accuracy: 0.9271 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9492\n",
      "Epoch 40: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2049 - accuracy: 0.9493 - val_loss: 0.2159 - val_accuracy: 0.9339 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1899 - accuracy: 0.9513\n",
      "Epoch 41: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1899 - accuracy: 0.9513 - val_loss: 0.2075 - val_accuracy: 0.9339 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9503\n",
      "Epoch 42: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1901 - accuracy: 0.9503 - val_loss: 0.2221 - val_accuracy: 0.9294 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1792 - accuracy: 0.9539\n",
      "Epoch 43: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1792 - accuracy: 0.9539 - val_loss: 0.2133 - val_accuracy: 0.9271 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9556\n",
      "Epoch 44: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1765 - accuracy: 0.9556 - val_loss: 0.2057 - val_accuracy: 0.9339 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1781 - accuracy: 0.9535\n",
      "Epoch 45: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1786 - accuracy: 0.9533 - val_loss: 0.2221 - val_accuracy: 0.9271 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9501\n",
      "Epoch 46: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1861 - accuracy: 0.9501 - val_loss: 0.2177 - val_accuracy: 0.9317 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9553\n",
      "Epoch 47: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1776 - accuracy: 0.9554 - val_loss: 0.2110 - val_accuracy: 0.9294 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9544\n",
      "Epoch 48: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1795 - accuracy: 0.9544 - val_loss: 0.2182 - val_accuracy: 0.9248 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9582\n",
      "Epoch 49: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1716 - accuracy: 0.9582 - val_loss: 0.2128 - val_accuracy: 0.9248 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9588\n",
      "Epoch 50: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1674 - accuracy: 0.9589 - val_loss: 0.2091 - val_accuracy: 0.9248 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9596\n",
      "Epoch 51: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1560 - accuracy: 0.9597 - val_loss: 0.2209 - val_accuracy: 0.9271 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9579\n",
      "Epoch 52: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1615 - accuracy: 0.9579 - val_loss: 0.2106 - val_accuracy: 0.9294 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.9621\n",
      "Epoch 53: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1576 - accuracy: 0.9620 - val_loss: 0.2184 - val_accuracy: 0.9248 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9597Restoring model weights from the end of the best epoch: 34.\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.93394\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1508 - accuracy: 0.9597 - val_loss: 0.2247 - val_accuracy: 0.9248 - lr: 3.1250e-04\n",
      "Epoch 54: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 12.8090 - accuracy: 0.7518\n",
      "Epoch 1: val_accuracy improved from -inf to 0.73576, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 42ms/step - loss: 12.7841 - accuracy: 0.7520 - val_loss: 0.6522 - val_accuracy: 0.7358 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.8294\n",
      "Epoch 2: val_accuracy did not improve from 0.73576\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.6368 - accuracy: 0.8294 - val_loss: 3.9315 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.8268\n",
      "Epoch 3: val_accuracy did not improve from 0.73576\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.6281 - accuracy: 0.8268 - val_loss: 1.4905 - val_accuracy: 0.2642 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5723 - accuracy: 0.8483\n",
      "Epoch 4: val_accuracy did not improve from 0.73576\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5722 - accuracy: 0.8484 - val_loss: 0.6547 - val_accuracy: 0.6150 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5735 - accuracy: 0.8438\n",
      "Epoch 5: val_accuracy improved from 0.73576 to 0.90205, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5733 - accuracy: 0.8438 - val_loss: 0.3469 - val_accuracy: 0.9021 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5860 - accuracy: 0.8440\n",
      "Epoch 6: val_accuracy improved from 0.90205 to 0.90661, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.5858 - accuracy: 0.8441 - val_loss: 0.3065 - val_accuracy: 0.9066 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.8479\n",
      "Epoch 7: val_accuracy did not improve from 0.90661\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5798 - accuracy: 0.8479 - val_loss: 0.3124 - val_accuracy: 0.9066 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5416 - accuracy: 0.8557\n",
      "Epoch 8: val_accuracy did not improve from 0.90661\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5415 - accuracy: 0.8557 - val_loss: 0.3229 - val_accuracy: 0.8907 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5302 - accuracy: 0.8628\n",
      "Epoch 9: val_accuracy did not improve from 0.90661\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5302 - accuracy: 0.8628 - val_loss: 0.2941 - val_accuracy: 0.8929 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.8661\n",
      "Epoch 10: val_accuracy improved from 0.90661 to 0.91800, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5133 - accuracy: 0.8661 - val_loss: 0.2936 - val_accuracy: 0.9180 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5316 - accuracy: 0.8600\n",
      "Epoch 11: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5316 - accuracy: 0.8600 - val_loss: 0.3436 - val_accuracy: 0.8929 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5159 - accuracy: 0.8697\n",
      "Epoch 12: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5158 - accuracy: 0.8697 - val_loss: 0.3386 - val_accuracy: 0.9134 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5312 - accuracy: 0.8613\n",
      "Epoch 13: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5312 - accuracy: 0.8613 - val_loss: 0.3353 - val_accuracy: 0.9066 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5254 - accuracy: 0.8628\n",
      "Epoch 14: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.5250 - accuracy: 0.8631 - val_loss: 0.2912 - val_accuracy: 0.9021 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5090 - accuracy: 0.8689\n",
      "Epoch 15: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.5086 - accuracy: 0.8692 - val_loss: 0.2803 - val_accuracy: 0.9180 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5032 - accuracy: 0.8727\n",
      "Epoch 16: val_accuracy did not improve from 0.91800\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5038 - accuracy: 0.8725 - val_loss: 0.2821 - val_accuracy: 0.9066 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.8712\n",
      "Epoch 17: val_accuracy improved from 0.91800 to 0.92027, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4952 - accuracy: 0.8715 - val_loss: 0.2770 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5157 - accuracy: 0.8641\n",
      "Epoch 18: val_accuracy did not improve from 0.92027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5151 - accuracy: 0.8644 - val_loss: 0.3304 - val_accuracy: 0.9203 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.8646\n",
      "Epoch 19: val_accuracy improved from 0.92027 to 0.92938, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5191 - accuracy: 0.8646 - val_loss: 0.3048 - val_accuracy: 0.9294 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4868 - accuracy: 0.8750\n",
      "Epoch 20: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4865 - accuracy: 0.8750 - val_loss: 0.2344 - val_accuracy: 0.9134 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4759 - accuracy: 0.8750\n",
      "Epoch 21: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4759 - accuracy: 0.8750 - val_loss: 0.2473 - val_accuracy: 0.9180 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.8803\n",
      "Epoch 22: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4787 - accuracy: 0.8803 - val_loss: 0.2728 - val_accuracy: 0.9134 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4967 - accuracy: 0.8702\n",
      "Epoch 23: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4967 - accuracy: 0.8702 - val_loss: 0.2875 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.8745\n",
      "Epoch 24: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4872 - accuracy: 0.8745 - val_loss: 0.2421 - val_accuracy: 0.9248 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.8780\n",
      "Epoch 25: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4813 - accuracy: 0.8780 - val_loss: 0.2604 - val_accuracy: 0.9157 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4509 - accuracy: 0.8808\n",
      "Epoch 26: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4509 - accuracy: 0.8808 - val_loss: 0.2877 - val_accuracy: 0.9089 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4667 - accuracy: 0.8813\n",
      "Epoch 27: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4667 - accuracy: 0.8813 - val_loss: 0.2693 - val_accuracy: 0.9112 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.8813\n",
      "Epoch 28: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4605 - accuracy: 0.8813 - val_loss: 0.2844 - val_accuracy: 0.9021 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.8765\n",
      "Epoch 29: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4856 - accuracy: 0.8765 - val_loss: 0.2596 - val_accuracy: 0.9157 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4522 - accuracy: 0.8811\n",
      "Epoch 30: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4518 - accuracy: 0.8813 - val_loss: 0.3032 - val_accuracy: 0.9089 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4363 - accuracy: 0.8872\n",
      "Epoch 31: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4366 - accuracy: 0.8872 - val_loss: 0.2638 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4442 - accuracy: 0.8816\n",
      "Epoch 32: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4442 - accuracy: 0.8816 - val_loss: 0.2502 - val_accuracy: 0.9112 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4602 - accuracy: 0.8839\n",
      "Epoch 33: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4596 - accuracy: 0.8841 - val_loss: 0.2744 - val_accuracy: 0.9134 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4482 - accuracy: 0.8867\n",
      "Epoch 34: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4474 - accuracy: 0.8869 - val_loss: 0.2548 - val_accuracy: 0.9157 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.8874\n",
      "Epoch 35: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4492 - accuracy: 0.8877 - val_loss: 0.2485 - val_accuracy: 0.9066 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4561 - accuracy: 0.8826\n",
      "Epoch 36: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4555 - accuracy: 0.8829 - val_loss: 0.2583 - val_accuracy: 0.9066 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.8892\n",
      "Epoch 37: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4393 - accuracy: 0.8889 - val_loss: 0.2898 - val_accuracy: 0.9112 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4576 - accuracy: 0.8836\n",
      "Epoch 38: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4576 - accuracy: 0.8836 - val_loss: 0.2619 - val_accuracy: 0.9021 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4738 - accuracy: 0.8786Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.92938\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4743 - accuracy: 0.8783 - val_loss: 0.2875 - val_accuracy: 0.9066 - lr: 0.0012\n",
      "Epoch 39: early stopping\n",
      "439/439 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 15.6953 - accuracy: 0.8107\n",
      "Epoch 1: val_accuracy improved from -inf to 0.52740, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 41ms/step - loss: 15.6615 - accuracy: 0.8101 - val_loss: 0.6666 - val_accuracy: 0.5274 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.8720\n",
      "Epoch 2: val_accuracy did not improve from 0.52740\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4623 - accuracy: 0.8720 - val_loss: 0.7578 - val_accuracy: 0.3470 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4216 - accuracy: 0.8852\n",
      "Epoch 3: val_accuracy improved from 0.52740 to 0.79909, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4216 - accuracy: 0.8852 - val_loss: 0.4375 - val_accuracy: 0.7991 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.8885\n",
      "Epoch 4: val_accuracy improved from 0.79909 to 0.80594, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4309 - accuracy: 0.8882 - val_loss: 0.4215 - val_accuracy: 0.8059 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.8824\n",
      "Epoch 5: val_accuracy improved from 0.80594 to 0.86758, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4301 - accuracy: 0.8824 - val_loss: 0.3234 - val_accuracy: 0.8676 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8943\n",
      "Epoch 6: val_accuracy improved from 0.86758 to 0.89041, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4077 - accuracy: 0.8943 - val_loss: 0.2697 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4093 - accuracy: 0.8923\n",
      "Epoch 7: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4093 - accuracy: 0.8923 - val_loss: 0.2698 - val_accuracy: 0.8881 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4097 - accuracy: 0.8943\n",
      "Epoch 8: val_accuracy improved from 0.89041 to 0.89954, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4131 - accuracy: 0.8940 - val_loss: 0.2766 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.9002\n",
      "Epoch 9: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3932 - accuracy: 0.9004 - val_loss: 0.2721 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.9070\n",
      "Epoch 10: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3629 - accuracy: 0.9070 - val_loss: 0.2684 - val_accuracy: 0.8927 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9113\n",
      "Epoch 11: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3431 - accuracy: 0.9115 - val_loss: 0.2649 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.9162\n",
      "Epoch 12: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3481 - accuracy: 0.9163 - val_loss: 0.2816 - val_accuracy: 0.8858 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.9123\n",
      "Epoch 13: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3327 - accuracy: 0.9125 - val_loss: 0.2471 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.9098\n",
      "Epoch 14: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3291 - accuracy: 0.9098 - val_loss: 0.2683 - val_accuracy: 0.8790 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.9087\n",
      "Epoch 15: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3418 - accuracy: 0.9087 - val_loss: 0.2897 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.9131\n",
      "Epoch 16: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3225 - accuracy: 0.9133 - val_loss: 0.2972 - val_accuracy: 0.8858 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3531 - accuracy: 0.9085\n",
      "Epoch 17: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3531 - accuracy: 0.9085 - val_loss: 0.2769 - val_accuracy: 0.8927 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.9212\n",
      "Epoch 18: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3127 - accuracy: 0.9209 - val_loss: 0.3144 - val_accuracy: 0.8699 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.9077\n",
      "Epoch 19: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3369 - accuracy: 0.9077 - val_loss: 0.2693 - val_accuracy: 0.8813 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.9237\n",
      "Epoch 20: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3028 - accuracy: 0.9237 - val_loss: 0.2885 - val_accuracy: 0.8881 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.9247\n",
      "Epoch 21: val_accuracy improved from 0.89954 to 0.91096, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.2782 - accuracy: 0.9247 - val_loss: 0.2556 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2726 - accuracy: 0.9291\n",
      "Epoch 22: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2734 - accuracy: 0.9290 - val_loss: 0.2688 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9354\n",
      "Epoch 23: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2483 - accuracy: 0.9354 - val_loss: 0.2506 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9351\n",
      "Epoch 24: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2552 - accuracy: 0.9351 - val_loss: 0.2531 - val_accuracy: 0.9041 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9336\n",
      "Epoch 25: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2487 - accuracy: 0.9336 - val_loss: 0.4061 - val_accuracy: 0.8425 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9314\n",
      "Epoch 26: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2595 - accuracy: 0.9313 - val_loss: 0.2771 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.9296\n",
      "Epoch 27: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2543 - accuracy: 0.9295 - val_loss: 0.3909 - val_accuracy: 0.8425 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.9298\n",
      "Epoch 28: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2502 - accuracy: 0.9298 - val_loss: 0.2494 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2415 - accuracy: 0.9345\n",
      "Epoch 29: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2428 - accuracy: 0.9343 - val_loss: 0.2863 - val_accuracy: 0.8950 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9392\n",
      "Epoch 30: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2221 - accuracy: 0.9392 - val_loss: 0.2648 - val_accuracy: 0.9041 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2122 - accuracy: 0.9461\n",
      "Epoch 31: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2137 - accuracy: 0.9458 - val_loss: 0.2846 - val_accuracy: 0.8904 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1860 - accuracy: 0.9501\n",
      "Epoch 32: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1865 - accuracy: 0.9496 - val_loss: 0.2963 - val_accuracy: 0.8973 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9479\n",
      "Epoch 33: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2019 - accuracy: 0.9480 - val_loss: 0.2930 - val_accuracy: 0.9110 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1960 - accuracy: 0.9449\n",
      "Epoch 34: val_accuracy improved from 0.91096 to 0.91781, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.1959 - accuracy: 0.9450 - val_loss: 0.2745 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1854 - accuracy: 0.9474\n",
      "Epoch 35: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1852 - accuracy: 0.9475 - val_loss: 0.3029 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9483\n",
      "Epoch 36: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1842 - accuracy: 0.9483 - val_loss: 0.2994 - val_accuracy: 0.8995 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9521\n",
      "Epoch 37: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1777 - accuracy: 0.9521 - val_loss: 0.2907 - val_accuracy: 0.8950 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9536\n",
      "Epoch 38: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1741 - accuracy: 0.9536 - val_loss: 0.3071 - val_accuracy: 0.8995 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9554\n",
      "Epoch 39: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1788 - accuracy: 0.9544 - val_loss: 0.3506 - val_accuracy: 0.8973 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9535\n",
      "Epoch 40: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1742 - accuracy: 0.9529 - val_loss: 0.3116 - val_accuracy: 0.9064 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.9573\n",
      "Epoch 41: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1538 - accuracy: 0.9574 - val_loss: 0.3030 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1500 - accuracy: 0.9568\n",
      "Epoch 42: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1497 - accuracy: 0.9569 - val_loss: 0.3103 - val_accuracy: 0.9041 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9563\n",
      "Epoch 43: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1472 - accuracy: 0.9559 - val_loss: 0.3293 - val_accuracy: 0.8973 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9554\n",
      "Epoch 44: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1529 - accuracy: 0.9554 - val_loss: 0.3123 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9592\n",
      "Epoch 45: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.1427 - accuracy: 0.9592 - val_loss: 0.3526 - val_accuracy: 0.8950 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9555\n",
      "Epoch 46: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.1455 - accuracy: 0.9554 - val_loss: 0.3167 - val_accuracy: 0.9155 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9612\n",
      "Epoch 47: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.1380 - accuracy: 0.9612 - val_loss: 0.3319 - val_accuracy: 0.9132 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.9591\n",
      "Epoch 48: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.1404 - accuracy: 0.9592 - val_loss: 0.3373 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9599\n",
      "Epoch 49: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1408 - accuracy: 0.9599 - val_loss: 0.3407 - val_accuracy: 0.9064 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9588\n",
      "Epoch 50: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1393 - accuracy: 0.9587 - val_loss: 0.3572 - val_accuracy: 0.9132 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9629\n",
      "Epoch 51: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1282 - accuracy: 0.9630 - val_loss: 0.3329 - val_accuracy: 0.9110 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9634\n",
      "Epoch 52: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1262 - accuracy: 0.9635 - val_loss: 0.3554 - val_accuracy: 0.9155 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9629\n",
      "Epoch 53: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1221 - accuracy: 0.9630 - val_loss: 0.3534 - val_accuracy: 0.9155 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9648Restoring model weights from the end of the best epoch: 34.\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1185 - accuracy: 0.9648 - val_loss: 0.3581 - val_accuracy: 0.9155 - lr: 3.1250e-04\n",
      "Epoch 54: early stopping\n",
      "438/438 [==============================] - 1s 2ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.6301 - accuracy: 0.8033\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23973, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 43ms/step - loss: 9.6301 - accuracy: 0.8033 - val_loss: 1.1717 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.8682\n",
      "Epoch 2: val_accuracy did not improve from 0.23973\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5135 - accuracy: 0.8682 - val_loss: 1.3162 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.8730\n",
      "Epoch 3: val_accuracy did not improve from 0.23973\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.5031 - accuracy: 0.8730 - val_loss: 1.3953 - val_accuracy: 0.2397 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4879 - accuracy: 0.8809\n",
      "Epoch 4: val_accuracy improved from 0.23973 to 0.74886, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4879 - accuracy: 0.8809 - val_loss: 0.5978 - val_accuracy: 0.7489 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4817 - accuracy: 0.8811\n",
      "Epoch 5: val_accuracy improved from 0.74886 to 0.88584, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4820 - accuracy: 0.8809 - val_loss: 0.3236 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4853 - accuracy: 0.8740\n",
      "Epoch 6: val_accuracy improved from 0.88584 to 0.90639, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4859 - accuracy: 0.8740 - val_loss: 0.3200 - val_accuracy: 0.9064 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.8804\n",
      "Epoch 7: val_accuracy improved from 0.90639 to 0.90868, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4878 - accuracy: 0.8804 - val_loss: 0.3352 - val_accuracy: 0.9087 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4923 - accuracy: 0.8882\n",
      "Epoch 8: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4934 - accuracy: 0.8877 - val_loss: 0.3702 - val_accuracy: 0.8950 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.8869\n",
      "Epoch 9: val_accuracy did not improve from 0.90868\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5001 - accuracy: 0.8869 - val_loss: 0.3556 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4455 - accuracy: 0.8897\n",
      "Epoch 10: val_accuracy improved from 0.90868 to 0.91324, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4455 - accuracy: 0.8897 - val_loss: 0.2885 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.9014\n",
      "Epoch 11: val_accuracy improved from 0.91324 to 0.91553, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4157 - accuracy: 0.9014 - val_loss: 0.3279 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.9014\n",
      "Epoch 12: val_accuracy did not improve from 0.91553\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3982 - accuracy: 0.9016 - val_loss: 0.3013 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3900 - accuracy: 0.9060\n",
      "Epoch 13: val_accuracy improved from 0.91553 to 0.92466, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 6s 45ms/step - loss: 0.3899 - accuracy: 0.9060 - val_loss: 0.3209 - val_accuracy: 0.9247 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3857 - accuracy: 0.9060\n",
      "Epoch 14: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 43ms/step - loss: 0.3864 - accuracy: 0.9057 - val_loss: 0.3096 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3746 - accuracy: 0.9057\n",
      "Epoch 15: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3746 - accuracy: 0.9057 - val_loss: 0.3525 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3913 - accuracy: 0.9057\n",
      "Epoch 16: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3913 - accuracy: 0.9057 - val_loss: 0.2871 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3807 - accuracy: 0.9052\n",
      "Epoch 17: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3801 - accuracy: 0.9054 - val_loss: 0.3141 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3861 - accuracy: 0.9065\n",
      "Epoch 18: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3869 - accuracy: 0.9062 - val_loss: 0.3489 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.9073\n",
      "Epoch 19: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3779 - accuracy: 0.9072 - val_loss: 0.2780 - val_accuracy: 0.9201 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3324 - accuracy: 0.9217\n",
      "Epoch 20: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3319 - accuracy: 0.9219 - val_loss: 0.3055 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3306 - accuracy: 0.9146\n",
      "Epoch 21: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3300 - accuracy: 0.9148 - val_loss: 0.3118 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.9252\n",
      "Epoch 22: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3211 - accuracy: 0.9252 - val_loss: 0.2971 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.9184\n",
      "Epoch 23: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3322 - accuracy: 0.9181 - val_loss: 0.3199 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.9230\n",
      "Epoch 24: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3218 - accuracy: 0.9232 - val_loss: 0.3300 - val_accuracy: 0.9041 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.9284\n",
      "Epoch 25: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3110 - accuracy: 0.9280 - val_loss: 0.3089 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.9228\n",
      "Epoch 26: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3155 - accuracy: 0.9229 - val_loss: 0.3526 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.9248\n",
      "Epoch 27: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3152 - accuracy: 0.9250 - val_loss: 0.3124 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.9256\n",
      "Epoch 28: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3099 - accuracy: 0.9255 - val_loss: 0.3145 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.9220\n",
      "Epoch 29: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3106 - accuracy: 0.9222 - val_loss: 0.3239 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9314\n",
      "Epoch 30: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2854 - accuracy: 0.9316 - val_loss: 0.3410 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9359\n",
      "Epoch 31: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2692 - accuracy: 0.9359 - val_loss: 0.3335 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.9337\n",
      "Epoch 32: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2638 - accuracy: 0.9338 - val_loss: 0.3618 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2654 - accuracy: 0.9365Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2651 - accuracy: 0.9366 - val_loss: 0.3364 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 33: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 14.5925 - accuracy: 0.7942\n",
      "Epoch 1: val_accuracy improved from -inf to 0.73744, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 41ms/step - loss: 14.5595 - accuracy: 0.7947 - val_loss: 0.7126 - val_accuracy: 0.7374 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5123 - accuracy: 0.8725\n",
      "Epoch 2: val_accuracy did not improve from 0.73744\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.5117 - accuracy: 0.8728 - val_loss: 0.7280 - val_accuracy: 0.7374 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.8755\n",
      "Epoch 3: val_accuracy improved from 0.73744 to 0.83105, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4808 - accuracy: 0.8755 - val_loss: 0.3979 - val_accuracy: 0.8311 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4495 - accuracy: 0.8877\n",
      "Epoch 4: val_accuracy improved from 0.83105 to 0.89041, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4501 - accuracy: 0.8877 - val_loss: 0.2906 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4334 - accuracy: 0.8874\n",
      "Epoch 5: val_accuracy improved from 0.89041 to 0.89954, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4332 - accuracy: 0.8875 - val_loss: 0.2627 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4239 - accuracy: 0.8857\n",
      "Epoch 6: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.4238 - accuracy: 0.8857 - val_loss: 0.2729 - val_accuracy: 0.8973 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4313 - accuracy: 0.8910\n",
      "Epoch 7: val_accuracy improved from 0.89954 to 0.90411, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4308 - accuracy: 0.8913 - val_loss: 0.2979 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.8852\n",
      "Epoch 8: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.4192 - accuracy: 0.8852 - val_loss: 0.3047 - val_accuracy: 0.9018 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8920\n",
      "Epoch 9: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.4190 - accuracy: 0.8920 - val_loss: 0.2927 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8943\n",
      "Epoch 10: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3913 - accuracy: 0.8946 - val_loss: 0.2648 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3742 - accuracy: 0.9040\n",
      "Epoch 11: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.3743 - accuracy: 0.9037 - val_loss: 0.3156 - val_accuracy: 0.8767 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3621 - accuracy: 0.9052\n",
      "Epoch 12: val_accuracy improved from 0.90411 to 0.90639, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3615 - accuracy: 0.9054 - val_loss: 0.2630 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3660 - accuracy: 0.9014\n",
      "Epoch 13: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.3654 - accuracy: 0.9016 - val_loss: 0.2697 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.9113\n",
      "Epoch 14: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.3447 - accuracy: 0.9110 - val_loss: 0.2942 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3570 - accuracy: 0.9113\n",
      "Epoch 15: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.3575 - accuracy: 0.9110 - val_loss: 0.2694 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3569 - accuracy: 0.9027\n",
      "Epoch 16: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.3573 - accuracy: 0.9024 - val_loss: 0.2640 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3260 - accuracy: 0.9098\n",
      "Epoch 17: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.3259 - accuracy: 0.9098 - val_loss: 0.2670 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.9098\n",
      "Epoch 18: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.3384 - accuracy: 0.9095 - val_loss: 0.2679 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.9128\n",
      "Epoch 19: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3229 - accuracy: 0.9128 - val_loss: 0.2856 - val_accuracy: 0.8950 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.9200\n",
      "Epoch 20: val_accuracy improved from 0.90639 to 0.91781, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3045 - accuracy: 0.9199 - val_loss: 0.2531 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.9255\n",
      "Epoch 21: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2901 - accuracy: 0.9255 - val_loss: 0.2548 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.9257\n",
      "Epoch 22: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2913 - accuracy: 0.9257 - val_loss: 0.2678 - val_accuracy: 0.9018 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9278\n",
      "Epoch 23: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.2919 - accuracy: 0.9275 - val_loss: 0.2934 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.9263\n",
      "Epoch 24: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.2805 - accuracy: 0.9262 - val_loss: 0.2791 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.9267\n",
      "Epoch 25: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2761 - accuracy: 0.9267 - val_loss: 0.3197 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.9278\n",
      "Epoch 26: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.2763 - accuracy: 0.9278 - val_loss: 0.2755 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.9219\n",
      "Epoch 27: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2801 - accuracy: 0.9219 - val_loss: 0.2945 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9318\n",
      "Epoch 28: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2668 - accuracy: 0.9318 - val_loss: 0.3215 - val_accuracy: 0.8950 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2613 - accuracy: 0.9276\n",
      "Epoch 29: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2613 - accuracy: 0.9278 - val_loss: 0.2637 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9261\n",
      "Epoch 30: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2526 - accuracy: 0.9260 - val_loss: 0.2936 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.9380\n",
      "Epoch 31: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2292 - accuracy: 0.9381 - val_loss: 0.3069 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2229 - accuracy: 0.9367\n",
      "Epoch 32: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2225 - accuracy: 0.9369 - val_loss: 0.3245 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2174 - accuracy: 0.9352\n",
      "Epoch 33: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.2176 - accuracy: 0.9351 - val_loss: 0.3292 - val_accuracy: 0.9041 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2384 - accuracy: 0.9367\n",
      "Epoch 34: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.2383 - accuracy: 0.9369 - val_loss: 0.3027 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9458\n",
      "Epoch 35: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2110 - accuracy: 0.9458 - val_loss: 0.2969 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2218 - accuracy: 0.9352\n",
      "Epoch 36: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2216 - accuracy: 0.9354 - val_loss: 0.3103 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 0.9374\n",
      "Epoch 37: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.2128 - accuracy: 0.9374 - val_loss: 0.2955 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2148 - accuracy: 0.9398\n",
      "Epoch 38: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2148 - accuracy: 0.9399 - val_loss: 0.3128 - val_accuracy: 0.8950 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2007 - accuracy: 0.9446\n",
      "Epoch 39: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2010 - accuracy: 0.9445 - val_loss: 0.3085 - val_accuracy: 0.9041 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9441Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1951 - accuracy: 0.9442 - val_loss: 0.3480 - val_accuracy: 0.8927 - lr: 6.2500e-04\n",
      "Epoch 40: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2551 - accuracy: 0.8170\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26027, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 42ms/step - loss: 7.2551 - accuracy: 0.8170 - val_loss: 1.0666 - val_accuracy: 0.2603 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4900 - accuracy: 0.8725\n",
      "Epoch 2: val_accuracy did not improve from 0.26027\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4900 - accuracy: 0.8725 - val_loss: 2.8962 - val_accuracy: 0.2603 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.8786\n",
      "Epoch 3: val_accuracy improved from 0.26027 to 0.26712, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4762 - accuracy: 0.8786 - val_loss: 1.1617 - val_accuracy: 0.2671 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4712 - accuracy: 0.8841\n",
      "Epoch 4: val_accuracy improved from 0.26712 to 0.87900, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4706 - accuracy: 0.8844 - val_loss: 0.3056 - val_accuracy: 0.8790 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.8837\n",
      "Epoch 5: val_accuracy improved from 0.87900 to 0.89498, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4808 - accuracy: 0.8837 - val_loss: 0.2880 - val_accuracy: 0.8950 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.8806\n",
      "Epoch 6: val_accuracy did not improve from 0.89498\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4747 - accuracy: 0.8806 - val_loss: 0.3094 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4708 - accuracy: 0.8852\n",
      "Epoch 7: val_accuracy did not improve from 0.89498\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4710 - accuracy: 0.8852 - val_loss: 0.3464 - val_accuracy: 0.8927 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4938 - accuracy: 0.8765\n",
      "Epoch 8: val_accuracy did not improve from 0.89498\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4939 - accuracy: 0.8766 - val_loss: 0.2863 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5203 - accuracy: 0.8735\n",
      "Epoch 9: val_accuracy did not improve from 0.89498\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.5203 - accuracy: 0.8735 - val_loss: 0.3470 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4577 - accuracy: 0.8880\n",
      "Epoch 10: val_accuracy did not improve from 0.89498\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4576 - accuracy: 0.8880 - val_loss: 0.2762 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.9016\n",
      "Epoch 11: val_accuracy improved from 0.89498 to 0.90183, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 6s 45ms/step - loss: 0.4265 - accuracy: 0.9016 - val_loss: 0.2938 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.9007\n",
      "Epoch 12: val_accuracy did not improve from 0.90183\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4051 - accuracy: 0.9009 - val_loss: 0.2656 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.8976\n",
      "Epoch 13: val_accuracy did not improve from 0.90183\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4182 - accuracy: 0.8976 - val_loss: 0.2847 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.9017\n",
      "Epoch 14: val_accuracy did not improve from 0.90183\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4227 - accuracy: 0.9014 - val_loss: 0.3582 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8989\n",
      "Epoch 15: val_accuracy improved from 0.90183 to 0.90639, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4082 - accuracy: 0.8989 - val_loss: 0.2750 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3881 - accuracy: 0.9083\n",
      "Epoch 16: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.3894 - accuracy: 0.9080 - val_loss: 0.2806 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.9131\n",
      "Epoch 17: val_accuracy improved from 0.90639 to 0.92009, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.3773 - accuracy: 0.9131 - val_loss: 0.2851 - val_accuracy: 0.9201 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.9144\n",
      "Epoch 18: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3638 - accuracy: 0.9146 - val_loss: 0.2577 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.9055\n",
      "Epoch 19: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3769 - accuracy: 0.9057 - val_loss: 0.2845 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.9262\n",
      "Epoch 20: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3304 - accuracy: 0.9262 - val_loss: 0.2503 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.9207\n",
      "Epoch 21: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3306 - accuracy: 0.9209 - val_loss: 0.2689 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.9276\n",
      "Epoch 22: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3167 - accuracy: 0.9278 - val_loss: 0.2747 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.9271\n",
      "Epoch 23: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3210 - accuracy: 0.9270 - val_loss: 0.2619 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.9333\n",
      "Epoch 24: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3142 - accuracy: 0.9333 - val_loss: 0.2510 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9301\n",
      "Epoch 25: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.2990 - accuracy: 0.9303 - val_loss: 0.2867 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.9324\n",
      "Epoch 26: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3077 - accuracy: 0.9326 - val_loss: 0.2493 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.9329\n",
      "Epoch 27: val_accuracy improved from 0.92009 to 0.92466, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.3066 - accuracy: 0.9326 - val_loss: 0.2650 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.9390\n",
      "Epoch 28: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2827 - accuracy: 0.9392 - val_loss: 0.2424 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.9304\n",
      "Epoch 29: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2975 - accuracy: 0.9303 - val_loss: 0.2532 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2669 - accuracy: 0.9418\n",
      "Epoch 30: val_accuracy improved from 0.92466 to 0.92694, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.2665 - accuracy: 0.9420 - val_loss: 0.2504 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.9464\n",
      "Epoch 31: val_accuracy improved from 0.92694 to 0.92922, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.2528 - accuracy: 0.9465 - val_loss: 0.2309 - val_accuracy: 0.9292 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2636 - accuracy: 0.9400\n",
      "Epoch 32: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2641 - accuracy: 0.9399 - val_loss: 0.2537 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.9436\n",
      "Epoch 33: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2358 - accuracy: 0.9435 - val_loss: 0.2889 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2382 - accuracy: 0.9487\n",
      "Epoch 34: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2381 - accuracy: 0.9485 - val_loss: 0.2794 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.9502\n",
      "Epoch 35: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2349 - accuracy: 0.9503 - val_loss: 0.2870 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2219 - accuracy: 0.9515\n",
      "Epoch 36: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2220 - accuracy: 0.9511 - val_loss: 0.2835 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2174 - accuracy: 0.9533\n",
      "Epoch 37: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2170 - accuracy: 0.9534 - val_loss: 0.2819 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2202 - accuracy: 0.9502\n",
      "Epoch 38: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2200 - accuracy: 0.9503 - val_loss: 0.2993 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2175 - accuracy: 0.9548\n",
      "Epoch 39: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2180 - accuracy: 0.9546 - val_loss: 0.2973 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2179 - accuracy: 0.9545\n",
      "Epoch 40: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2175 - accuracy: 0.9546 - val_loss: 0.3200 - val_accuracy: 0.9224 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1983 - accuracy: 0.9566\n",
      "Epoch 41: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1979 - accuracy: 0.9567 - val_loss: 0.2983 - val_accuracy: 0.9224 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9639\n",
      "Epoch 42: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1772 - accuracy: 0.9640 - val_loss: 0.3097 - val_accuracy: 0.9201 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2054 - accuracy: 0.9586\n",
      "Epoch 43: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2050 - accuracy: 0.9587 - val_loss: 0.2956 - val_accuracy: 0.9269 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1923 - accuracy: 0.9599\n",
      "Epoch 44: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1921 - accuracy: 0.9599 - val_loss: 0.3338 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9639\n",
      "Epoch 45: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1878 - accuracy: 0.9640 - val_loss: 0.3461 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1826 - accuracy: 0.9604\n",
      "Epoch 46: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1823 - accuracy: 0.9605 - val_loss: 0.3357 - val_accuracy: 0.9201 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9614\n",
      "Epoch 47: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1889 - accuracy: 0.9610 - val_loss: 0.3251 - val_accuracy: 0.9269 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9670\n",
      "Epoch 48: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1762 - accuracy: 0.9670 - val_loss: 0.3244 - val_accuracy: 0.9247 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9634\n",
      "Epoch 49: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1814 - accuracy: 0.9635 - val_loss: 0.3322 - val_accuracy: 0.9247 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1602 - accuracy: 0.9705\n",
      "Epoch 50: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1600 - accuracy: 0.9706 - val_loss: 0.3501 - val_accuracy: 0.9201 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9703Restoring model weights from the end of the best epoch: 31.\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.1610 - accuracy: 0.9703 - val_loss: 0.3527 - val_accuracy: 0.9155 - lr: 3.1250e-04\n",
      "Epoch 51: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 7.8927 - accuracy: 0.8153\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26941, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 42ms/step - loss: 7.8759 - accuracy: 0.8155 - val_loss: 1.9077 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4993 - accuracy: 0.8714\n",
      "Epoch 2: val_accuracy did not improve from 0.26941\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4991 - accuracy: 0.8717 - val_loss: 1.6847 - val_accuracy: 0.2694 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4914 - accuracy: 0.8699\n",
      "Epoch 3: val_accuracy improved from 0.26941 to 0.88813, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4905 - accuracy: 0.8702 - val_loss: 0.3162 - val_accuracy: 0.8881 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4688 - accuracy: 0.8844\n",
      "Epoch 4: val_accuracy improved from 0.88813 to 0.90183, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4690 - accuracy: 0.8842 - val_loss: 0.2298 - val_accuracy: 0.9018 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.8811\n",
      "Epoch 5: val_accuracy did not improve from 0.90183\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4575 - accuracy: 0.8814 - val_loss: 0.2211 - val_accuracy: 0.8973 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4540 - accuracy: 0.8819\n",
      "Epoch 6: val_accuracy did not improve from 0.90183\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4537 - accuracy: 0.8819 - val_loss: 0.3445 - val_accuracy: 0.8562 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4322 - accuracy: 0.8920\n",
      "Epoch 7: val_accuracy improved from 0.90183 to 0.90411, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4328 - accuracy: 0.8918 - val_loss: 0.2646 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4392 - accuracy: 0.8887\n",
      "Epoch 8: val_accuracy improved from 0.90411 to 0.90639, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4387 - accuracy: 0.8890 - val_loss: 0.2372 - val_accuracy: 0.9064 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.8887\n",
      "Epoch 9: val_accuracy improved from 0.90639 to 0.91096, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4485 - accuracy: 0.8887 - val_loss: 0.2176 - val_accuracy: 0.9110 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4356 - accuracy: 0.8979\n",
      "Epoch 10: val_accuracy improved from 0.91096 to 0.91324, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4356 - accuracy: 0.8978 - val_loss: 0.2192 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4010 - accuracy: 0.9073\n",
      "Epoch 11: val_accuracy improved from 0.91324 to 0.92466, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4003 - accuracy: 0.9075 - val_loss: 0.2287 - val_accuracy: 0.9247 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.9032\n",
      "Epoch 12: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3940 - accuracy: 0.9032 - val_loss: 0.2285 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3750 - accuracy: 0.9042\n",
      "Epoch 13: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3744 - accuracy: 0.9044 - val_loss: 0.2294 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3616 - accuracy: 0.9096\n",
      "Epoch 14: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3642 - accuracy: 0.9093 - val_loss: 0.2063 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3503 - accuracy: 0.9113\n",
      "Epoch 15: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3499 - accuracy: 0.9115 - val_loss: 0.2351 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.9179\n",
      "Epoch 16: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3253 - accuracy: 0.9176 - val_loss: 0.2023 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.9123\n",
      "Epoch 17: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3374 - accuracy: 0.9120 - val_loss: 0.2327 - val_accuracy: 0.9110 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.9199\n",
      "Epoch 18: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3241 - accuracy: 0.9199 - val_loss: 0.2290 - val_accuracy: 0.9155 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.9157\n",
      "Epoch 19: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3377 - accuracy: 0.9153 - val_loss: 0.2426 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9309\n",
      "Epoch 20: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2880 - accuracy: 0.9311 - val_loss: 0.2040 - val_accuracy: 0.9247 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2625 - accuracy: 0.9337\n",
      "Epoch 21: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2620 - accuracy: 0.9338 - val_loss: 0.2143 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2727 - accuracy: 0.9309\n",
      "Epoch 22: val_accuracy did not improve from 0.92466\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2722 - accuracy: 0.9311 - val_loss: 0.2087 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.9311\n",
      "Epoch 23: val_accuracy improved from 0.92466 to 0.92922, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.2635 - accuracy: 0.9311 - val_loss: 0.2007 - val_accuracy: 0.9292 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2488 - accuracy: 0.9345\n",
      "Epoch 24: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2491 - accuracy: 0.9343 - val_loss: 0.2112 - val_accuracy: 0.9292 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.9385\n",
      "Epoch 25: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2513 - accuracy: 0.9384 - val_loss: 0.2089 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.9322\n",
      "Epoch 26: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2537 - accuracy: 0.9321 - val_loss: 0.1959 - val_accuracy: 0.9269 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.9372\n",
      "Epoch 27: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2369 - accuracy: 0.9371 - val_loss: 0.2058 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9405\n",
      "Epoch 28: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2369 - accuracy: 0.9407 - val_loss: 0.1960 - val_accuracy: 0.9292 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2224 - accuracy: 0.9408\n",
      "Epoch 29: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 37ms/step - loss: 0.2228 - accuracy: 0.9407 - val_loss: 0.2117 - val_accuracy: 0.9292 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2122 - accuracy: 0.9459\n",
      "Epoch 30: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2120 - accuracy: 0.9458 - val_loss: 0.2088 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9512\n",
      "Epoch 31: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1938 - accuracy: 0.9513 - val_loss: 0.2188 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1837 - accuracy: 0.9525\n",
      "Epoch 32: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1846 - accuracy: 0.9521 - val_loss: 0.2194 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2003 - accuracy: 0.9505\n",
      "Epoch 33: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2001 - accuracy: 0.9506 - val_loss: 0.2204 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1802 - accuracy: 0.9550\n",
      "Epoch 34: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1802 - accuracy: 0.9549 - val_loss: 0.2281 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9566\n",
      "Epoch 35: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1763 - accuracy: 0.9564 - val_loss: 0.2326 - val_accuracy: 0.9292 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.9609\n",
      "Epoch 36: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1667 - accuracy: 0.9610 - val_loss: 0.2534 - val_accuracy: 0.9247 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1857 - accuracy: 0.9515\n",
      "Epoch 37: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1854 - accuracy: 0.9516 - val_loss: 0.2358 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1703 - accuracy: 0.9578\n",
      "Epoch 38: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1703 - accuracy: 0.9579 - val_loss: 0.2308 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9568\n",
      "Epoch 39: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1699 - accuracy: 0.9569 - val_loss: 0.2466 - val_accuracy: 0.9292 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1609 - accuracy: 0.9583\n",
      "Epoch 40: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1608 - accuracy: 0.9584 - val_loss: 0.2363 - val_accuracy: 0.9224 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9609\n",
      "Epoch 41: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1516 - accuracy: 0.9610 - val_loss: 0.2329 - val_accuracy: 0.9292 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9606\n",
      "Epoch 42: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1447 - accuracy: 0.9607 - val_loss: 0.2505 - val_accuracy: 0.9224 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9634Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.92922\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1452 - accuracy: 0.9635 - val_loss: 0.2476 - val_accuracy: 0.9201 - lr: 6.2500e-04\n",
      "Epoch 43: early stopping\n",
      "438/438 [==============================] - 1s 2ms/step\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 13.8955 - accuracy: 0.7949\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23744, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 43ms/step - loss: 13.8955 - accuracy: 0.7949 - val_loss: 1.0103 - val_accuracy: 0.2374 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4831 - accuracy: 0.8648\n",
      "Epoch 2: val_accuracy did not improve from 0.23744\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4827 - accuracy: 0.8649 - val_loss: 2.7454 - val_accuracy: 0.2374 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4504 - accuracy: 0.8788\n",
      "Epoch 3: val_accuracy improved from 0.23744 to 0.27169, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4500 - accuracy: 0.8786 - val_loss: 1.1333 - val_accuracy: 0.2717 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4340 - accuracy: 0.8872\n",
      "Epoch 4: val_accuracy improved from 0.27169 to 0.70091, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 43ms/step - loss: 0.4342 - accuracy: 0.8872 - val_loss: 0.6356 - val_accuracy: 0.7009 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4136 - accuracy: 0.8862\n",
      "Epoch 5: val_accuracy improved from 0.70091 to 0.88128, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4153 - accuracy: 0.8859 - val_loss: 0.3260 - val_accuracy: 0.8813 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4086 - accuracy: 0.8900\n",
      "Epoch 6: val_accuracy improved from 0.88128 to 0.90639, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4080 - accuracy: 0.8902 - val_loss: 0.2791 - val_accuracy: 0.9064 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4153 - accuracy: 0.8900\n",
      "Epoch 7: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4150 - accuracy: 0.8902 - val_loss: 0.3221 - val_accuracy: 0.8721 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3974 - accuracy: 0.8900\n",
      "Epoch 8: val_accuracy did not improve from 0.90639\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4013 - accuracy: 0.8895 - val_loss: 0.3096 - val_accuracy: 0.9064 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.8915\n",
      "Epoch 9: val_accuracy improved from 0.90639 to 0.91781, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4273 - accuracy: 0.8915 - val_loss: 0.2801 - val_accuracy: 0.9178 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3806 - accuracy: 0.9055\n",
      "Epoch 10: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3806 - accuracy: 0.9054 - val_loss: 0.3159 - val_accuracy: 0.9178 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3707 - accuracy: 0.9062\n",
      "Epoch 11: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3707 - accuracy: 0.9062 - val_loss: 0.3476 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3731 - accuracy: 0.9062\n",
      "Epoch 12: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3727 - accuracy: 0.9065 - val_loss: 0.3907 - val_accuracy: 0.8813 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.9014\n",
      "Epoch 13: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3592 - accuracy: 0.9014 - val_loss: 0.2992 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.9134\n",
      "Epoch 14: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3274 - accuracy: 0.9136 - val_loss: 0.3311 - val_accuracy: 0.9087 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3366 - accuracy: 0.9171\n",
      "Epoch 15: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3366 - accuracy: 0.9171 - val_loss: 0.3076 - val_accuracy: 0.9132 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3469 - accuracy: 0.9144\n",
      "Epoch 16: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3483 - accuracy: 0.9141 - val_loss: 0.2855 - val_accuracy: 0.9064 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.9149\n",
      "Epoch 17: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3193 - accuracy: 0.9151 - val_loss: 0.3062 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.9159\n",
      "Epoch 18: val_accuracy improved from 0.91781 to 0.92009, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.3200 - accuracy: 0.9156 - val_loss: 0.2717 - val_accuracy: 0.9201 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.9111\n",
      "Epoch 19: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3418 - accuracy: 0.9110 - val_loss: 0.3215 - val_accuracy: 0.8973 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.9243\n",
      "Epoch 20: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3007 - accuracy: 0.9245 - val_loss: 0.3423 - val_accuracy: 0.9041 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.9275\n",
      "Epoch 21: val_accuracy improved from 0.92009 to 0.92237, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.2811 - accuracy: 0.9275 - val_loss: 0.3284 - val_accuracy: 0.9224 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.9309\n",
      "Epoch 22: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2780 - accuracy: 0.9311 - val_loss: 0.2954 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.9329\n",
      "Epoch 23: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2683 - accuracy: 0.9328 - val_loss: 0.2990 - val_accuracy: 0.9155 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2741 - accuracy: 0.9334\n",
      "Epoch 24: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2744 - accuracy: 0.9333 - val_loss: 0.3254 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9336\n",
      "Epoch 25: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2589 - accuracy: 0.9336 - val_loss: 0.3380 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.9352\n",
      "Epoch 26: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2658 - accuracy: 0.9354 - val_loss: 0.3555 - val_accuracy: 0.9201 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.9322\n",
      "Epoch 27: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2611 - accuracy: 0.9321 - val_loss: 0.3662 - val_accuracy: 0.9178 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9428\n",
      "Epoch 28: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2476 - accuracy: 0.9427 - val_loss: 0.4039 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9359\n",
      "Epoch 29: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2571 - accuracy: 0.9359 - val_loss: 0.3315 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9425\n",
      "Epoch 30: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.2330 - accuracy: 0.9425 - val_loss: 0.3987 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2160 - accuracy: 0.9449\n",
      "Epoch 31: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2162 - accuracy: 0.9447 - val_loss: 0.3672 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2156 - accuracy: 0.9423\n",
      "Epoch 32: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2154 - accuracy: 0.9425 - val_loss: 0.3705 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2107 - accuracy: 0.9527\n",
      "Epoch 33: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2104 - accuracy: 0.9529 - val_loss: 0.4278 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9492\n",
      "Epoch 34: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2025 - accuracy: 0.9493 - val_loss: 0.4027 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.9479\n",
      "Epoch 35: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2080 - accuracy: 0.9478 - val_loss: 0.3972 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9489\n",
      "Epoch 36: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.2021 - accuracy: 0.9490 - val_loss: 0.3926 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9508\n",
      "Epoch 37: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1940 - accuracy: 0.9508 - val_loss: 0.4155 - val_accuracy: 0.9224 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1903 - accuracy: 0.9530\n",
      "Epoch 38: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1900 - accuracy: 0.9531 - val_loss: 0.3771 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9548\n",
      "Epoch 39: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.1868 - accuracy: 0.9549 - val_loss: 0.4156 - val_accuracy: 0.9064 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "122/124 [============================>.] - ETA: 0s - loss: 0.1740 - accuracy: 0.9603\n",
      "Epoch 40: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1740 - accuracy: 0.9599 - val_loss: 0.4551 - val_accuracy: 0.9087 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9596Restoring model weights from the end of the best epoch: 21.\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.1699 - accuracy: 0.9592 - val_loss: 0.4359 - val_accuracy: 0.9201 - lr: 6.2500e-04\n",
      "Epoch 41: early stopping\n",
      "438/438 [==============================] - 1s 1ms/step\n",
      "Epoch 1/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 11.5169 - accuracy: 0.7579\n",
      "Epoch 1: val_accuracy improved from -inf to 0.34475, saving model to best_model_640.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nandan\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 6s 41ms/step - loss: 11.4937 - accuracy: 0.7579 - val_loss: 0.6754 - val_accuracy: 0.3447 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5709 - accuracy: 0.7891\n",
      "Epoch 2: val_accuracy improved from 0.34475 to 0.68265, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5709 - accuracy: 0.7891 - val_loss: 0.6292 - val_accuracy: 0.6826 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5577 - accuracy: 0.7594\n",
      "Epoch 3: val_accuracy improved from 0.68265 to 0.86073, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.5584 - accuracy: 0.7594 - val_loss: 0.4431 - val_accuracy: 0.8607 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5365 - accuracy: 0.8209\n",
      "Epoch 4: val_accuracy improved from 0.86073 to 0.89041, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5368 - accuracy: 0.8205 - val_loss: 0.3235 - val_accuracy: 0.8904 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5710 - accuracy: 0.8550\n",
      "Epoch 5: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5710 - accuracy: 0.8550 - val_loss: 0.3008 - val_accuracy: 0.8813 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5641 - accuracy: 0.8487\n",
      "Epoch 6: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5641 - accuracy: 0.8487 - val_loss: 0.3347 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5274 - accuracy: 0.8598\n",
      "Epoch 7: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.5268 - accuracy: 0.8601 - val_loss: 0.3319 - val_accuracy: 0.8813 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5649 - accuracy: 0.8598\n",
      "Epoch 8: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.5649 - accuracy: 0.8598 - val_loss: 0.2827 - val_accuracy: 0.8881 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.6096 - accuracy: 0.8549\n",
      "Epoch 9: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.6095 - accuracy: 0.8550 - val_loss: 0.3985 - val_accuracy: 0.8813 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5690 - accuracy: 0.8707\n",
      "Epoch 10: val_accuracy did not improve from 0.89041\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.5690 - accuracy: 0.8707 - val_loss: 0.3856 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5562 - accuracy: 0.8712\n",
      "Epoch 11: val_accuracy improved from 0.89041 to 0.89269, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5562 - accuracy: 0.8712 - val_loss: 0.3680 - val_accuracy: 0.8927 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5740 - accuracy: 0.8664\n",
      "Epoch 12: val_accuracy improved from 0.89269 to 0.89954, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5745 - accuracy: 0.8662 - val_loss: 0.3709 - val_accuracy: 0.8995 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5805 - accuracy: 0.8679\n",
      "Epoch 13: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5805 - accuracy: 0.8677 - val_loss: 0.3862 - val_accuracy: 0.8881 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5417 - accuracy: 0.8773\n",
      "Epoch 14: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5421 - accuracy: 0.8771 - val_loss: 0.3850 - val_accuracy: 0.8904 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.5507 - accuracy: 0.8753\n",
      "Epoch 15: val_accuracy did not improve from 0.89954\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.5507 - accuracy: 0.8753 - val_loss: 0.3712 - val_accuracy: 0.8790 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5487 - accuracy: 0.8699\n",
      "Epoch 16: val_accuracy improved from 0.89954 to 0.90183, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5480 - accuracy: 0.8702 - val_loss: 0.3592 - val_accuracy: 0.9018 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5426 - accuracy: 0.8780\n",
      "Epoch 17: val_accuracy did not improve from 0.90183\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5426 - accuracy: 0.8781 - val_loss: 0.3727 - val_accuracy: 0.8721 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5495 - accuracy: 0.8735\n",
      "Epoch 18: val_accuracy did not improve from 0.90183\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5487 - accuracy: 0.8738 - val_loss: 0.3845 - val_accuracy: 0.8813 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5238 - accuracy: 0.8808\n",
      "Epoch 19: val_accuracy improved from 0.90183 to 0.90411, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.5239 - accuracy: 0.8809 - val_loss: 0.3663 - val_accuracy: 0.9041 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.5004 - accuracy: 0.8882\n",
      "Epoch 20: val_accuracy did not improve from 0.90411\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.5004 - accuracy: 0.8882 - val_loss: 0.3507 - val_accuracy: 0.8973 - lr: 0.0025\n",
      "Epoch 21/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4892 - accuracy: 0.8925\n",
      "Epoch 21: val_accuracy improved from 0.90411 to 0.91096, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4893 - accuracy: 0.8925 - val_loss: 0.3353 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 22/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.8953\n",
      "Epoch 22: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4816 - accuracy: 0.8956 - val_loss: 0.3385 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 23/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4676 - accuracy: 0.8971\n",
      "Epoch 23: val_accuracy did not improve from 0.91096\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4676 - accuracy: 0.8971 - val_loss: 0.3481 - val_accuracy: 0.8995 - lr: 0.0025\n",
      "Epoch 24/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4693 - accuracy: 0.8999\n",
      "Epoch 24: val_accuracy improved from 0.91096 to 0.91324, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 42ms/step - loss: 0.4701 - accuracy: 0.8996 - val_loss: 0.3538 - val_accuracy: 0.9132 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4878 - accuracy: 0.8905\n",
      "Epoch 25: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4871 - accuracy: 0.8907 - val_loss: 0.3328 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 26/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4747 - accuracy: 0.8956\n",
      "Epoch 26: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4741 - accuracy: 0.8958 - val_loss: 0.3890 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 27/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.8958\n",
      "Epoch 27: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4754 - accuracy: 0.8958 - val_loss: 0.3311 - val_accuracy: 0.9064 - lr: 0.0025\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4639 - accuracy: 0.9044\n",
      "Epoch 28: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4639 - accuracy: 0.9044 - val_loss: 0.3524 - val_accuracy: 0.9087 - lr: 0.0025\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.9014\n",
      "Epoch 29: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4690 - accuracy: 0.9014 - val_loss: 0.3886 - val_accuracy: 0.9110 - lr: 0.0025\n",
      "Epoch 30/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.9090\n",
      "Epoch 30: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4288 - accuracy: 0.9090 - val_loss: 0.3731 - val_accuracy: 0.9018 - lr: 0.0012\n",
      "Epoch 31/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4598 - accuracy: 0.9037\n",
      "Epoch 31: val_accuracy did not improve from 0.91324\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4611 - accuracy: 0.9032 - val_loss: 0.3491 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4417 - accuracy: 0.9078\n",
      "Epoch 32: val_accuracy improved from 0.91324 to 0.91781, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4418 - accuracy: 0.9077 - val_loss: 0.3496 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 33/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.9144\n",
      "Epoch 33: val_accuracy did not improve from 0.91781\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4202 - accuracy: 0.9143 - val_loss: 0.3573 - val_accuracy: 0.9178 - lr: 0.0012\n",
      "Epoch 34/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.9034\n",
      "Epoch 34: val_accuracy improved from 0.91781 to 0.92009, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 41ms/step - loss: 0.4514 - accuracy: 0.9034 - val_loss: 0.3473 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.9085\n",
      "Epoch 35: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4368 - accuracy: 0.9085 - val_loss: 0.3552 - val_accuracy: 0.9155 - lr: 0.0012\n",
      "Epoch 36/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4344 - accuracy: 0.9088\n",
      "Epoch 36: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4337 - accuracy: 0.9090 - val_loss: 0.3566 - val_accuracy: 0.9201 - lr: 0.0012\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.9125\n",
      "Epoch 37: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.4230 - accuracy: 0.9125 - val_loss: 0.3594 - val_accuracy: 0.9087 - lr: 0.0012\n",
      "Epoch 38/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4123 - accuracy: 0.9177\n",
      "Epoch 38: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.4117 - accuracy: 0.9179 - val_loss: 0.3628 - val_accuracy: 0.9132 - lr: 0.0012\n",
      "Epoch 39/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.9101\n",
      "Epoch 39: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4250 - accuracy: 0.9103 - val_loss: 0.3940 - val_accuracy: 0.9041 - lr: 0.0012\n",
      "Epoch 40/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4099 - accuracy: 0.9164\n",
      "Epoch 40: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4095 - accuracy: 0.9163 - val_loss: 0.3699 - val_accuracy: 0.9110 - lr: 6.2500e-04\n",
      "Epoch 41/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4095 - accuracy: 0.9195\n",
      "Epoch 41: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4089 - accuracy: 0.9196 - val_loss: 0.3543 - val_accuracy: 0.9155 - lr: 6.2500e-04\n",
      "Epoch 42/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4117 - accuracy: 0.9197\n",
      "Epoch 42: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4118 - accuracy: 0.9196 - val_loss: 0.3535 - val_accuracy: 0.9201 - lr: 6.2500e-04\n",
      "Epoch 43/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.9220\n",
      "Epoch 43: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4096 - accuracy: 0.9219 - val_loss: 0.3594 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 44/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4119 - accuracy: 0.9192\n",
      "Epoch 44: val_accuracy did not improve from 0.92009\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4141 - accuracy: 0.9186 - val_loss: 0.3574 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 45/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3999 - accuracy: 0.9207\n",
      "Epoch 45: val_accuracy improved from 0.92009 to 0.92237, saving model to best_model_640.h5\n",
      "124/124 [==============================] - 5s 40ms/step - loss: 0.3997 - accuracy: 0.9207 - val_loss: 0.3583 - val_accuracy: 0.9224 - lr: 6.2500e-04\n",
      "Epoch 46/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3961 - accuracy: 0.9237\n",
      "Epoch 46: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3961 - accuracy: 0.9237 - val_loss: 0.4187 - val_accuracy: 0.9064 - lr: 6.2500e-04\n",
      "Epoch 47/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.9243\n",
      "Epoch 47: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3870 - accuracy: 0.9245 - val_loss: 0.3723 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 48/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.9304\n",
      "Epoch 48: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3770 - accuracy: 0.9303 - val_loss: 0.3760 - val_accuracy: 0.9155 - lr: 6.2500e-04\n",
      "Epoch 49/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3963 - accuracy: 0.9223\n",
      "Epoch 49: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3957 - accuracy: 0.9224 - val_loss: 0.3730 - val_accuracy: 0.9178 - lr: 6.2500e-04\n",
      "Epoch 50/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.9204\n",
      "Epoch 50: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4060 - accuracy: 0.9204 - val_loss: 0.3746 - val_accuracy: 0.9178 - lr: 3.1250e-04\n",
      "Epoch 51/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.9237\n",
      "Epoch 51: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3944 - accuracy: 0.9237 - val_loss: 0.3757 - val_accuracy: 0.9110 - lr: 3.1250e-04\n",
      "Epoch 52/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.9182\n",
      "Epoch 52: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4103 - accuracy: 0.9184 - val_loss: 0.3746 - val_accuracy: 0.9201 - lr: 3.1250e-04\n",
      "Epoch 53/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3878 - accuracy: 0.9258\n",
      "Epoch 53: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3872 - accuracy: 0.9260 - val_loss: 0.3816 - val_accuracy: 0.9178 - lr: 3.1250e-04\n",
      "Epoch 54/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3932 - accuracy: 0.9230\n",
      "Epoch 54: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.3934 - accuracy: 0.9229 - val_loss: 0.3780 - val_accuracy: 0.9201 - lr: 3.1250e-04\n",
      "Epoch 55/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3963 - accuracy: 0.9228\n",
      "Epoch 55: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3956 - accuracy: 0.9229 - val_loss: 0.3944 - val_accuracy: 0.9155 - lr: 3.1250e-04\n",
      "Epoch 56/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.9288\n",
      "Epoch 56: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3810 - accuracy: 0.9288 - val_loss: 0.4050 - val_accuracy: 0.9110 - lr: 3.1250e-04\n",
      "Epoch 57/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3965 - accuracy: 0.9234\n",
      "Epoch 57: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3965 - accuracy: 0.9234 - val_loss: 0.3804 - val_accuracy: 0.9178 - lr: 3.1250e-04\n",
      "Epoch 58/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.4009 - accuracy: 0.9224\n",
      "Epoch 58: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 38ms/step - loss: 0.4009 - accuracy: 0.9224 - val_loss: 0.3848 - val_accuracy: 0.9178 - lr: 3.1250e-04\n",
      "Epoch 59/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3877 - accuracy: 0.9257\n",
      "Epoch 59: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3877 - accuracy: 0.9257 - val_loss: 0.3969 - val_accuracy: 0.9110 - lr: 3.1250e-04\n",
      "Epoch 60/100\n",
      "124/124 [==============================] - ETA: 0s - loss: 0.3878 - accuracy: 0.9260\n",
      "Epoch 60: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3878 - accuracy: 0.9260 - val_loss: 0.4013 - val_accuracy: 0.9110 - lr: 1.5625e-04\n",
      "Epoch 61/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3852 - accuracy: 0.9278\n",
      "Epoch 61: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3846 - accuracy: 0.9280 - val_loss: 0.3968 - val_accuracy: 0.9178 - lr: 1.5625e-04\n",
      "Epoch 62/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3916 - accuracy: 0.9240\n",
      "Epoch 62: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3910 - accuracy: 0.9242 - val_loss: 0.3944 - val_accuracy: 0.9155 - lr: 1.5625e-04\n",
      "Epoch 63/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.9263\n",
      "Epoch 63: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3957 - accuracy: 0.9265 - val_loss: 0.4029 - val_accuracy: 0.9178 - lr: 1.5625e-04\n",
      "Epoch 64/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3909 - accuracy: 0.9248\n",
      "Epoch 64: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3903 - accuracy: 0.9250 - val_loss: 0.4061 - val_accuracy: 0.9201 - lr: 1.5625e-04\n",
      "Epoch 65/100\n",
      "123/124 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.9281Restoring model weights from the end of the best epoch: 45.\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.92237\n",
      "124/124 [==============================] - 5s 39ms/step - loss: 0.3819 - accuracy: 0.9283 - val_loss: 0.3981 - val_accuracy: 0.9178 - lr: 1.5625e-04\n",
      "Epoch 65: early stopping\n",
      "438/438 [==============================] - 1s 2ms/step\n",
      "Average Accuracy: 0.9251625217129009\n",
      "Average Balanced Accuracy: 0.8850218902298959\n",
      "Average Sensitivity (Sn): 0.8027720376241602\n",
      "Average Specificity (Sp): 0.9672717428356318\n",
      "Average MCC: 0.7986606165834131\n",
      "Average AUC: 0.947165198625668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import math\n",
    "\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "\n",
    "# Convert back to DataFrames if needed (assuming X_train is still a numpy array)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "# Result collection lists\n",
    "ACC_collection = []\n",
    "BACC_collection = []\n",
    "Sn_collection = []\n",
    "Sp_collection = []\n",
    "MCC_collection = []\n",
    "AUC_collection = []\n",
    "\n",
    "# Function to train the model for each fold\n",
    "def ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV):\n",
    "    # Train the model with the training fold\n",
    "    model = train_model(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
    "    return model\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(y_train):\n",
    "    X_train_CV, X_valid_CV = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_CV, y_valid_CV = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Train the model for this fold\n",
    "    model = ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
    "    \n",
    "    # Load the best model\n",
    "    saved_model = load_model('best_model_640.h5')\n",
    "    \n",
    "    # Predict probabilities\n",
    "    predicted_probabilities = saved_model.predict(X_valid_CV, batch_size=1)\n",
    "    \n",
    "    # Convert probabilities to class predictions\n",
    "    predicted_class = (predicted_probabilities > 0.5).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    y_true = y_valid_CV.values\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Sn = TP / (TP + FN)\n",
    "    Sp = TN / (TN + FP)\n",
    "    MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "    BACC = 0.5 * (Sn + Sp)\n",
    "    AUC = roc_auc_score(y_true, predicted_probabilities)\n",
    "    \n",
    "    # Append metrics to collection lists\n",
    "    ACC_collection.append(ACC)\n",
    "    Sn_collection.append(Sn)\n",
    "    Sp_collection.append(Sp)\n",
    "    MCC_collection.append(MCC)\n",
    "    BACC_collection.append(BACC)\n",
    "    AUC_collection.append(AUC)\n",
    "\n",
    "# Display the results for each fold\n",
    "print(\"Average Accuracy:\", np.mean(ACC_collection))\n",
    "print(\"Average Balanced Accuracy:\", np.mean(BACC_collection))\n",
    "print(\"Average Sensitivity (Sn):\", np.mean(Sn_collection))\n",
    "print(\"Average Specificity (Sp):\", np.mean(Sp_collection))\n",
    "print(\"Average MCC:\", np.mean(MCC_collection))\n",
    "print(\"Average AUC:\", np.mean(AUC_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e27b8c-0f2b-4ad8-adc4-b6a4e656e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Mean = 0.9251625217129009 , Std = 0.005019545886289918\n",
      "Balanced Accuracy: Mean = 0.8850218902298959 , Std = 0.007883626561320512\n",
      "Sensitivity (Sn): Mean = 0.8027720376241602 , Std = 0.01808511332681345\n",
      "Specificity (Sp): Mean = 0.9672717428356318 , Std = 0.009262500237727034\n",
      "MCC: Mean = 0.7986606165834131 , Std = 0.016867459101472037\n",
      "AUC: Mean = 0.947165198625668 , Std = 0.01460181098666191\n"
     ]
    }
   ],
   "source": [
    "# Display the results for each fold with mean and standard deviation\n",
    "print(\"Accuracy: Mean =\", np.mean(ACC_collection), \", Std =\", np.std(ACC_collection))\n",
    "print(\"Balanced Accuracy: Mean =\", np.mean(BACC_collection), \", Std =\", np.std(BACC_collection))\n",
    "print(\"Sensitivity (Sn): Mean =\", np.mean(Sn_collection), \", Std =\", np.std(Sn_collection))\n",
    "print(\"Specificity (Sp): Mean =\", np.mean(Sp_collection), \", Std =\", np.std(Sp_collection))\n",
    "print(\"MCC: Mean =\", np.mean(MCC_collection), \", Std =\", np.std(MCC_collection))\n",
    "print(\"AUC: Mean =\", np.mean(AUC_collection), \", Std =\", np.std(AUC_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5141fb51-a6ef-4f76-91b2-ac2fb5a3d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 15ms/step\n",
      "\n",
      "Optimized Test Dataset Results:\n",
      "Accuracy (ACC): 0.9233576642335767\n",
      "Balanced Accuracy (BACC): 0.8797910326995128\n",
      "Sensitivity (Sn): 0.7913669064748201\n",
      "Specificity (Sp): 0.9682151589242054\n",
      "MCC: 0.7921304996385995\n",
      "AUC: 0.8797910326995128\n",
      "True Positives (TP): 220\n",
      "False Positives (FP): 26\n",
      "True Negatives (TN): 792\n",
      "False Negatives (FN): 58\n",
      "Total Positive: 278\n",
      "Total Negative: 818\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test dataset\n",
    "predicted_probas_test = saved_model.predict(X_test, batch_size=32)\n",
    "best_threshold_test, best_mcc_test = optimize_threshold(y_test, predicted_probas_test)\n",
    "predicted_classes_test = (predicted_probas_test > best_threshold_test).astype(int)\n",
    "\n",
    "# Calculate metrics for the test dataset with optimized threshold\n",
    "accuracy_test = accuracy_score(y_test, predicted_classes_test)\n",
    "sensitivity_test = recall_score(y_test, predicted_classes_test)  # Sensitivity (Recall)\n",
    "TN_test, FP_test, FN_test, TP_test = confusion_matrix(y_test, predicted_classes_test).ravel()\n",
    "specificity_test = TN_test / (TN_test + FP_test)  # Corrected Specificity calculation\n",
    "MCC_test = matthews_corrcoef(y_test, predicted_classes_test)\n",
    "auc_test = roc_auc_score(y_test, predicted_classes_test)\n",
    "\n",
    "# Compute the correct balanced accuracy\n",
    "balanced_accuracy_test = (sensitivity_test + specificity_test) / 2\n",
    "\n",
    "# Print the adjusted results for the test dataset\n",
    "print(\"\\nOptimized Test Dataset Results:\")\n",
    "print(f\"Accuracy (ACC): {accuracy_test}\")\n",
    "print(f\"Balanced Accuracy (BACC): {balanced_accuracy_test}\")\n",
    "print(f\"Sensitivity (Sn): {sensitivity_test}\")\n",
    "print(f\"Specificity (Sp): {specificity_test}\")\n",
    "print(f\"MCC: {MCC_test}\")\n",
    "print(f\"AUC: {auc_test}\")\n",
    "print(f\"True Positives (TP): {TP_test}\")\n",
    "print(f\"False Positives (FP): {FP_test}\")\n",
    "print(f\"True Negatives (TN): {TN_test}\")\n",
    "print(f\"False Negatives (FN): {FN_test}\")\n",
    "\n",
    "# Print the total positive and total negative\n",
    "total_positive = np.sum(y_test)\n",
    "total_negative = len(y_test) - total_positive\n",
    "print(f\"Total Positive: {total_positive}\")\n",
    "print(f\"Total Negative: {total_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc0232f1-1e5f-4a1a-9c9c-46b4dbede579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 14ms/step\n",
      "\n",
      "Optimized External Dataset (KELM) Results:\n",
      "Accuracy (ACC): 0.8802083333333334\n",
      "Balanced Accuracy (BACC): 0.8802083333333334\n",
      "Sensitivity (Sn): 0.8645833333333334\n",
      "Specificity (Sp): 0.8958333333333334\n",
      "MCC: 0.7607882360348789\n",
      "AUC: 0.8802083333333334\n",
      "True Positives (TP): 83\n",
      "False Positives (FP): 10\n",
      "True Negatives (TN): 86\n",
      "False Negatives (FN): 13\n",
      "Total Positive: 96\n",
      "Total Negative: 96\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the external dataset (KELM)\n",
    "dataset_external = pd.read_csv('kelm_dataset.csv', na_filter=False)\n",
    "X_external_data_name = 'kelm_dataset_esm2_t30_150M_UR50D_unified_640_dimension.csv'\n",
    "X_external_data = pd.read_csv(X_external_data_name, header=0, index_col=0, delimiter=',')\n",
    "X_external = np.array(X_external_data)\n",
    "y_external = np.array(dataset_external['label'])\n",
    "\n",
    "# Normalize the external dataset\n",
    "X_external_normalized = scaler.transform(X_external)\n",
    "\n",
    "# Predict probabilities for external dataset\n",
    "predicted_probas_ext = saved_model.predict(X_external_normalized, batch_size=32)\n",
    "best_threshold_ext, best_mcc_ext = optimize_threshold(y_external, predicted_probas_ext)\n",
    "predicted_classes_ext = (predicted_probas_ext > best_threshold_ext).astype(int)\n",
    "\n",
    "# Calculate metrics for the external dataset with optimized threshold\n",
    "accuracy_ext = accuracy_score(y_external, predicted_classes_ext)\n",
    "sensitivity_ext = recall_score(y_external, predicted_classes_ext)  # Sensitivity (Recall)\n",
    "TN_ext, FP_ext, FN_ext, TP_ext = confusion_matrix(y_external, predicted_classes_ext).ravel()\n",
    "specificity_ext = TN_ext / (TN_ext + FP_ext)  # Corrected Specificity calculation\n",
    "MCC_ext = matthews_corrcoef(y_external, predicted_classes_ext)\n",
    "auc_ext = roc_auc_score(y_external, predicted_classes_ext)\n",
    "\n",
    "# Compute the correct balanced accuracy\n",
    "balanced_accuracy_ext = (sensitivity_ext + specificity_ext) / 2\n",
    "\n",
    "# Print the adjusted results for the external dataset\n",
    "print(\"\\nOptimized External Dataset (KELM) Results:\")\n",
    "print(f\"Accuracy (ACC): {accuracy_ext}\")\n",
    "print(f\"Balanced Accuracy (BACC): {balanced_accuracy_ext}\")\n",
    "print(f\"Sensitivity (Sn): {sensitivity_ext}\")\n",
    "print(f\"Specificity (Sp): {specificity_ext}\")\n",
    "print(f\"MCC: {MCC_ext}\")\n",
    "print(f\"AUC: {auc_ext}\")\n",
    "print(f\"True Positives (TP): {TP_ext}\")\n",
    "print(f\"False Positives (FP): {FP_ext}\")\n",
    "print(f\"True Negatives (TN): {TN_ext}\")\n",
    "print(f\"False Negatives (FN): {FN_ext}\")\n",
    "\n",
    "# Print the total positive and total negative\n",
    "total_positive_ext = np.sum(y_external)\n",
    "total_negative_ext = len(y_external) - total_positive_ext\n",
    "print(f\"Total Positive: {total_positive_ext}\")\n",
    "print(f\"Total Negative: {total_negative_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "012045d6-990b-4b33-899d-b04f9e7af60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_true, y_pred_probas, title='ROC Curve'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_probas)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curve for the test dataset\n",
    "plot_roc_curve(y_test, predicted_probas_test, title='ROC Curve for Test Dataset')\n",
    "\n",
    "# Plot ROC curve for the external dataset\n",
    "plot_roc_curve(y_external, predicted_probas_ext, title='ROC Curve for External Dataset (KELM)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b62e1c37-de03-4931-9c41-ad2929ae28e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fpr_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Example saving for the test dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse Positive Rate\u001b[39m\u001b[38;5;124m'\u001b[39m: fpr_test,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Positive Rate\u001b[39m\u001b[38;5;124m'\u001b[39m: tpr_test,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROC AUC\u001b[39m\u001b[38;5;124m'\u001b[39m: [roc_auc_test] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(fpr_test)  \u001b[38;5;66;03m# AUC is constant across the curve\u001b[39;00m\n\u001b[0;32m      8\u001b[0m })\n\u001b[0;32m      9\u001b[0m df_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mESM-640_test_dataset_roc_values.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Example saving for the external dataset\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fpr_test' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example saving for the test dataset\n",
    "df_test = pd.DataFrame({\n",
    "    'False Positive Rate': fpr_test,\n",
    "    'True Positive Rate': tpr_test,\n",
    "    'ROC AUC': [roc_auc_test] * len(fpr_test)  # AUC is constant across the curve\n",
    "})\n",
    "df_test.to_csv('ESM-640_test_dataset_roc_values.csv', index=False)\n",
    "\n",
    "# Example saving for the external dataset\n",
    "df_ext = pd.DataFrame({\n",
    "    'False Positive Rate': fpr_ext,\n",
    "    'True Positive Rate': tpr_ext,\n",
    "    'ROC AUC': [roc_auc_ext] * len(fpr_ext)  # AUC is constant across the curve\n",
    "})\n",
    "df_ext.to_csv('ESM-640_external_dataset_roc_values.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d99bd-5712-474c-8abc-eab7fd5faca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
